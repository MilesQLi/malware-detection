# Generate file id and magic values for all files.
#
# Inputs: av-file-id-labels.csv (scalar labels for all file types.)
#         row format = [file_type, mime, strength, id]
#
# Output: xxxx-sorted-file-id-features.csv ( 4 x feature files )
#         row format = [file_name, file_type, file_id]
#
#         Combined into one feature file:
#
#         sorted-file-id-features.csv
#
#
# Author: Derek Chadwick
# Date  : 27/08/2016



from multiprocessing import Pool
import os
import sys
import re
import pandas as pd
import subprocess as sub




def load_file_id_map():
    # Load the file ID scalar labels and create a map. There are a lot of duplicate names so the total is less than
    # the number of packers listed in the file db.
    file_id_map = {}

    counter = 0
    fip = open('data/av-file-id-labels.csv','r')
    in_lines = fip.readlines()
    fip.close()
    
    for idx in range(1,len(in_lines)): # Skip the column header row.
        tokens = in_lines[idx].split(',')
        file_type_name = tokens[0]
        if file_type_name not in file_id_map.keys():
            file_id_map[file_type_name] = int(tokens[3])
            counter += 1

    
    print('Completed loading {:d} file IDs.'.format(counter))
    
    return file_id_map




def sort_and_save_file_id_feature_file():
    # Load in the combined feature files, sort and save.
    # NOTE: add a file name argument so the final filename can be
    #       specified during runs on multiple datasets.
    
    fid = pd.read_csv('data/file-id-features.csv')
    # DataFrame.sort() is deprecated, but this is an old version of pandas, does not have sort_values().
    sorted_ids = fid.sort('file_name')
    sorted_ids.to_csv('data/sorted-file-id-features.csv', index=False)
    sorted_ids.head(20)
    
    return


def combine_file_id_files():
    # Function to combine the four file id feature files in one file
    # 1. list data directory
    # 2. For each file in file list that matches (\d\d\d\d-file-id-features.csv)
    # 3. Trim the filenames if necessary (should remove VirusShare_  prefix).
    # 4. Concatenate the unsorted file id feature files.
    # 5. Sort and write to data/sorted-file-id-features.csv
    # NOTE: add a file name argument so the final filename can be
    #       specified during runs on multiple datasets.
    
    fop = open('data/file-id-features.csv','w')
    fop.write('file_name,file_type,file_id\n')
    p1 = re.compile('\d{3,5}-sorted-file-id-features.csv') # This is the PID prefix for each file.
    file_list = os.listdir('data/')
    counter = 0
    
    for file_name in file_list:
        if p1.match(file_name):
            fip = open('data/' + file_name, 'r')
            in_lines = fip.readlines()
            #if counter > 0:
            #    in_lines = in_lines[1:] # skip the column header row
            fop.writelines(in_lines)
            counter += len(in_lines)
            fip.close()
            
    print('Completed combine of {:d} file ID features.'.format(counter))  
    
    fop.close()
    
    sort_and_save_file_id_feature_file()
    
    return

    


def generate_sample_file_id(file_list):
    # Generate scalar file ID for each sample.
    pid = os.getpid()
    file_name = "data/" + str(pid) + "-sorted-file-id-features.csv"
    fop = open(file_name,'w')
    out_lines = []
    file_id_map = load_file_id_map()
    signat = 'unknown'
    file_counter = 0
    
    for idx, file_name in enumerate(file_list):
        tokens = file_name.split('_')
        truncated_file_name = tokens[1] # remove the VirusShare_ prefix from the filename.
        file_path = ext_drive + file_name
        file_id = 0
        
        signat = sub.check_output(["file","-b", file_path]) # Use the brief option, we do not need the file name.
        signat = signat.replace(',','').rstrip() # get rid of newlines and commas they are annoying
        
        
        if (signat in file_id_map.keys()):
            file_id = file_id_map[signat]
            
            
        row = truncated_file_name + "," + signat + "," + str(file_id) + "\n"
    
        out_lines.append(row)
        
        file_counter += 1
        
        if (idx % 1000) == 0: # print progress
            fop.writelines(out_lines)
            out_lines = []
            print('{:s} - {:s} - {:d} - {:s}'.format(str(pid), truncated_file_name, idx, signat))


    if len(out_lines) > 0:
        fop.writelines(out_lines)
        out_lines = []

    fop.close()

    print('{:s} - Completed {:d} files.'.format(str(pid), file_counter))
    
    return


    
# Start of script.

# TODO: add command line arguments to specify input files.

#ext_drive = '/opt/vs/train1/'
#ext_drive = '/opt/vs/train2/'
#ext_drive = '/opt/vs/train3/'
#ext_drive = '/opt/vs/train4/'
ext_drive = '/opt/vs/apt/'

tfiles = os.listdir(ext_drive)
quart = len(tfiles)/4
train1 = tfiles[:quart]
train2 = tfiles[quart:(2*quart)]
train3 = tfiles[(2*quart):(3*quart)]
train4 = tfiles[(3*quart):]

print("Files: {:d} - {:d} - {:d}".format(len(tfiles), quart, (len(train1)+len(train2)+len(train3)+len(train4))))

trains = [train1, train2, train3, train4]
p = Pool(4)
p.map(generate_sample_file_id, trains)

print('Completed processing {:d} files.'.format(len(tfiles)))

combine_file_id_files()

# End of Script.