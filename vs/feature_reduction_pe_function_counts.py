# feature_reduction_function_names.py
#
# Read a a bunch of function name feature sets and use chi2 tests
# to remove features that are independent of the label.
#
# Input : function count feature sets in CSV files.
#         row format = [file_name, [list of function names]...]
#
# Output: all-reduced-function-counts.csv
#         row format = [file_name, [list of function names]...]
#
#
#
# Author: Derek Chadwick
# Date  : 14/11/2016
#
# TODO: optimise and many many things

import os
from csv import writer
import numpy as np
import pandas as pd
import math
import scipy.misc
import array
import re
from sklearn.feature_selection import SelectKBest, SelectPercentile
from sklearn.feature_selection import chi2



def reduce_feature_set(feature_set_file, train_label_file, function_name_file):
    # Use chi2 tests to determine the 10% best features see (mmmc/feature-reduction-call-graphs.ipynb).
    # Ok, so we still have 100000+ features even after severely reducing the function name lengths.
    # This is a problem. Having to process such a huge sparse matrix requires a lot of memory.
    # Solution 1: rent an AWS server with plenty-o-ram. (costs money and requires high bandwidth for file transfer)
    # Solution 2: buy more RAM for my linux box. (costs money)
    # Solution 3: break the sparse matrix into smaller chunks and process individually. (Ok)
    # Solution 4: try the pandas sparse matrix data structure. (too slow)
    
    # -> Solution 3: slice the matrix into smaller chunks for processing.
    # the pandas spare matrix still takes too long, break up into 10 different feature sets and try again.
    
    # Procedure:
    # 1. Open the PE function count feature file.
    # 2. Open the PE function name file and get the number of column names.
    # 3. Divide the number of columns by 10 to get the column subset length.
    # 4. Load the malware label set.
    # 5. Use pandas to load and sort each column subset.
    # 6. Do the chi2 tests to reduce each column subset to 10 percent best features.
    # 7. Recombine the column subsets.
    # 8. Perform the chi2 test again on the combined reduced feature set.
    # 9. Write out the final reduced feature set to a csv file.
    
    # Open PE function name file and get a list of token names.
    token_list = []
    fip = open('data/' + function_name_file, 'r')
    inlines = fip.readlines()
    token_list_len = len(inlines)
    for token in inlines: # Remove whitespace/newlines.
        token_list.append(token.rstrip())    
    
    fip.close()

    # Load training labels
    sorted_train_labels = pd.read_csv("data/" + train_label_file)
    #sorted_train_labels.head()
        
    # Load and sort the malware sample names.
    # sample_names = pd.read_csv(feature_set_file, usecols = [0], index_col=None, na_filter=False)
    # DEPRECATED: 
    # sorted_sample_names = sample_names.sort('filename')
    # Includes the header line in the sort!

    #print("Sample len: {:d}".format(len(sample_names))) #,len(sorted_sample_names)))
    #print(sample_names)
    #sample_names.to_csv('data/temp-sample-names.csv')
    #sorted_sample_names.to_csv('data/temp-sorted-sample-names.csv')

    # Debug: bad function names in feature sets, regenerate using the multi-line function names files.
    # sample_names = []
    # fip = open(feature_set_file, 'r')
    # fop = open('data/tempsamplenames.txt', 'w')
    # for line in fip:
    #    fname = line[0:line.find(',')]
    #    if fname == 'filename':
    #        continue
    #    if len(fname) > 0:
    #        sample_names.append(fname)
    #        fop.write(fname + '\n')
        
    # fop.close()
    # fip.close()

    #return

    # Now get the labels of the PE malware samples from the label set.
    counter = 0
    y = []
    fip = open('data/' + train_label_file)
    lines = fip.readlines()
    for line in lines:
        line = line.rstrip()
        y.append(int(line))
    
    #for fname in sample_names:
    #    counter += 1
    #    if counter % 100 == 1:
    #        #print("Counter = {:d}".format(counter))
    #        print("Appending filename {:d} -> {:s}".format(counter, fname))
    #    for idx, fname2 in enumerate(sorted_train_labels['file_name']):
    #        if (fname2 == fname):
    #            y.append(sorted_train_labels.iloc[idx, 4]) # Append the family class label.
    #            break
    
    ###############################
    # Write out the PE/COFF sample train labels for later use and validation.
    #fop = open('data/' + temp_train_labels, 'w')
    #fop.writelines("\n".join(str(x) for x in y))
    #fop.close()
    ###############################
    
    # Load column subset and sort, then 
    # Perform chi2 test to get 10% best features.
    
    onetenth = int(token_list_len / 10)
    startidx = 1 # skip the filename column
    endidx = onetenth

    for idx in range(0,10):
        print("Processing function count column set {:d} -> {:d}".format(startidx, endidx))
        column_numbers = [ 0 ] + list(range(startidx, endidx, 1))
        feature_subset = pd.read_csv(feature_set_file, usecols = column_numbers)
        
        # Sort the feature subset on file_name column.
        sorted_feature_subset = feature_subset.sort('filename')
        
        X = sorted_feature_subset.iloc[:,1:] # skip the filename, get the family class label for this feature subset.

        # Find the top 10 percent variance features.
        print("Sorted feature subset - slice {:d} of 10.".format(idx))
        print("Subset shape: {:d} {:d}".format(X.shape[0], X.shape[1]))
        print("Length of y: {:d}".format(len(y)))
        #sorted_feature_subset.head()
        
        # Now select the 10% best features for this feature subset.
        # Try to make the subset file sizes smaller.
        fsp = SelectPercentile(chi2, 10)
        X_new_10 = fsp.fit_transform(X,y)
        selected_names = fsp.get_support(indices=True)
        selected_names = selected_names + 1 # the column name indices start at 0 so add 1 to all.
        
        data_trimmed = sorted_feature_subset.iloc[:,selected_names]
        data_fnames = pd.DataFrame(sorted_feature_subset['filename'])
        data_reduced = data_fnames.join(data_trimmed)
        
        # Write to file as we do not have enough memory.
        filename = "data/pe-function-count-temp-" + str(idx) + "-10perc.csv"
        data_reduced.to_csv(filename, index=False)
        
        # TEST AND VALIDATION ONLY.
        ############################################
        #out_subset = sorted_feature_subset.iloc[:,0:2]
        #out_subset.to_csv(filename, index=False)
        print("Writing reduced feature file: {:s}".format(filename))
        ############################################
        
        startidx = endidx
        endidx += onetenth


    return


def combine_reduced_feature_sets(reduced_feature_file_name):
    # Now recombine the reduced sets and perform chi-squared tests again.
    fname = "data/pe-function-count-temp-0-10perc.csv"
    reduced_feature_counts = pd.read_csv(fname)
    for idx in range(1,10):
        fname = "data/pe-function-count-temp-" + str(idx) + "-10perc.csv"
        print("Processing file: {:s}".format(fname))
        nextfc = pd.read_csv(fname)
        reduced_feature_counts = pd.merge(reduced_feature_counts, nextfc, on='file_name')


    reduced_feature_counts.to_csv("data/" + reduced_feature_file_name, index=False)
    print("Saved reduced feature set: {:d} {:d}".format(reduced_feature_counts.shape[0], reduced_feature_counts.shape[1]))
    
    return


def final_feature_set_reduction(reduced_feature_file_name, final_file_name, train_label_file):
    sorted_train_data = pd.read_csv('data/' + reduced_feature_file_name)
    y = []
    X = sorted_train_data.iloc[:,1:]
    fip = open('data/' + train_label_file)
    lines = fip.readlines()
    for line in lines:
        line = line.rstrip()
        y.append(int(line))

    print("Final feature reduction: {:s}".format(reduced_feature_file_name))
    print("Training labels length: {:d}".format(len(y)))
    print("X Feature set dimensionality: {:d} {:d}".format(X.shape[0], X.shape[1]))
    print("In Feature set dimensionality: {:d} {:d}".format(sorted_train_data.shape[0], sorted_train_data.shape[1]))

    # find the top 10 percent variance features, from ~1000 -> ~100 features
    fsp = SelectPercentile(chi2, 10)
    X_new_10 = fsp.fit_transform(X,y)
    print("Final 10 Percent Dimensions: {:d} {:d}".format(X_new_10.shape[0], X_new_10.shape[1]))
    
    selected_names = fsp.get_support(indices=True)
    selected_names = selected_names + 1

    #data_reduced = sorted_train_data.iloc[:,[0] + selected_names]
    #Does not put the file_name as the first column.
    data_trimmed = sorted_train_data.iloc[:,selected_names]
    data_fnames = pd.DataFrame(sorted_train_data['filename'])
    data_reduced = data_fnames.join(data_trimmed)
    
    data_reduced.to_csv('data/' + final_file_name, index=False)
    print("Completed reduction in {:s}".format(final_file_name))
    
    return



# Start of Script.

ext_drive = '/opt/vs/'

feature_file = ext_drive + 'call-graph-reduced-function_counts-vs263.csv'
reduced_feature_file = 'reduced-function-counts-vs263.csv'
function_names_file = 'all-column-names-multi-line-vs263.txt'
training_labels_file = 'sorted-train-labels-vs263.csv'
temp_train_labels_file = 'pe-train-labels-vs263.txt'
final_file_name = 'sorted-pe-function-count-features-10percent-vs263.csv'

#feature_file = ext_drive + 'call-graph-reduced-function_counts-vs264.csv'
#reduced_feature_file = 'reduced-function-counts-vs264.csv'
#function_names_file = 'all-column-names-multi-line-vs264.txt'
#training_labels_file = 'sorted-train-labels-vs264.csv'
#temp_train_labels_file = 'pe-train-labels-vs264.txt'
#final_file_name = 'sorted-pe-function-count-features-10percent-vs264.csv'

#######################################################
# NOTE: there is also a problem with vs263 and vs264 
#       feature sets, all the rows are zeroes!!!.
#######################################################

reduce_feature_set(feature_file, temp_train_labels_file, function_names_file)

combine_reduced_feature_sets(reduced_feature_file)

final_feature_set_reduction(reduced_feature_file, final_file_name, temp_train_labels_file)

# End of Script.
