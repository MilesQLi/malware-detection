# generate-elf-header-tokens.py
#
# Parse a bunch of ELF Header dump files generated by objdump and
# extract keywords such section names, import libs and functions.
# These tokens will be used for feature extraction from the
# ELF headers.
#
# Inputs : list of ELF Header files.
#          Temp file name for token counts.
#          File name for combined token counts.
#
# Outputs: elf-header-tokens.txt
#          elf-header-token-counts.csv
#          row format = [token_name, count]
#
# Author: Derek Chadwick
# Date  : 17/09/2016


import os
from csv import writer
import numpy as np
import pandas as pd
import re


elf_header_names = ['Magic:','Class:','Data:','Version:','OS/ABI:','ABI Version:','Type:','Machine:','Version:',
                    'Entry point address:','Start of program headers:','Start of section headers:','Flags:',
                    'Size of this header:','Size of program headers:','Number of program headers:',
                    'Size of section headers:','Number of section headers:','Section header string table index:']


def save_token_counts(token_counter_map, out_file_name):
    # Output the PE Header token counts.
    pid = os.getpid()
    out_file = "data/" + str(pid) + "-" + out_file_name
    fop = open(out_file, 'w')
    csv_wouter = writer(fop)

    outlines = []
    sorted_keys = token_counter_map.keys()
    sorted_keys.sort()
    counter = 0
    
    for key in sorted_keys:
        outlines.append([key, token_counter_map[key]])
        counter += 1
        if (counter % 100) == 0: # write out some lines
            csv_wouter.writerows(outlines)
            outlines = []
            print("Processed token {:s} -> {:d}.".format(key, token_counter_map[key]))

    # Finish off.
    if (len(outlines) > 0):
        csv_wouter.writerows(outlines)
        outlines = []

    print("Completed writing {:d} tokens.".format(len(sorted_keys)))    
    fop.close()

    return


def get_token_count_map(token_df):
    # Read in the token count file and create a dict.
    token_dict = {}
    type_y = np.array(token_df['token_name'])
    
    for idx in range(token_df.shape[0]): # First fill the dict with the token counts
        token_dict[token_df.iloc[idx,0]] = token_df.iloc[idx,1]
        

    return token_dict

    
def generate_elf_tokens(mp_params):
    # Parse a bunch of ELF headers dumped by objdump and extract
    # section names and other useful information.
    file_list = mp_params.file_list
    out_count_file = mp_params.count_file
    
    psections = re.compile('\s+\[\d{1,2}\]\s+(\.\w+|\w+)\s+')  # Pattern for section names.
    pfunctions = re.compile('\s+\w+\s+\d{1,4}\s+(.+)\s*')      # Pattern for import function names.

    
    token_counter_map = {}
    counter = 0
    pid = os.getpid()
    
    for idx, fname in enumerate(file_list):

        fip = open(fname, 'r')
        in_lines = fip.readlines()
        
        counter += 1
        
        for line in in_lines:

            line = line.rstrip() # get rid of newlines they are annoying.
            token_val = ""

            m = psections.match(line)
            if m != None:
                token_val = m.group(1)
                #print("Section: {:s}".format(token_val))
            else:
                m = pfunctions.match(line)
                if m != None:
                    token_val = m.group(1)
                else:
                    continue
                        
            # Clean the token name, the function name regex is picking up random crap.
            idx = token_val.find('\t')
            if idx > 0:
                token_val = token_val[0:idx]
                
            # Count the token type.
            if token_val in token_counter_map.keys():
                token_counter_map[token_val] += 1
            else:
                token_counter_map[token_val] = 1


        if (counter % 1000) == 0:
            print("{:d} Processed {:d} header files.".format(pid, counter))

        fip.close()
        
        
    save_token_counts(token_counter_map, out_count_file)
    
    return





class Multi_Params(object):
    def __init__(self, tokenfile="", countfile="", filelist=[]):
        self.token_file = tokenfile
        self.count_file = countfile
        self.file_list = filelist
        

# Start of script.

#TODO: parse command line options for input/output file names.

#token_file = 'elf-header-tokens-vs263.csv'
#count_file = 'elf-header-token-counts-vs263.csv'
#ext_drive = '/opt/vs/train3hdr/'

token_file = 'elf-header-tokens-vs264.txt'
count_file = 'elf-header-token-counts-vs264.csv'
#ext_drive = '/opt/vs/train4hdr/'
ext_drive = '/home/derek/project/temp/'

file_list = os.listdir(ext_drive)
tfiles = []

for fname in file_list:
    fname = fname.rstrip()
    if fname.endswith('.elf.txt'):
        tfiles.append(ext_drive + fname)


print("Files: {:d}".format(len(tfiles)))

mp1 = Multi_Params(token_file, count_file, tfiles)

generate_elf_tokens(mp1)

# End of Script.
