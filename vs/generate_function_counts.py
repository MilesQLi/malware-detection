# generate_function_counts.py
#
# Read a bunch of call graph files in GraphViz format and generate function count
# feature sets in CSV format.
#
# Input : Call graph files in GraphViz format.
#         GraphViz directed graph format:
#
#            digraph graph_name {
#                node_name1 -> { node_name2 ; node_name3 ; .... }
#                .
#                .
#                .
#                node_nameX -> { node_nameY ; node_nameZ ; .... }
#            }
# 
#
# Output: all-reduced-function-counts.csv
#         row format = [file_name, [list of function names]...]
#
#
#
# Author: Derek Chadwick
# Date  : 14/11/2016
#
# TODO: optimise and many many things

import numpy as np
import pandas as pd
import graph as gra # http://www.python-course.eu/graphs_python.php
import os
from csv import writer
from multiprocessing import Pool
import re


def generate_column_names(call_graph_file, column_file):
    counter = 0
    column_names = ['filename']
    graph_names = []
    graph_name = "none"
    graph_functions = {}

    #fapi = open("data/APIs.txt")
    #defined_apis = fapi.readlines()
    #defined_apis = defined_apis[0].split(',')
    #fapi.close()
    
    pid = os.getpid()
    print('Process id:', pid)
    #column_names_file = 'data/' + str(pid) + '-' + column_file 
    print('Column names file: {:s}'.format(column_file))
    #graph_names_file = 'data/' + str(pid) + '-graph-names.csv'  
    #print('Graph names file: {:s}'.format(graph_names_file))    

    with open(call_graph_file, 'r') as cfg:
        print("Starting graph file: {:s}".format(call_graph_file))
        for line in cfg:
            
            if line.startswith('digraph'):
                tokens = line.split()
                graph_name = tokens[1]
                graph_names.append(graph_name)
                continue
                
            line = line.rstrip('\r\n')  # get rid of newlines they are annoying.
            # get rid of all these things they are annoying.
            line = line.replace(';',' ').replace('{',' ').replace('}',' ').replace('->',' ')
            parts = line.split() # tokenize call graph line
            
            
            #graph_name = parts[0] # this is for single line call graphs.
            #parts = parts[1:]
            #graph_names.append(graph_name)
            #graph_functions = {}
            
            for func in parts:
                #if func not in defined_apis: # ignore these API functions, they have already been counted.
                if func.startswith('sub') or func.startswith('loc') or func.startswith('unk'):
                    func = func[:5] # lets try to reduce the vast number of functions.
                elif func.startswith('eax+') or func.startswith('ebx+') or func.startswith('ecx+') or func.startswith('edx+'):
                    func = func[:5]
                elif func.startswith('edi+') or func.startswith('esi+'):
                    func = func[:5]
                elif func.startswith('byte_') or func.startswith('word_'): # or func.startswith('nullsub')
                    func = func[:6]
                else: # reduce the feature set some more so my pissy pants PC can handle it.
                    func = func[:8]
                    
                if func not in column_names: # NOTE: or in Defined APIs, these have already been counted.    
                    column_names.append(func)

 
            counter += 1
            # Print progress
            if ((counter + 1) % 10000) == 0:
                print("Call Graph File: {:s} Processed number {:d} Graph name {:s} Total column names {:d}".format(call_graph_file, counter, graph_name, len(column_names)))       

                
    with open(column_file, 'w') as cols:
        fw = writer(cols)
        fw.writerow(column_names)
    
    print("Completed writing {:d} column names.".format(len(column_names)))

    #with open(graph_names_file, 'w') as gras:
    #    fw = writer(gras)
    #    fw.writerow(graph_names)
    
    #print("Completed writing {:d} graph names.".format(len(graph_names)))
    
    return



def merge_column_names_single_line(column_name_files, combined_column_name_file):
    # Generate the merged column names file single line.
    counter = 0
    column_names = []
    
    for cnamefile in column_name_files:
        with open(cnamefile, 'r') as cras:
            print("Starting file: {:s}".format(cnamefile))
            colstr = cras.readline()
            colnames = colstr.split(',')
            for cname in colnames:
                if cname not in column_names:
                    column_names.append(cname)

                counter += 1
                # Print progress
                if ((counter + 1) % 1000) == 0:
                    print("Processed column names {:d}".format(counter))       

    with open(combined_column_name_file, 'w') as cols:
        fw = writer(cols)
        fw.writerow(column_names)

    print("Completed writing column names total = {:d}".format(len(column_names)))
    
    return


# Only use for testing.
def merge_column_names_multi_line():
    #Generate the merged column names file multiline.
    counter = 0
    column_names = []
    column_name_files = ['data/3346-reduced-column-names.csv', 'data/3347-reduced-column-names.csv', 'data/3348-reduced-column-names.csv', 'data/3349-reduced-column-names.csv']
    for cnamefile in column_name_files:
        with open(cnamefile, 'r') as cras:
            print("Starting file: {:s}".format(cnamefile))
            colstr = cras.readline()
            colnames = colstr.split(',')
            for cname in colnames:
                if cname not in column_names:    
                    column_names.append(cname)

                counter += 1
                # Print progress
                if ((counter + 1) % 1000) == 0:
                    print("Processed column names {:d}".format(counter))       

    with open('data/all-reduced-function-column-names-multiline.csv', 'w') as cols:
        for cname in column_names:
            outline = cname + "\n"
            cols.write(outline)

    print("Completed writing column names total = {:d}".format(len(column_names)))
    
    return




def generate_function_counts(call_graph_file):
    # Generate function counts from graph files of the ASM malware samples.
    
    counter = 0
    error_count = 0
    
    #fapi = open("data/APIs.txt")
    #defined_apis = fapi.readlines()
    #defined_apis = defined_apis[0].split(',')
    #fapi.close()
    
    colf = open('data/all-reduced-function-column-names.csv', 'r')
    all_column_names = []
    column_lines = colf.readlines()
    for line in column_lines:
        all_column_names += line.split(',')
    col_names_len = len(all_column_names)
    colf.close()
    print("Column Names: {:d}".format(col_names_len))
    
    pid = os.getpid()
    print('Process id:', pid)
    feature_file_name = 'data/' + str(pid) + '-call-graph-reduced-function_counts.csv'  
    print('Call graph function counts file: {:s}'.format(feature_file_name))
    feature_file = open(feature_file_name, 'w')
    fw = writer(feature_file)
    
    call_graph_function_features = []
    
    with open(call_graph_file, 'r', errors='ignore') as cfg:
        for line in cfg:
            
            if line.startswith('digraph') or line.startswith(' }'):
                continue
                
            line.rstrip('\r\n')  # get rid of newlines they are annoying.
            # get rid of all these things they are annoying.
            line = line.replace(';',' ').replace('{',' ').replace('}',' ').replace('->',' ')
            parts = line.split() # tokenize graph line
            
            graph_name = parts[0]
            parts = parts[1:]
            function_dict = {}
            
            # now generate the function counts for this call graph
            
            for func in parts:
                #if func not in defined_apis: # ignore these API functions, they have already been counted.
                if func.startswith('sub') or func.startswith('loc') or func.startswith('unk'):
                    func = func[:5] # lets try to reduce the vast number of functions.
                elif func.startswith('eax+') or func.startswith('ebx+') or func.startswith('ecx+') or func.startswith('edx+'):
                    func = func[:5]
                elif func.startswith('edi+') or func.startswith('esi+'):
                    func = func[:5]
                elif func.startswith('byte_') or func.startswith('word_'): # or func.startswith('nullsub')
                    func = func[:6]
                else: # reduce the feature set some more so my pissy pants PC can handle it.
                    func = func[:8]

                if (func in function_dict):
                    function_dict[func] += 1
                else:
                    function_dict[func] = 1
            
            # now generate the output row for this call graph

            function_counts = [0] * col_names_len # zero everything because this is a sparse matrix
            for func in function_dict:
                for idx, cname in enumerate(all_column_names):
                    if func == cname:
                        function_counts[idx] = function_dict[func]
                        break
                
            call_graph_function_features.append([graph_name] + function_counts)
            
            # Print progress and write out rows
            counter += 1
            if ((counter + 1) % 100) == 0:
                print("{:d} Graph: {:s} Count: {:d}".format(pid, graph_name, counter))
                fw.writerows(call_graph_function_features)
                call_graph_function_features = []
                
        # Write remaining files
        if len(call_graph_function_features) > 0:
            fw.writerows(call_graph_function_features)
            call_graph_function_features = []  
    
    feature_file.close()
    
    print("Completed processing {:d} graphs.".format(counter))
    
    return



# Start of script.

#TODO: parse command line options for input/output file names.

ext_drive = '/opt/vs/'
sample_set_id = 'vs251'

pecallgraphs = re.compile('\d{3,5}-pe-call-graphs-' + sample_set_id +'.gv') # This is the PID prefix for each file.

file_list = os.listdir(ext_drive)
call_graph_files = []
column_name_files = []

counter = 0
for file_name in file_list:
    if pecallgraphs.match(file_name):
        call_graph_files.append(file_name)
        counter += 1
        column_name_file = 'data/reduced-column-names-' + sample_set_id + '-' + str(counter) + '.txt'
        column_name_files.append(column_name_file)
        print("Found call graph file: {:s}".format(file_name))
        print("Column name file: {:s}".format(column_name_file))


for idx, call_graph_file_name in enumerate(call_graph_files):
    print("Doing column names: {:s} - {:s}".format(call_graph_file_name, column_name_files[idx]))
    generate_column_names('/opt/vs/' + call_graph_file_name, column_name_files[idx])

                    
merge_column_names_single_line(column_name_files, 'data/all-column-names-' + sample_set_id + '.txt')

              
# Now do the function counting.


              
              

# End of Script








