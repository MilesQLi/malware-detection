

import numpy as np
import pandas as pd
import graph as gra # http://www.python-course.eu/graphs_python.php
import os
from csv import writer
from multiprocessing import Pool



def generate_column_names(call_graph_file):
    counter = 0
    column_names = ['filename']
    graph_names = []
    graph_name = "none"
    graph_functions = {}

    #fapi = open("data/APIs.txt")
    #defined_apis = fapi.readlines()
    #defined_apis = defined_apis[0].split(',')
    #fapi.close()
    
    pid = os.getpid()
    print('Process id:', pid)
    column_names_file = 'data/' + str(pid) + '-reduced-column-names.csv'  
    print('Column names file: {:s}'.format(column_names_file))
    graph_names_file = 'data/' + str(pid) + '-graph-names.csv'  
    print('Graph names file: {:s}'.format(graph_names_file))    

    with open(call_graph_file, 'r') as cfg:
        print("Starting graph file: {:s}".format(call_graph_file))
        for line in cfg:
            
            if line.startswith('digraph'):
                tokens = line.split()
                graph_name = tokens[1]
                graph_names.append(graph_name)
                continue
                
            line = line.rstrip('\r\n')  # get rid of newlines they are annoying.
            # get rid of all these things they are annoying.
            line = line.replace(';',' ').replace('{',' ').replace('}',' ').replace('->',' ')
            parts = line.split() # tokenize call graph line
            
            
            #graph_name = parts[0] # this is for single line call graphs.
            #parts = parts[1:]
            #graph_names.append(graph_name)
            #graph_functions = {}
            
            for func in parts:
                #if func not in defined_apis: # ignore these API functions, they have already been counted.
                if func.startswith('sub') or func.startswith('loc') or func.startswith('unk'):
                    func = func[:5] # lets try to reduce the vast number of functions.
                elif func.startswith('eax+') or func.startswith('ebx+') or func.startswith('ecx+') or func.startswith('edx+'):
                    func = func[:5]
                elif func.startswith('edi+') or func.startswith('esi+'):
                    func = func[:5]
                elif func.startswith('byte_') or func.startswith('word_'): # or func.startswith('nullsub')
                    func = func[:6]
                else: # reduce the feature set some more so my pissy pants PC can handle it.
                    func = func[:8]
                if func not in column_names: # NOTE: or in Defined APIs, these have already been counted.    
                    column_names.append(func)

 
            counter += 1
            # Print progress
            if ((counter + 1) % 1000) == 0:
                print("Processed number {:d} Graph_name {:s} Total column names {:d}".format(counter, graph_name, len(column_names)))       

                
    with open(column_names_file, 'w') as cols:
        fw = writer(cols)
        fw.writerow(column_names)
    
    print("Completed writing {:d} column names.".format(len(column_names)))

    with open(graph_names_file, 'w') as gras:
        fw = writer(gras)
        fw.writerow(graph_names)
    
    print("Completed writing {:d} graph names.".format(len(graph_names)))
    
    return



def merge_column_names_single_line():
    # Generate the merged column names file single line.
    counter = 0
    column_names = []
    column_name_files = ['data/3346-reduced-column-names.csv', 'data/3347-reduced-column-names.csv', 'data/3348-reduced-column-names.csv', 'data/3349-reduced-column-names.csv']
    for cnamefile in column_name_files:
        with open(cnamefile, 'r') as cras:
            print("Starting file: {:s}".format(cnamefile))
            colstr = cras.readline()
            colnames = colstr.split(',')
            for cname in colnames:
                if cname not in column_names:
                    column_names.append(cname)

                counter += 1
                # Print progress
                if ((counter + 1) % 1000) == 0:
                    print("Processed column names {:d}".format(counter))       

    with open('data/all-reduced-function-column-names.csv', 'w') as cols:
        fw = writer(cols)
        fw.writerow(column_names)

    print("Completed writing column names total = {:d}".format(len(column_names)))
    
    return


def merge_column_names_multi_line():
    #Generate the merged column names file multiline.
    counter = 0
    column_names = []
    column_name_files = ['data/3346-reduced-column-names.csv', 'data/3347-reduced-column-names.csv', 'data/3348-reduced-column-names.csv', 'data/3349-reduced-column-names.csv']
    for cnamefile in column_name_files:
        with open(cnamefile, 'r') as cras:
            print("Starting file: {:s}".format(cnamefile))
            colstr = cras.readline()
            colnames = colstr.split(',')
            for cname in colnames:
                if cname not in column_names:    
                    column_names.append(cname)

                counter += 1
                # Print progress
                if ((counter + 1) % 1000) == 0:
                    print("Processed column names {:d}".format(counter))       

    with open('data/all-reduced-function-column-names-multiline.csv', 'w') as cols:
        for cname in column_names:
            outline = cname + "\n"
            cols.write(outline)

    print("Completed writing column names total = {:d}".format(len(column_names)))
    
    return




def generate_function_counts(call_graph_file):
    # Generate function counts from graph files of the ASM malware samples.
    
    counter = 0
    error_count = 0
    
    #fapi = open("data/APIs.txt")
    #defined_apis = fapi.readlines()
    #defined_apis = defined_apis[0].split(',')
    #fapi.close()
    
    colf = open('data/all-reduced-function-column-names.csv', 'r')
    all_column_names = []
    column_lines = colf.readlines()
    for line in column_lines:
        all_column_names += line.split(',')
    col_names_len = len(all_column_names)
    colf.close()
    print("Column Names: {:d}".format(col_names_len))
    
    pid = os.getpid()
    print('Process id:', pid)
    feature_file_name = 'data/' + str(pid) + '-call-graph-reduced-function_counts.csv'  
    print('Call graph function counts file: {:s}'.format(feature_file_name))
    feature_file = open(feature_file_name, 'w')
    fw = writer(feature_file)
    
    call_graph_function_features = []
    
    with open(call_graph_file, 'r', errors='ignore') as cfg:
        for line in cfg:
            
            if line.startswith('digraph') or line.startswith(' }'):
                continue
                
            line.rstrip('\r\n')  # get rid of newlines they are annoying.
            # get rid of all these things they are annoying.
            line = line.replace(';',' ').replace('{',' ').replace('}',' ').replace('->',' ')
            parts = line.split() # tokenize graph line
            
            graph_name = parts[0]
            parts = parts[1:]
            function_dict = {}
            
            # now generate the function counts for this call graph
            
            for func in parts:
                #if func not in defined_apis: # ignore these API functions, they have already been counted.
                if func.startswith('sub') or func.startswith('loc') or func.startswith('unk'):
                    func = func[:5] # lets try to reduce the vast number of functions.
                elif func.startswith('eax+') or func.startswith('ebx+') or func.startswith('ecx+') or func.startswith('edx+'):
                    func = func[:5]
                elif func.startswith('edi+') or func.startswith('esi+'):
                    func = func[:5]
                elif func.startswith('byte_') or func.startswith('word_'): # or func.startswith('nullsub')
                    func = func[:6]
                else: # reduce the feature set some more so my pissy pants PC can handle it.
                    func = func[:8]

                if (func in function_dict):
                    function_dict[func] += 1
                else:
                    function_dict[func] = 1
            
            # now generate the output row for this call graph

            function_counts = [0] * col_names_len # zero everything because this is a sparse matrix
            for func in function_dict:
                for idx, cname in enumerate(all_column_names):
                    if func == cname:
                        function_counts[idx] = function_dict[func]
                        break
                
            call_graph_function_features.append([graph_name] + function_counts)
            
            # Print progress and write out rows
            counter += 1
            if ((counter + 1) % 100) == 0:
                print("{:d} Graph: {:s} Count: {:d}".format(pid, graph_name, counter))
                fw.writerows(call_graph_function_features)
                call_graph_function_features = []
                
        # Write remaining files
        if len(call_graph_function_features) > 0:
            fw.writerows(call_graph_function_features)
            call_graph_function_features = []  
    
    feature_file.close()
    
    print("Completed processing {:d} graphs.".format(counter))
    
    return










