# generate-pe-header-tokens.py
#
# Parse a bunch of PE Header dump files generated by objdump and
# extract keywords such section names, import DLLs and functions.
#
# Inputs : list of PE Header files.
#
# Outputs: pe-header-tokens.txt
#          pe-header-token-counts.csv
#          row format = [token_name, count]
#
# Author: Derek Chadwick
# Date  : 12/09/2016


import os
from csv import writer
import numpy as np
import pandas as pd
import re



def save_token_counts(token_counter_map, out_file):
    # Output the malware sample classification counts.
    fop = open(out_file, 'w')
    csv_wouter = writer(fop)
    cols = ['token_name','count'] # write out the column names.
    csv_wouter.writerow(cols)
    outlines = []
    sorted_keys = token_counter_map.keys()
    sorted_keys.sort()
    counter = 0
    for key in sorted_keys:
        outlines.append([key, token_counter_map[key]])
        counter += 1
        if (counter % 100) == 0: # write out some lines
            csv_wouter.writerows(outlines)
            outlines = []
            print("Processed token {:s} -> {:d}.".format(key, token_counter_map[key]))

    # Finish off.
    if (len(outlines) > 0):
        csv_wouter.writerows(outlines)
        outlines = []

    print("Completed writing {:d} token.".format(len(sorted_keys)))    
    fop.close()

    return


def get_token_count_map(token_df):
    # Read in the token count file and create a dict.
    token_dict = {}
    type_y = np.array(token_df['token_name'])
    
    for idx in range(token_df.shape[0]): # First fill the dict with the token counts
        token_dict[token_df.iloc[idx,0]] = token_df.iloc[idx,1]
        

    return token_dict


def generate_pe_tokens(file_list, out_token_file, out_count_file):

    psections = re.compile('(\w+):(\w+)/(\w+)[!.-/]+(\w+)')   # Pattern for section names.
    pdlls = re.compile('(\w+):(\w+)/(\w+)')                   # Pattern for import DLL names.
    pfunctions = re.compile('(\w+)\.(\w+)\.(\w+)[!./-](\w+)') # Pattern for import function names.
    
    token_counter_map = {}
    
    for idx, fname in enumerate(file_list):
        # first count the token type
        if token_val in token_counter_map.keys():
            token_counter_map[token_val] += 1
        else:
            token_counter_map[token_val] = 1


        m = pwd1.match(x_val)
        if m != None:
            token_val = m.group(2)
            continue
                                                              
        m = pwd2.match(x_val)
        if m != None:
            token_val = m.group(2) 
            continue
            
        m = pcav.match(x_val)
        if m != None:
            token_val = m.group(1)
            continue



    save_token_counts(token_count_map, out_count_file)
    
    return

    

# Start of script.

#TODO: parse command line options for input/output file names.

file_list = os.listdir('/opt/vs/train1hdr/')
generate_pe_tokens(file_list,'data/pe-header-tokens.txt','data/pe-header-token-counts.csv')



# End of Script