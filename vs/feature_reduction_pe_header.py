# feature_reduction_pe_header.py
#
# Read a PE/COFF header feature file and use chi-squared tests to remove features
# that are independent of the label. Features include PE section names, 
# imported DLL names, imported function names, exported function names etc.
#
# Output: sorted-pe-header-features.csv
#         row format = [file_name, [keyword list...]]
#
# Output: sorted-pe-header-features-reduced.csv
#         row format = [file_name, [keyword list...]]
#
#
#
# Author: Derek Chadwick
# Date  : 30/09/2016
#
# TODO: optimise and many many things

from multiprocessing import Pool
import os
from csv import writer
import numpy as np
import pandas as pd
import math
import scipy.misc
import array
import time as tm
import re
import subprocess as sub
import matplotlib.pyplot as plt
from sklearn.feature_selection import SelectKBest, SelectPercentile
from sklearn.feature_selection import chi2



def reduce_feature_set(feature_set_file, reduced_set_file):
    # Use chi2 tests to determine the 10% best features see (mmmc/feature-reduction-call-graphs.ipynb).
    # Ok, so we still have 100000+ features even after severely reducing the function name lengths.
    # This is a problem. Having to process such a huge sparse matrix requires a lot of memory.
    # Solution 1: rent an AWS server with plenty-o-ram. (costs money and requires high bandwidth for file transfer)
    # Solution 2: buy more RAM for my linux box. (costs money)
    # Solution 3: break the sparse matrix into smaller chunks and process individually. (Ok)
    # Solution 4: try the pandas sparse matrix data structure. (too slow)
    
    # -> Solution 3: slice the matrix into smaller chunks for processing.
    # the pandas spare matrix still takes too long, break up into 10 different feature sets and try again.
    
    # Procedure:
    # 1. Open the feature file and get the number of columns.
    # 2. Divide the number of columns by 10 to get the column subset length.
    # 3. Use pandas to load and sort each column subset.
    # 4. Do the chi2 tests to reduce each column subset by 10 percent.
    # 5. Recombine the column subsets.
    # 6. Perform the chi2 test again on the combined reduced feature set.
    # 7. Write out the final reduced feature set.
    
    # Open file.
    
    # Load column subset and sort.
    
    # Perform chi2 test to get 10% best features.
    
    onetenth = int(sorted_call_graph_function_train_1.shape[1]/10)
    startidx = 1 # skip the filename column
    endidx = onetenth
    for idx1 in range(1,10):
        print("Processing column set {:d} -> {:d}".format(startidx, endidx))
        X = sorted_call_graph_function_train_1.iloc[:,startidx:endidx]
        y = []
        train_names = sorted_train_labels['Id']
        for fname in sorted_call_graph_function_train_1['filename']:
            # print("Appending {:s}".format(fname))
            for idx2,fname2 in enumerate(sorted_train_labels['Id']):
                if (fname2 == fname):
                    y.append(sorted_train_labels.iloc[idx2,1])
                    break

        # Find the top 10 percent variance features.
        print(X.shape)
        print(len(y))
        fsp = SelectPercentile(chi2, 10)
        X_new_10 = fsp.fit_transform(X,y)
        selected_names = fsp.get_support(indices=True)
        selected_names = selected_names + 1
        data_trimmed = sorted_call_graph_function_train_1.iloc[:,selected_names]
        data_fnames = pd.DataFrame(sorted_call_graph_function_train_1['filename'])
        data_reduced = data_fnames.join(data_trimmed)
        # Write to file as we do not have enough memory.
        filename = "data/sorted-function-counts-" + str(idx1) + "-10perc.csv"
        print("Writing file: {:s}".format(filename))
        data_reduced.to_csv(filename, index=False)
        startidx = endidx
        endidx += onetenth


    # finish of the remaining columns
    print("Processing final column set {:d} -> {:d}".format(startidx, endidx))
    X = sorted_call_graph_function_train_1.iloc[:,startidx:]
    y = []
    train_names = sorted_train_labels['Id']
    for fname in sorted_call_graph_function_train_1['filename']:
        for idx1,fname2 in enumerate(sorted_train_labels['Id']):
            if (fname2 == fname):
                y.append(sorted_train_labels.iloc[idx1,1])
                break

    # Find the top 10 percent variance features.
    fsp = SelectPercentile(chi2, 10)
    X_new_10 = fsp.fit_transform(X,y)
    selected_names = fsp.get_support(indices=True)
    selected_names = selected_names + 1
    data_trimmed = sorted_call_graph_function_train_1.iloc[:,selected_names]
    data_fnames = pd.DataFrame(sorted_call_graph_function_train_1['filename'])
    data_reduced = data_fnames.join(data_trimmed)
    # Write to file as we do not have enough memory.
    filename = "data/sorted-function-counts-10-10perc.csv"
    data_reduced.to_csv(filename, index=False)

    return


def combine_reduced_feature_sets():
    # Now recombine the reduced sets and perform chi-squared tests again.
    fname = "data/sorted-function-counts-1-10perc.csv"
    reduced_function_counts = pd.read_csv(fname)
    for idx in range(2,11):
        fname = "data/sorted-function-counts-" + str(idx) + "-10perc.csv"
        print("Processing file: {:s}".format(fname))
        nextfc = pd.read_csv(fname)
        reduced_function_counts = pd.merge(reduced_function_counts, nextfc, on='filename')


    reduced_function_counts.head(20)
    
    return



# Start of Script.
ext_drive = '/opt/vs/'
feature_file = 'xxxx-pe-header-features-vs251.csv'
reduced_feature_file = 

feature_column_sets = [] # We will have 10 sets of columns, process each set then recombine the reduced sets.

# pd.read_csv(filename, usecols=[ integer list ])

# End of Script.

