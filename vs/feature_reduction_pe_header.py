# feature_reduction_pe_header.py
#
# Read a PE/COFF header feature file and use chi-squared tests to remove features
# that are independent of the label. Features include PE section names, 
# imported DLL names, imported function names, exported function names etc.
#
# Output: sorted-pe-header-features.csv
#         row format = [file_name, [keyword list...]]
#
# Output: sorted-pe-header-features-reduced.csv
#         row format = [file_name, [keyword list...]]
#
#
#
# Author: Derek Chadwick
# Date  : 30/09/2016
#
# TODO: optimise and many many things


import os
from csv import writer
import numpy as np
import pandas as pd
import math
import scipy.misc
import array
import re
from sklearn.feature_selection import SelectKBest, SelectPercentile
from sklearn.feature_selection import chi2



def reduce_feature_set(feature_set_file, train_label_file, token_file, reduced_set_file):
    # Use chi2 tests to determine the 10% best features see (mmmc/feature-reduction-call-graphs.ipynb).
    # Ok, so we still have 100000+ features even after severely reducing the function name lengths.
    # This is a problem. Having to process such a huge sparse matrix requires a lot of memory.
    # Solution 1: rent an AWS server with plenty-o-ram. (costs money and requires high bandwidth for file transfer)
    # Solution 2: buy more RAM for my linux box. (costs money)
    # Solution 3: break the sparse matrix into smaller chunks and process individually. (Ok)
    # Solution 4: try the pandas sparse matrix data structure. (too slow)
    
    # -> Solution 3: slice the matrix into smaller chunks for processing.
    # the pandas spare matrix still takes too long, break up into 10 different feature sets and try again.
    
    # Procedure:
    # 1. Open the PE header feature file.
    # 2. Open the PE header token file and get the number of column names.
    # 3. Divide the number of columns by 10 to get the column subset length.
    # 4. Load the malware label set.
    # 5. Use pandas to load and sort each column subset.
    # 6. Do the chi2 tests to reduce each column subset to 10 percent best features.
    # 7. Recombine the column subsets.
    # 8. Perform the chi2 test again on the combined reduced feature set.
    # 9. Write out the final reduced feature set to a csv file.
    
    # Open PE header token file and get a list of token names.
    hdr_pd = pd.read_csv('data/' + token_file)
    token_list = list(hdr_pd['token_name'])
    token_list_len = len(token_list)
    for idx, token in enumerate(token_list): # Clamp the token name length and demangle C++ names, they are annoying.
        token = token.replace('@','').replace('$','').replace('?','')
        if len(token) > 32:
            token_list[idx] = token[:32]
        else:
            token_list[idx] = token
    
    # Load training labels
    sorted_train_labels = pd.read_csv(train_label_file)

    # Load column subset and sort, then 
    # Perform chi2 test to get 10% best features.
    
    onetenth = int(toke_list_len / 10)
    startidx = 1 # skip the filename column
    endidx = onetenth

    for idx1 in range(0,10):
        print("Processing column set {:d} -> {:d}".format(startidx, endidx))
        column_numbers = [ 0 ] + [ startidx : endidx ]
        feature_subset = pd.read_csv(feature_set_file, usecols = column_numbers)
        
        # Sort the feature subset on file_name column.
        sorted_feature_subset = feature_subset.sort('file_name')
        
        X = feature_subset.iloc[1:] # skip the filename, get the family class label for this feature subset.
        y = []
        train_names = sorted_train_labels['family_label']
        for fname in feature_subset['file_name']:
            print("Appending {:s}".format(fname))
            for idx2, fname2 in enumerate(sorted_train_labels['family_label']):
                if (fname2 == fname):
                    y.append(sorted_train_labels.iloc[idx2, 4]) # Append the family class label.
                    break

        # Find the top 10 percent variance features.
        print("Subset shape: {:d} {:d}".format(X.shape[0], X.shape[1]))
        print("Length of y: {:d}".format(len(y)))
        
        # Now select the 10% best features for this feature subset.
        #fsp = SelectPercentile(chi2, 10)
        #X_new_10 = fsp.fit_transform(X,y)
        #selected_names = fsp.get_support(indices=True)
        #selected_names = selected_names + 1 # the column name indices start at 0 so add 1 to all.
        
        #data_trimmed = sorted_feature_subset.iloc[:,selected_names]
        #data_fnames = pd.DataFrame(sorted_feature_subset['filename'])
        #data_reduced = data_fnames.join(data_trimmed)
        
        # Write to file as we do not have enough memory.
        filename = "data/" + reduced_set_file + "-" + str(idx1) + "-10perc.csv"
        #data_reduced.to_csv(filename, index=False)
        print("Writing file: {:s}".format(filename))

        startidx = endidx
        endidx += onetenth


    return


def combine_reduced_feature_sets(reduced_feature_file_name):
    # Now recombine the reduced sets and perform chi-squared tests again.
    fname = "data/" + reduced_feature_file_name + "-" + str(idx1) + "-10perc.csv"
    reduced_function_counts = pd.read_csv(fname)
    for idx in range(2,11):
        fname = "data/sorted-function-counts-" + str(idx) + "-10perc.csv"
        print("Processing file: {:s}".format(fname))
        nextfc = pd.read_csv(fname)
        reduced_function_counts = pd.merge(reduced_function_counts, nextfc, on='file_name')


    reduced_function_counts.head(20)
    
    return



# Start of Script.
ext_drive = '/opt/vs/'
feature_file = 'pe-header-features-vs251.csv'
reduced_feature_file = 'pe-header-features-reduced-vs251.csv'
token_names_file = 'data/pe-header-tokens-vs251.txt'
training_labels_file = 'data/sorted-train-labels-vs251.txt'

reduce_feature_set(feature_file, training_labels_file, token_names_file, reduced_feature_file)

combine_reduced_feature_sets()

# End of Script.

