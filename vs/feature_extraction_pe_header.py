# feature_extraction_pe_header.py
#
# Read a list of PE/COFF header files dumped
# by objdump and extract feature sets from them.
# Features include PE section names, imported DLL names,
# imported function names, exported function names etc.
#
# Input : pe-header-tokens.csv 
#         row format = [token_name, count]
#         (Contains the section names, imported DLL names etc.)
#
#         and
#
#         A list of PE header files dumped by objdump.
#
# Output: sorted-pe-header-features.csv
#         row format = [file_name, [keyword list...]]
#
#
#
# Author: Derek Chadwick
# Date  : 13/09/2016
#
# TODO: optimise and many many things

from multiprocessing import Pool
import os
from csv import writer
import numpy as np
import pandas as pd
import math
import scipy.misc
import array
import time as tm
import re
import subprocess as sub

field_list = [ "Characteristics","Time/Date","Magic","MajorLinkerVersion","MinorLinkerVersion",
"SizeOfCode","SizeOfInitializedData","SizeOfUninitializedData","AddressOfEntryPoint",
"BaseOfCode","BaseOfData","ImageBase","SectionAlignment","FileAlignment",
"MajorOSystemVersion","MinorOSystemVersion","MajorImageVersion","MinorImageVersion",
"MajorSubsystemVersion","MinorSubsystemVersion","Win32Version",
"SizeOfImage","SizeOfHeaders","CheckSum","Subsystem","DllCharacteristics","SizeOfStackReserve",
"SizeOfStackCommit","SizeOfHeapReserve","SizeOfHeapCommit","LoaderFlags","NumberOfRvaAndSizes" ]

ptime = re.compile("Time/Date\w+(.+)") # Time/Date pattern for PE Header field.


def get_field_values(header_lines, field_list):

    field_vals = []
    
    for idx1 in range(0,44): # The PE header fields are the first 44 lines of the file.
        
        line = header_lines[idx1].rstrip()
        tokens = line.split('\t')
        
        for idx2, field_name in enumerate(field_list):
            
            if field_name in tokens:
                if field_name.startswith("Time"):
                    time_match = ptime_match(field_name)
                    if time_match != None:   
                        time_str = time_match.group(1) 
                        time_s = tm.strptime(time_str, "%a %b %d %H:%M:%S %Y") # Convert time string to epoch int.
                        time_epoch = tm.mktime(time_s)
                    else:
                        time_epoch = 0
                        
                    field_vals[idx2] = time_epoch

                else:
                    field_vals[idx2] = int(tokens[1], 16) # Convert the hex value of the field to int.
                
    return field_vals
                
                
def count_header_keywords(asm_code, keywords, klen):
    
    keywords_values = [0] * klen
    
    for row in asm_code:
        for i in range(klen):
            if keywords[i] in row:
                keywords_values[i] += 1
                break
                
    return keywords_values


def extract_header_features(multi_parameters):
    # 1. Get the feature file and token/keyword file names
    # 2. Create an array of token/keyword values.
    # 3. Iterate throught the PE header file list and counter the occurrence of the keywords in each file.

    pid = os.getpid()
    feature_file = 'data/' + str(pid) + "-" + multi_parameters.out_file  
    token_file = 'data/' + multi_parameters.token_file
    
    print('Process id: {:d} - Feature file: {:s} - Keyword file: {:s}'.format(pid, feature_file, token_file))

    hdr_pd = pd.read_csv('data/' + token_file)
    tokens = np.array(hdr_pd['token_name'])
    tlen = len(tokens)

    asm_files = [i for i in tfiles if '.pe.txt' in i]
    ftot = len(asm_files)
    
    feature_counts = []
    with open(feature_file, 'w') as f:

        fw = writer(f)
        
        for idx, fname in enumerate(asm_files):
            
            fasm = open(ext_drive + fname, 'r')
            content = fasm.readlines()
            fasm.close()
            
            field_vals = get_field_values(content, header_fields)
            keyword_vals = count_header_keywords(content, tokens, tlen)
            
            feature_counts.append([fname[0:fname.find('pe.txt')]] + field_vals + keyword_vals)   
            
            # Writing rows after every 10 files processed
            if (idx+1) % 10 == 0:
                print("{:d} - {:d} of {:d} files processed.",format(pid, idx + 1, ftot)
                fw.writerows(feature_counts)
                feature_counts = []
                
        # Writing remaining features
        if len(feature_counts) > 0:
            fw.writerows(feature_counts)
            feature_counts = []

    print("{:d} Completed processing {:d} PE header files.".format(pid, ftot))
                      
    return


def combine_feature_files(feature_file_name, token_file):
    # Function to combine the newly generated PE header feature files into one file:
    # 1. list data directory
    # 2. For each file in file list that matches (\d\d\d\d-pe-header-features.csv)
    # 3. Trim the filenames if necessary (should remove VirusShare_  prefix).
    # 4. Concatenate the unsorted pe header feature files.
    # 5. Sort and write to data/sorted-pe-header-features.csv
    
    hdr_pd = pd.read_csv('data/' + token_file)
    fop = open('data/' + feature_file_name,'w')
    colnames = np.array(hdr_pd['token_name'])
    fop.write(colnames)                    

    p1 = re.compile('\d{3,5}-' + feature_file_name) # This is the PID prefix for each file.
    file_list = os.listdir('data/')
    counter = 0
    
    for file_name in file_list:
        if p1.match(file_name):
            fip = open('data/' + file_name, 'r')
            in_lines = fip.readlines()
            fop.writelines(in_lines)
            counter += len(in_lines)
            fip.close()
            
    
    fop.close()
    
    features = pd.read_csv('data/' + feature_file_name)
    # DataFrame.sort() is deprecated, but this is an old version of pandas, does not have sort_values().
    sorted_features = features.sort('file_name')
    sorted_features.to_csv('data/sorted-' + feature_file_name, index=False)
    
    print('Completed combine of {:d} PE header file features.'.format(counter))  
    
    return


class Multi_Params(object):
    def __init__(self, outfile="", tokenfile="", fieldnames=[], filelist=[]):
        self.out_file = outfile
        self.token_file = tokenfile
        self.field_names = fieldnames
        self.file_list = filelist
        

# Start of script.

# TODO: add command line arguments to specify file names.

header_field_names = 'data/pe-coff-header-field-names.txt'
out_file = 'pe-header-features-apt.csv'
token_file = 'pe-header-tokens-apt.csv'
ext_drive = '/opt/vs/apthdr/'
tfiles = os.listdir(ext_drive)
                      
# Divide the train files into four groups for multiprocessing.

quart = len(tfiles)/4
train1 = tfiles[:quart]
train2 = tfiles[quart:(2*quart)]
train3 = tfiles[(2*quart):(3*quart)]
train4 = tfiles[(3*quart):]

print("Files: {:d} - {:d} - {:d}".format(len(tfiles), quart, (len(train1)+len(train2)+len(train3)+len(train4))))

mp1 = Multi_Params(out_file, token_file, field_names, train1)
mp2 = Multi_Params(out_file, token_file, field_names, train2)
mp3 = Multi_Params(out_file, token_file, field_names, train3)
mp4 = Multi_Params(out_file, token_file, field_names, train4)

trains = [mp1, mp2, mp3, mp4]
p = Pool(4)
p.map(extract_header_features, trains)

combine_feature_files(out_file)

# End of script.
