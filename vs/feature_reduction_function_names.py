# feature_reduction_function_names.py
#
# Read a a bunch of function name feature sets and use chi2 tests
# to remove features that are independent of the label.
#
# Input : function count feature sets in CSV files.
#         row format = [file_name, [list of function names]...]
#
# Output: all-reduced-function-counts.csv
#         row format = [file_name, [list of function names]...]
#
#
#
# Author: Derek Chadwick
# Date  : 14/11/2016
#
# TODO: optimise and many many things

import numpy as np
import pandas as pd
import graph as gra # http://www.python-course.eu/graphs_python.php
import os
from csv import writer
from multiprocessing import Pool



def get_function_column_names()
    # Preliminary column name setup.
    colf = open('data/all-reduced-function-column-names.csv', 'r')
    all_column_names = []
    column_lines = colf.readlines()
    for line in column_lines:
        all_column_names += line.split(',')

    col_names_len = len(all_column_names)
    colf.close()
    print("Column Names: {:d}".format(col_names_len))
    
    return all_column_names
    
    
    
#TODO: everything.

def reduce_function_names():
    # Solution 3: slice the matrix into small chunks for processing.
    # the pandas spare matrix still takes too long, break up into 10 different feature sets and try again.
    
    onetenth = int(sorted_call_graph_function_train_1.shape[1]/10)
    startidx = 1 # skip the filename column
    endidx = onetenth
    for idx1 in range(1,10):
        print("Processing column set {:d} -> {:d}".format(startidx, endidx))
        X = sorted_call_graph_function_train_1.iloc[:,startidx:endidx]
        y = []
        train_names = sorted_train_labels['Id']
        for fname in sorted_call_graph_function_train_1['filename']:
            # print("Appending {:s}".format(fname))
            for idx2,fname2 in enumerate(sorted_train_labels['Id']):
                if (fname2 == fname):
                    y.append(sorted_train_labels.iloc[idx2,1])
                    break

        # Find the top 10 percent variance features.
        print(X.shape)
        print(len(y))
        fsp = SelectPercentile(chi2, 10)
        X_new_10 = fsp.fit_transform(X,y)
        selected_names = fsp.get_support(indices=True)
        selected_names = selected_names + 1
        data_trimmed = sorted_call_graph_function_train_1.iloc[:,selected_names]
        data_fnames = pd.DataFrame(sorted_call_graph_function_train_1['filename'])
        data_reduced = data_fnames.join(data_trimmed)
        # Write to file as we do not have enough memory.
        filename = "data/sorted-function-counts-" + str(idx1) + "-10perc.csv"
        print("Writing file: {:s}".format(filename))
        data_reduced.to_csv(filename, index=False)
        startidx = endidx
        endidx += onetenth


    # finish of the remaining columns
    print("Processing final column set {:d} -> {:d}".format(startidx, endidx))
    X = sorted_call_graph_function_train_1.iloc[:,startidx:]
    y = []
    train_names = sorted_train_labels['Id']
    for fname in sorted_call_graph_function_train_1['filename']:
        for idx1,fname2 in enumerate(sorted_train_labels['Id']):
            if (fname2 == fname):
                y.append(sorted_train_labels.iloc[idx1,1])
                break



    # Find the top 10 percent variance features.
    fsp = SelectPercentile(chi2, 10)
    X_new_10 = fsp.fit_transform(X,y)
    selected_names = fsp.get_support(indices=True)
    selected_names = selected_names + 1
    data_trimmed = sorted_call_graph_function_train_1.iloc[:,selected_names]
    data_fnames = pd.DataFrame(sorted_call_graph_function_train_1['filename'])
    data_reduced = data_fnames.join(data_trimmed)
    # Write to file as we do not have enough memory.
    filename = "data/sorted-function-counts-10-10perc.csv"
    data_reduced.to_csv(filename, index=False)

    return


#TODO: everything.

def recombine_reduce_sets():
    # Now recombine the reduced sets and perform chi-squared tests again.
    fname = "data/sorted-function-counts-1-10perc.csv"
    reduced_function_counts = pd.read_csv(fname)
    for idx in range(2,11):
        fname = "data/sorted-function-counts-" + str(idx) + "-10perc.csv"
        print("Processing file: {:s}".format(fname))
        nextfc = pd.read_csv(fname)
        reduced_function_counts = pd.merge(reduced_function_counts, nextfc, on='filename')


    reduced_function_counts.head(20)

    return


def concatenate_reduced_feature_sets():
    # Now recombine the reduced sets and perform chi-squared tests again. Create a list of dataframes then
    # call the pd.concat funtion once as this is more efficient than calling the pd.concat function
    # multiple times.

    dflist = []
    for idx in range(1,5):
        fname = "data/reduced-fcounts-" + str(idx) + ".csv"
        print("Processing file: {:s}".format(fname))
        dflist.append(pd.read_csv(fname))


    reduced_function_counts = pd.concat(dflist, ignore_index=True)
    # Replace all the NaN values with 0
    reduced_function_counts.fillna(0, inplace=True)
    sorted_reduced_function_counts = reduced_function_counts.sort_values(by='filename')
    final_reduced_function_counts = sorted_call_graph_features_train.merge(sorted_reduced_function_counts, on='filename')
    final_reduced_function_counts.head(20)

    return


def get_top_10_percent_features():
    # Find the top 10 percent variance features.
    X = final_reduced_function_counts.iloc[:,1:]
    y = sorted_train_labels.iloc[:,1]
    fsp = SelectPercentile(chi2, 10)
    X_new_10 = fsp.fit_transform(X,y)
    selected_names = fsp.get_support(indices=True)
    selected_names = selected_names + 1
    data_trimmed = final_reduced_function_counts.iloc[:,selected_names]
    data_fnames = pd.DataFrame(final_reduced_function_counts['filename'])
    data_reduced = data_fnames.join(data_trimmed)
    # Write to file as we do not have enough memory.
    filename = "data/final-call-graph-features-10percent.csv"
    data_reduced.to_csv(filename, index=False)
    data_reduced.head(20)

    return



