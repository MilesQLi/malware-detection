{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Model Selection.\n",
    "    - XGBoost\n",
    "    - LightGBM (Microsoft)\n",
    "    - CNTK (Microsoft)\n",
    "    - Leaf (https://github.com/autumnai/leaf)\n",
    "    - ExtraTreesClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "/usr/local/lib/python2.7/dist-packages/sklearn/grid_search.py:43: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "#import sklearn as skl\n",
    "import matplotlib.pyplot as plt\n",
    "from time import time\n",
    "from scipy.stats import randint as sp_randint\n",
    "from sklearn.metrics import log_loss, confusion_matrix, accuracy_score, classification_report\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, GradientBoostingClassifier\n",
    "from sklearn.cross_validation import cross_val_score, KFold, train_test_split\n",
    "from sklearn.grid_search import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.linear_model import RidgeClassifierCV\n",
    "from sklearn.svm import SVC\n",
    "import seaborn as sns\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "%pylab inline\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_cv(X, y, clf, num_iters):\n",
    "\n",
    "    # Construct a kfolds object:\n",
    "    # For softprob prediction this will only work if the distribution of \n",
    "    # label values is even throughout each sub-sample, so only large sample\n",
    "    # sizes will generally work, small sample sizes with a large number of\n",
    "    # label values will generate errors when doing the softprob assignment\n",
    "    # to y_prob because the training set will likely have a different number\n",
    "    # of unique label values from the full sample set.\n",
    "    # In this case comment out the softprob assignment and y_prob return\n",
    "    # value.\n",
    "    \n",
    "    len_y = len(y)\n",
    "    num_labels = y.nunique()\n",
    "    \n",
    "    kf = KFold(len_y, n_folds=num_iters, shuffle=True)\n",
    "    y_prob = np.zeros((len_y, num_labels))\n",
    "    y_pred = np.zeros(len_y)\n",
    "    \n",
    "    # Iterate through folds\n",
    "    for train_index, test_index in kf:\n",
    "        print(test_index, train_index)\n",
    "        X_train = X.loc[train_index,:]\n",
    "        X_test = X.loc[test_index,:]\n",
    "        y_train = y[train_index]\n",
    "\n",
    "        clf.fit(X_train, y_train) # use flatten to get rid of data conversion warnings\n",
    "        \n",
    "        y_prob[test_index] = clf.predict_proba(X_test)\n",
    "        y_pred[test_index] = clf.predict(X_test)\n",
    "        #print(clf.get_params())\n",
    "    \n",
    "    return y_prob, y_pred\n",
    "\n",
    "\n",
    "def run_vs_cv(X, y, clf, num_iters):\n",
    "\n",
    "    # Construct a kfolds object:\n",
    "    # For softprob prediction this will only work if the distribution of \n",
    "    # label values is even throughout each sub-sample, so only large sample\n",
    "    # sizes will generally work, small sample sizes with a large number of\n",
    "    # label values will generate errors when doing the softprob assignment\n",
    "    # to y_prob because the training set will likely have a different number\n",
    "    # of unique label values from the full sample set.\n",
    "    # In this case comment out the softprob assignment and y_prob return\n",
    "    # value.\n",
    "    \n",
    "    len_y = len(y)\n",
    "    num_labels = y.nunique()\n",
    "    \n",
    "    kf = KFold(len_y, n_folds=num_iters, shuffle=True)\n",
    "    y_prob = np.zeros((len_y, num_labels))\n",
    "    y_pred = np.zeros(len_y)\n",
    "    \n",
    "    # Iterate through folds\n",
    "    for train_index, test_index in kf:\n",
    "        print(test_index, train_index)\n",
    "        X_train = X.loc[train_index,:]\n",
    "        X_test = X.loc[test_index,:]\n",
    "        y_train = y[train_index]\n",
    "\n",
    "        clf.fit(X_train, y_train) # use flatten to get rid of data conversion warnings\n",
    "        \n",
    "        #y_prob[test_index] = clf.predict_proba(X_test)\n",
    "        y_pred[test_index] = clf.predict(X_test)\n",
    "        #print(clf.get_params())\n",
    "    \n",
    "    return y_pred\n",
    "\n",
    "\n",
    "def write_confusion_matrix(cm, out_file_name):\n",
    "    #fop = open('data/confusion-matrix-apt.txt','w')\n",
    "    fop = open('data/' + out_file_name, 'w')\n",
    "    # this is rubbish ->  cm.tofile(fop, \",\")\n",
    "    for a_idx in range(0,cm.shape[0]):\n",
    "        #line = \",\".join(cm[a_idx])\n",
    "        line = ','.join(str(x) for x in cm[a_idx])\n",
    "        #print(\"{:d} -> {:s}\".format(a_idx, line))\n",
    "        fop.write(line + \"\\n\")\n",
    "        #for b_idx in cm.shape[1]:\n",
    "        #    line = line + \",\".acm[a_idx,b_idx]\n",
    "\n",
    "    fop.close()\n",
    "    \n",
    "    return\n",
    "\n",
    "\n",
    "def reduce_feature_set(X, y, out_filename):\n",
    "    # Find the top 10 percent variance features.\n",
    "    print(\"Sorted feature subset: {:d}\".format(idx))\n",
    "    print(\"Subset shape: {:d} {:d}\".format(X.shape[0], X.shape[1]))\n",
    "    print(\"Length of y: {:d}\".format(len(y)))\n",
    "    #sorted_feature_subset.head()\n",
    "\n",
    "    # Now select the 10% best features for this feature subset.\n",
    "    fsp = SelectPercentile(chi2, 10)\n",
    "    X_new_10 = fsp.fit_transform(X,y)\n",
    "    selected_names = fsp.get_support(indices=True)\n",
    "    selected_names = selected_names + 1 # the column name indices start at 0 so add 1 to all.\n",
    "\n",
    "    data_trimmed = sorted_feature_subset.iloc[:,selected_names]\n",
    "    data_fnames = pd.DataFrame(sorted_feature_subset['file_name'])\n",
    "    data_reduced = data_fnames.join(data_trimmed)\n",
    "\n",
    "    # Write to file as we do not have enough memory.\n",
    "    filename = \"data/\" + out_file_name\n",
    "    data_reduced.to_csv(filename, index=False)\n",
    "    #sorted_feature_subset['file_name'].to_csv(filename, index=False)\n",
    "    print(\"Writing file: {:s}\".format(filename))\n",
    "                                      \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 VirusShare 251 Feature Set Model Selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label\n",
       "0      6\n",
       "1      9\n",
       "2     10\n",
       "3     11\n",
       "4     13"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load in the combined feature set and training labels\n",
    "combined_train_features = pd.read_csv('data/combined-pe-features-vs251.csv')\n",
    "train_labels = pd.read_csv('data/sorted-pe-coff-train-labels-vs251.csv')\n",
    "train_labels.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_name</th>\n",
       "      <th>edx</th>\n",
       "      <th>esi</th>\n",
       "      <th>es</th>\n",
       "      <th>fs</th>\n",
       "      <th>ds</th>\n",
       "      <th>ss</th>\n",
       "      <th>gs</th>\n",
       "      <th>cs</th>\n",
       "      <th>ah</th>\n",
       "      <th>...</th>\n",
       "      <th>vertex_count</th>\n",
       "      <th>edge_count</th>\n",
       "      <th>delta_max</th>\n",
       "      <th>density</th>\n",
       "      <th>entropy</th>\n",
       "      <th>file_size</th>\n",
       "      <th>file_id</th>\n",
       "      <th>percentage</th>\n",
       "      <th>trid_id</th>\n",
       "      <th>packer_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0004376a62e22f6ad359467eb742b8ff</td>\n",
       "      <td>1219</td>\n",
       "      <td>837</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>34</td>\n",
       "      <td>...</td>\n",
       "      <td>465</td>\n",
       "      <td>740</td>\n",
       "      <td>57</td>\n",
       "      <td>0.022321</td>\n",
       "      <td>0.803515</td>\n",
       "      <td>149720</td>\n",
       "      <td>1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000634f03457d088c71dbffb897b1315</td>\n",
       "      <td>15109</td>\n",
       "      <td>23202</td>\n",
       "      <td>13</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>68</td>\n",
       "      <td>...</td>\n",
       "      <td>5099</td>\n",
       "      <td>14671</td>\n",
       "      <td>633</td>\n",
       "      <td>0.006351</td>\n",
       "      <td>0.957584</td>\n",
       "      <td>1725502</td>\n",
       "      <td>1</td>\n",
       "      <td>4.6</td>\n",
       "      <td>21</td>\n",
       "      <td>1101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00072ed24314e91b63b425b3dc572f50</td>\n",
       "      <td>2528</td>\n",
       "      <td>2137</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2460</td>\n",
       "      <td>2458</td>\n",
       "      <td>2058</td>\n",
       "      <td>409.666667</td>\n",
       "      <td>0.486112</td>\n",
       "      <td>328093</td>\n",
       "      <td>1</td>\n",
       "      <td>4.4</td>\n",
       "      <td>5</td>\n",
       "      <td>1060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00092d369958b67557da8661cc9093bc</td>\n",
       "      <td>11363</td>\n",
       "      <td>12611</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>53</td>\n",
       "      <td>...</td>\n",
       "      <td>4811</td>\n",
       "      <td>11668</td>\n",
       "      <td>308</td>\n",
       "      <td>0.007287</td>\n",
       "      <td>0.845657</td>\n",
       "      <td>522936</td>\n",
       "      <td>1</td>\n",
       "      <td>2.2</td>\n",
       "      <td>1</td>\n",
       "      <td>1101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0009a64f786fa29bfa6423278cc74f02</td>\n",
       "      <td>202</td>\n",
       "      <td>883</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>547</td>\n",
       "      <td>881</td>\n",
       "      <td>289</td>\n",
       "      <td>0.201556</td>\n",
       "      <td>0.996663</td>\n",
       "      <td>671280</td>\n",
       "      <td>1</td>\n",
       "      <td>7.4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 241 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                          file_name    edx    esi  es  fs  ds  ss  gs  cs  ah  \\\n",
       "0  0004376a62e22f6ad359467eb742b8ff   1219    837   0   0   0   0   0   0  34   \n",
       "1  000634f03457d088c71dbffb897b1315  15109  23202  13   3   3   3   3   3  68   \n",
       "2  00072ed24314e91b63b425b3dc572f50   2528   2137   1   0   0   0   0   0   0   \n",
       "3  00092d369958b67557da8661cc9093bc  11363  12611   3   3   7   7   3   4  53   \n",
       "4  0009a64f786fa29bfa6423278cc74f02    202    883   0   0   0   0   0   0   0   \n",
       "\n",
       "     ...      vertex_count  edge_count  delta_max     density   entropy  \\\n",
       "0    ...               465         740         57    0.022321  0.803515   \n",
       "1    ...              5099       14671        633    0.006351  0.957584   \n",
       "2    ...              2460        2458       2058  409.666667  0.486112   \n",
       "3    ...              4811       11668        308    0.007287  0.845657   \n",
       "4    ...               547         881        289    0.201556  0.996663   \n",
       "\n",
       "   file_size  file_id  percentage  trid_id  packer_id  \n",
       "0     149720        1         3.5       13          0  \n",
       "1    1725502        1         4.6       21       1101  \n",
       "2     328093        1         4.4        5       1060  \n",
       "3     522936        1         2.2        1       1101  \n",
       "4     671280        1         7.4        1          0  \n",
       "\n",
       "[5 rows x 241 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_train_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of y: 54911\n",
      "Shape of X: 54911 240\n"
     ]
    }
   ],
   "source": [
    "# X,y = get_training_data(combined_train_features, sorted_train_labels, 'data/sorted-pe-coff-train-labels-vs251.csv')\n",
    "X = combined_train_features.iloc[:,1:]\n",
    "y = train_labels['label']\n",
    "print(\"Length of y: {:d}\".format(len(y)))\n",
    "print(\"Shape of X: {:d} {:d}\".format(X.shape[0], X.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.series.Series"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### ExtraTreesClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Getting memory errors :-(\n",
    "# GridSearchCV needs more memberberries.\n",
    "\n",
    "clfextra = ExtraTreesClassifier(n_jobs=4)\n",
    "\n",
    "# use a full grid over all parameters, most important parameters are n_estimators (larger is better) and\n",
    "# max_features (for classification best value is square root of the number of features)\n",
    "# Reference: http://scikit-learn.org/stable/modules/ensemble.html\n",
    "param_grid = {\"n_estimators\": [1000, 2000],\n",
    "              \"max_depth\": [3, None],\n",
    "              \"max_features\": [1, 2],\n",
    "              \"min_samples_split\": [1, 3, 10],\n",
    "              \"min_samples_leaf\": [1, 3, 10],\n",
    "              \"bootstrap\": [True, False],\n",
    "              \"criterion\": [\"gini\", \"entropy\"]}\n",
    "\n",
    "# run grid search\n",
    "grid_search = GridSearchCV(clfextra, param_grid=param_grid)\n",
    "start = time()\n",
    "grid_search.fit(X, y)\n",
    "\n",
    "print(\"ExtraTreesClassifier - GridSearchCV:\")\n",
    "print(\" \")\n",
    "print(\"Best parameters set found on training set:\")\n",
    "print(\" \")\n",
    "print(grid_search.best_params_)\n",
    "print(\" \")\n",
    "#print(\"Grid scores on training set:\")\n",
    "#print(\" \")\n",
    "#report(grid_search.grid_scores_)\n",
    "#print(\" \")\n",
    "print(\"Classification report:\")\n",
    "print(\"GridSearchCV took {:.2f} seconds.\".format((time() - start)))\n",
    "print(\" \")\n",
    "y_pred = grid_search.predict(X)\n",
    "print(classification_report(y, y_pred))\n",
    "print(\" \")\n",
    "y_prob = grid_search.predict_proba(X)\n",
    "print(\"logloss = {:.3f}\".format(log_loss(y, y_prob)))\n",
    "print(\"score = {:.3f}\".format(accuracy_score(y, y_pred)))\n",
    "cm = confusion_matrix(y, y_pred)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([    7,    20,    30, ..., 54897, 54903, 54907]), array([    0,     1,     2, ..., 54908, 54909, 54910]))\n",
      "(array([    4,    14,    23, ..., 54898, 54901, 54906]), array([    0,     1,     2, ..., 54908, 54909, 54910]))\n",
      "(array([    0,     8,     9, ..., 54866, 54868, 54874]), array([    1,     2,     3, ..., 54908, 54909, 54910]))\n",
      "(array([   10,    29,    42, ..., 54848, 54873, 54889]), array([    0,     1,     2, ..., 54908, 54909, 54910]))\n",
      "(array([   16,    19,    32, ..., 54886, 54904, 54909]), array([    0,     1,     2, ..., 54907, 54908, 54910]))\n",
      "(array([   12,    17,    27, ..., 54895, 54902, 54905]), array([    0,     1,     2, ..., 54908, 54909, 54910]))\n",
      "(array([    1,     5,    18, ..., 54891, 54893, 54908]), array([    0,     2,     3, ..., 54907, 54909, 54910]))\n",
      "(array([    2,     6,    24, ..., 54888, 54892, 54899]), array([    0,     1,     3, ..., 54908, 54909, 54910]))\n",
      "(array([   46,    52,    60, ..., 54894, 54900, 54910]), array([    0,     1,     2, ..., 54907, 54908, 54909]))\n",
      "(array([    3,    11,    13, ..., 54869, 54875, 54890]), array([    0,     1,     2, ..., 54908, 54909, 54910]))\n",
      " \n",
      "Classification report:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.71      0.91      0.80      5079\n",
      "          1       0.83      0.89      0.86       197\n",
      "          2       1.00      1.00      1.00       158\n",
      "          3       1.00      0.92      0.96        12\n",
      "          5       1.00      1.00      1.00       181\n",
      "          6       1.00      1.00      1.00       960\n",
      "          8       1.00      1.00      1.00       222\n",
      "          9       0.81      0.74      0.77        78\n",
      "         10       0.67      0.91      0.77       400\n",
      "         11       1.00      1.00      1.00      3779\n",
      "         12       0.97      0.99      0.98       699\n",
      "         13       0.00      0.00      0.00         2\n",
      "         14       0.91      0.92      0.92       276\n",
      "         15       0.96      0.95      0.95       998\n",
      "         16       0.75      1.00      0.86         6\n",
      "         17       0.86      0.67      0.75        18\n",
      "         19       0.99      1.00      1.00      1938\n",
      "         20       0.94      0.94      0.94       450\n",
      "         21       1.00      1.00      1.00       768\n",
      "         22       1.00      0.83      0.91         6\n",
      "         23       0.98      0.99      0.98      3316\n",
      "         24       0.96      1.00      0.98       287\n",
      "         25       0.99      0.98      0.99       295\n",
      "         26       0.49      0.59      0.54      1084\n",
      "         27       0.79      0.81      0.80       391\n",
      "         29       1.00      1.00      1.00      6134\n",
      "         30       0.95      0.73      0.83        49\n",
      "         31       1.00      1.00      1.00      1548\n",
      "         33       0.54      0.36      0.43        73\n",
      "         34       0.50      0.25      0.33         8\n",
      "         35       1.00      1.00      1.00       226\n",
      "         36       0.90      1.00      0.95        18\n",
      "         37       0.97      0.97      0.97       259\n",
      "         38       1.00      1.00      1.00       516\n",
      "         39       0.94      0.89      0.91       148\n",
      "         40       0.83      1.00      0.91        10\n",
      "         41       0.83      0.62      0.71       251\n",
      "         42       0.97      0.91      0.94        33\n",
      "         43       1.00      0.22      0.36        18\n",
      "         44       0.00      0.00      0.00         1\n",
      "         45       1.00      1.00      1.00       311\n",
      "         46       1.00      1.00      1.00       204\n",
      "         47       0.00      0.00      0.00         1\n",
      "         48       0.68      0.43      0.53        65\n",
      "         49       0.67      0.70      0.68        23\n",
      "         50       0.00      0.00      0.00         4\n",
      "         51       0.94      0.99      0.96       144\n",
      "         52       0.93      1.00      0.96        53\n",
      "         53       0.67      0.20      0.31        10\n",
      "         54       0.97      0.97      0.97        37\n",
      "         55       1.00      1.00      1.00         3\n",
      "         56       0.97      0.97      0.97        94\n",
      "         57       1.00      0.75      0.86         4\n",
      "         58       0.50      0.27      0.35        93\n",
      "         59       0.89      0.94      0.92        18\n",
      "         60       0.00      0.00      0.00         3\n",
      "         61       0.47      0.22      0.30       249\n",
      "         62       1.00      0.50      0.67         4\n",
      "         63       0.00      0.00      0.00         1\n",
      "         64       0.91      0.81      0.86        37\n",
      "         66       0.00      0.00      0.00         1\n",
      "         67       1.00      1.00      1.00        12\n",
      "         68       0.86      0.92      0.89        26\n",
      "         70       0.00      0.00      0.00         4\n",
      "         71       1.00      0.80      0.89         5\n",
      "         72       0.67      0.46      0.55        13\n",
      "         73       0.99      1.00      1.00      3767\n",
      "         75       1.00      1.00      1.00       275\n",
      "         76       1.00      1.00      1.00       313\n",
      "         77       0.62      0.43      0.51        58\n",
      "         78       1.00      1.00      1.00      4035\n",
      "         79       0.88      0.95      0.91        39\n",
      "         80       0.81      0.46      0.59        28\n",
      "         81       0.84      0.82      0.83        44\n",
      "         82       0.98      1.00      0.99        56\n",
      "         83       0.99      1.00      0.99       218\n",
      "         84       0.00      0.00      0.00         1\n",
      "         85       0.00      0.00      0.00         2\n",
      "         86       0.00      0.00      0.00         1\n",
      "         88       0.85      0.85      0.85        67\n",
      "         90       0.88      0.92      0.90       116\n",
      "         91       0.24      0.11      0.15        38\n",
      "         92       1.00      1.00      1.00         7\n",
      "         94       0.86      0.96      0.91       118\n",
      "         95       0.94      0.97      0.96       117\n",
      "         96       0.69      0.65      0.67       175\n",
      "         97       0.63      0.53      0.57        59\n",
      "         99       0.97      0.95      0.96       112\n",
      "        100       0.82      0.88      0.85        16\n",
      "        101       0.89      0.96      0.93        26\n",
      "        102       0.50      0.09      0.15        11\n",
      "        103       0.75      0.90      0.82        10\n",
      "        104       0.95      1.00      0.97        19\n",
      "        105       0.00      0.00      0.00         6\n",
      "        106       0.00      0.00      0.00        29\n",
      "        107       0.82      0.89      0.85        98\n",
      "        108       0.55      0.67      0.60         9\n",
      "        110       0.97      0.97      0.97        37\n",
      "        111       0.69      0.52      0.59        42\n",
      "        112       0.00      0.00      0.00         1\n",
      "        113       1.00      1.00      1.00        32\n",
      "        114       0.82      0.60      0.69        15\n",
      "        115       0.71      0.79      0.75        80\n",
      "        116       1.00      0.83      0.91         6\n",
      "        119       0.31      0.25      0.28        20\n",
      "        121       0.67      0.18      0.29        11\n",
      "        122       0.00      0.00      0.00         6\n",
      "        123       1.00      0.33      0.50         6\n",
      "        124       1.00      1.00      1.00       194\n",
      "        125       0.98      1.00      0.99        60\n",
      "        126       0.90      0.82      0.85        65\n",
      "        127       0.52      0.57      0.55       135\n",
      "        128       0.77      0.64      0.70       129\n",
      "        130       0.00      0.00      0.00         1\n",
      "        131       0.94      1.00      0.97        30\n",
      "        132       0.00      0.00      0.00         3\n",
      "        133       0.94      0.94      0.94        32\n",
      "        134       0.74      0.65      0.70       150\n",
      "        135       0.88      1.00      0.93        14\n",
      "        136       0.00      0.00      0.00        10\n",
      "        137       1.00      0.99      0.99       156\n",
      "        138       1.00      1.00      1.00        16\n",
      "        139       0.50      0.22      0.31         9\n",
      "        140       0.97      0.99      0.98       264\n",
      "        141       1.00      1.00      1.00         3\n",
      "        142       1.00      0.50      0.67         2\n",
      "        143       0.67      0.67      0.67         3\n",
      "        144       0.71      0.76      0.74        42\n",
      "        145       0.83      1.00      0.91         5\n",
      "        146       0.73      0.58      0.65       100\n",
      "        147       0.92      0.98      0.95        46\n",
      "        148       1.00      1.00      1.00         5\n",
      "        149       1.00      0.93      0.97        15\n",
      "        150       1.00      1.00      1.00        23\n",
      "        151       0.00      0.00      0.00         4\n",
      "        152       0.52      0.41      0.46       135\n",
      "        153       0.91      0.94      0.92        32\n",
      "        154       0.91      0.95      0.93        64\n",
      "        155       1.00      1.00      1.00        91\n",
      "        156       0.93      1.00      0.96        27\n",
      "        157       0.00      0.00      0.00         1\n",
      "        158       0.52      0.61      0.56        23\n",
      "        159       1.00      0.67      0.80         6\n",
      "        160       0.50      0.80      0.62         5\n",
      "        161       0.91      0.82      0.86        87\n",
      "        163       0.97      0.94      0.95       132\n",
      "        164       1.00      0.92      0.96        51\n",
      "        165       0.93      0.90      0.92       171\n",
      "        166       0.00      0.00      0.00         1\n",
      "        167       0.77      0.89      0.83        27\n",
      "        168       0.69      0.69      0.69        16\n",
      "        169       0.00      0.00      0.00         3\n",
      "        170       1.00      0.80      0.89        10\n",
      "        171       1.00      0.57      0.73         7\n",
      "        172       0.80      0.75      0.78        44\n",
      "        173       0.92      1.00      0.96        24\n",
      "        174       0.97      0.97      0.97       156\n",
      "        175       1.00      1.00      1.00         3\n",
      "        176       0.76      0.84      0.80        50\n",
      "        177       0.90      0.95      0.92        57\n",
      "        178       0.95      0.95      0.95        19\n",
      "        179       0.00      0.00      0.00         1\n",
      "        180       1.00      0.60      0.75         5\n",
      "        181       0.44      0.17      0.25        23\n",
      "        182       0.00      0.00      0.00         1\n",
      "        183       0.38      0.21      0.27        14\n",
      "        184       0.76      0.78      0.77        37\n",
      "        185       0.53      0.46      0.49        57\n",
      "        186       0.00      0.00      0.00         2\n",
      "        187       1.00      1.00      1.00       172\n",
      "        188       0.98      1.00      0.99        59\n",
      "        189       0.00      0.00      0.00         2\n",
      "        190       0.00      0.00      0.00         1\n",
      "        191       0.94      0.93      0.93       162\n",
      "        192       1.00      0.12      0.21        17\n",
      "        193       0.81      0.77      0.79        22\n",
      "        196       0.43      0.43      0.43         7\n",
      "        197       0.00      0.00      0.00         4\n",
      "        198       0.83      0.42      0.56        12\n",
      "        199       1.00      0.33      0.50         3\n",
      "        200       0.77      0.69      0.73        39\n",
      "        201       0.31      0.14      0.19        78\n",
      "        203       1.00      0.22      0.36         9\n",
      "        204       0.71      0.62      0.67         8\n",
      "        205       1.00      1.00      1.00        33\n",
      "        207       0.00      0.00      0.00         2\n",
      "        208       0.71      0.48      0.57        25\n",
      "        209       0.00      0.00      0.00         1\n",
      "        210       0.67      0.67      0.67        24\n",
      "        211       0.52      0.65      0.58        49\n",
      "        212       0.83      0.62      0.71         8\n",
      "        213       0.94      1.00      0.97        16\n",
      "        214       0.93      1.00      0.96        39\n",
      "        215       0.93      0.87      0.90        15\n",
      "        216       0.81      0.94      0.87        18\n",
      "        217       0.00      0.00      0.00         1\n",
      "        218       0.98      1.00      0.99        82\n",
      "        219       0.76      0.45      0.57        29\n",
      "        221       0.67      0.11      0.18        19\n",
      "        222       0.00      0.00      0.00         1\n",
      "        223       0.65      0.72      0.68        46\n",
      "        224       0.70      1.00      0.82         7\n",
      "        225       1.00      1.00      1.00       197\n",
      "        226       0.88      0.91      0.89        46\n",
      "        228       0.97      0.92      0.95        39\n",
      "        229       0.98      1.00      0.99        43\n",
      "        230       0.86      0.64      0.73        28\n",
      "        231       0.87      0.81      0.84        16\n",
      "        232       1.00      1.00      1.00        12\n",
      "        233       0.83      1.00      0.91         5\n",
      "        234       1.00      0.40      0.57         5\n",
      "        235       0.88      0.82      0.85        34\n",
      "        237       1.00      0.73      0.85        15\n",
      "        239       0.75      0.75      0.75         4\n",
      "        240       0.78      0.85      0.82        34\n",
      "        241       0.84      0.68      0.75       183\n",
      "        244       0.90      1.00      0.95        18\n",
      "        245       0.00      0.00      0.00         1\n",
      "        248       0.83      0.71      0.77        14\n",
      "        249       0.93      0.93      0.93        15\n",
      "        250       0.20      0.18      0.19        17\n",
      "        251       0.98      0.92      0.95        49\n",
      "        254       0.00      0.00      0.00         5\n",
      "        255       0.00      0.00      0.00         2\n",
      "        256       0.97      1.00      0.99       147\n",
      "        257       0.67      1.00      0.80         4\n",
      "        258       0.00      0.00      0.00         4\n",
      "        259       0.79      0.80      0.80        71\n",
      "        260       0.00      0.00      0.00         1\n",
      "        261       0.48      0.47      0.48        34\n",
      "        262       0.85      0.77      0.81        92\n",
      "        264       0.00      0.00      0.00         2\n",
      "        265       0.83      0.77      0.80        74\n",
      "        266       0.91      1.00      0.95        10\n",
      "        268       0.58      0.73      0.64        26\n",
      "        269       1.00      0.15      0.27        13\n",
      "        270       0.98      0.97      0.98        62\n",
      "        271       0.64      0.33      0.44        21\n",
      "        272       0.99      1.00      0.99        70\n",
      "        273       0.00      0.00      0.00         3\n",
      "        274       1.00      1.00      1.00         2\n",
      "        275       0.00      0.00      0.00         1\n",
      "        276       0.00      0.00      0.00         1\n",
      "        277       0.98      0.98      0.98       178\n",
      "        278       1.00      0.75      0.86         4\n",
      "        280       0.50      0.33      0.40         3\n",
      "        281       0.09      0.03      0.05        33\n",
      "        282       0.00      0.00      0.00         7\n",
      "        285       0.00      0.00      0.00         1\n",
      "        286       0.89      0.94      0.91        17\n",
      "        288       0.75      1.00      0.86        12\n",
      "        289       0.00      0.00      0.00         8\n",
      "        290       1.00      0.40      0.57         5\n",
      "        291       0.94      1.00      0.97        30\n",
      "        292       0.94      0.76      0.84        21\n",
      "        293       0.00      0.00      0.00         1\n",
      "        295       0.65      0.62      0.63        53\n",
      "        297       0.00      0.00      0.00         3\n",
      "        298       0.00      0.00      0.00         1\n",
      "        299       0.80      0.67      0.73        12\n",
      "        300       0.50      0.33      0.40         3\n",
      "        301       0.80      0.80      0.80         5\n",
      "        303       0.71      0.83      0.77        12\n",
      "        304       1.00      1.00      1.00         2\n",
      "        305       0.65      1.00      0.79        11\n",
      "        306       0.67      0.20      0.31        10\n",
      "        307       0.00      0.00      0.00         2\n",
      "        308       0.97      0.95      0.96        91\n",
      "        309       0.00      0.00      0.00         3\n",
      "        310       0.89      0.89      0.89        18\n",
      "        311       0.75      0.82      0.78        11\n",
      "        313       0.00      0.00      0.00         1\n",
      "        314       0.00      0.00      0.00         1\n",
      "        315       0.77      0.45      0.57        22\n",
      "        316       0.73      0.79      0.76        14\n",
      "        317       0.75      1.00      0.86        12\n",
      "        318       0.90      0.88      0.89        49\n",
      "        319       0.00      0.00      0.00         1\n",
      "        320       0.67      0.86      0.75         7\n",
      "        321       0.73      0.92      0.81        12\n",
      "        322       1.00      1.00      1.00         9\n",
      "        323       1.00      0.50      0.67         6\n",
      "        324       1.00      1.00      1.00        36\n",
      "        325       0.92      0.92      0.92        36\n",
      "        326       0.00      0.00      0.00         4\n",
      "        327       0.94      1.00      0.97        17\n",
      "        328       0.90      0.69      0.78        13\n",
      "        329       0.79      0.73      0.76        15\n",
      "        330       0.98      1.00      0.99        41\n",
      "        331       1.00      1.00      1.00        14\n",
      "        333       0.44      0.44      0.44         9\n",
      "        334       1.00      0.75      0.86         4\n",
      "        336       0.52      0.41      0.45        37\n",
      "        337       0.82      1.00      0.90         9\n",
      "        338       0.00      0.00      0.00         1\n",
      "        340       0.00      0.00      0.00         1\n",
      "        341       0.00      0.00      0.00         3\n",
      "        342       0.99      1.00      1.00      2214\n",
      "        344       0.25      0.20      0.22         5\n",
      "        346       0.86      0.78      0.82        23\n",
      "        347       0.81      0.80      0.80        49\n",
      "        348       0.85      1.00      0.92        11\n",
      "        349       0.00      0.00      0.00         1\n",
      "        351       0.94      0.80      0.86        20\n",
      "        352       0.79      0.69      0.74        49\n",
      "        353       1.00      1.00      1.00        24\n",
      "        355       0.00      0.00      0.00         4\n",
      "        356       1.00      0.50      0.67         4\n",
      "        358       0.00      0.00      0.00         5\n",
      "        359       1.00      0.94      0.97        33\n",
      "        360       0.67      0.50      0.57         8\n",
      "        361       0.00      0.00      0.00         4\n",
      "        362       0.50      1.00      0.67         4\n",
      "        363       0.96      0.71      0.82        35\n",
      "        364       0.00      0.00      0.00         1\n",
      "        365       1.00      1.00      1.00        80\n",
      "        367       1.00      1.00      1.00         2\n",
      "        369       0.00      0.00      0.00         4\n",
      "        370       0.88      0.96      0.92        23\n",
      "        372       0.00      0.00      0.00         1\n",
      "        374       1.00      1.00      1.00         7\n",
      "        375       0.00      0.00      0.00         1\n",
      "        376       0.00      0.00      0.00        15\n",
      "        377       1.00      0.42      0.59        12\n",
      "        378       0.00      0.00      0.00         3\n",
      "        381       0.71      0.83      0.77         6\n",
      "        382       0.88      0.97      0.92        30\n",
      "        383       1.00      0.50      0.67         6\n",
      "        384       0.00      0.00      0.00         1\n",
      "        385       0.50      0.20      0.29         5\n",
      "        386       0.67      0.67      0.67         6\n",
      "        388       0.73      1.00      0.84         8\n",
      "        389       1.00      1.00      1.00        79\n",
      "        392       1.00      1.00      1.00        50\n",
      "        393       0.00      0.00      0.00         1\n",
      "        394       1.00      1.00      1.00        22\n",
      "        396       1.00      1.00      1.00         4\n",
      "        397       0.71      0.48      0.57        21\n",
      "        399       0.78      0.88      0.82         8\n",
      "        400       0.96      1.00      0.98        22\n",
      "        401       0.00      0.00      0.00         1\n",
      "        402       0.64      0.58      0.61        12\n",
      "        403       0.83      1.00      0.91         5\n",
      "        404       0.00      0.00      0.00         1\n",
      "        405       1.00      0.88      0.93         8\n",
      "        406       0.00      0.00      0.00         1\n",
      "        407       0.00      0.00      0.00         1\n",
      "        408       0.00      0.00      0.00         2\n",
      "        409       0.92      0.92      0.92        12\n",
      "        410       0.89      0.94      0.92        18\n",
      "        411       0.00      0.00      0.00         2\n",
      "        412       1.00      0.67      0.80         3\n",
      "        413       0.67      0.80      0.73        10\n",
      "        414       0.00      0.00      0.00         7\n",
      "        415       0.89      0.67      0.76        12\n",
      "        416       0.00      0.00      0.00         1\n",
      "        417       1.00      1.00      1.00       218\n",
      "        419       1.00      1.00      1.00         2\n",
      "        420       0.00      0.00      0.00         1\n",
      "        421       0.79      0.62      0.70        24\n",
      "        422       0.43      0.38      0.40         8\n",
      "        423       0.00      0.00      0.00         4\n",
      "        426       1.00      1.00      1.00         8\n",
      "        429       0.91      1.00      0.95        10\n",
      "        430       0.50      0.33      0.40         3\n",
      "        431       0.41      0.29      0.34        24\n",
      "        432       0.67      0.86      0.75         7\n",
      "        433       0.91      1.00      0.95        21\n",
      "        434       1.00      0.93      0.96        14\n",
      "        435       0.75      0.36      0.49        25\n",
      "        438       0.00      0.00      0.00         1\n",
      "        439       1.00      1.00      1.00        11\n",
      "        441       1.00      0.67      0.80         3\n",
      "        442       1.00      1.00      1.00        27\n",
      "        443       0.75      0.75      0.75         4\n",
      "        444       0.67      0.33      0.44         6\n",
      "        445       0.58      0.47      0.52        15\n",
      "        446       0.27      0.30      0.29        10\n",
      "        447       0.89      0.67      0.76        12\n",
      "        448       0.75      1.00      0.86         3\n",
      "        450       0.00      0.00      0.00         3\n",
      "        451       0.95      0.90      0.93        21\n",
      "        452       0.00      0.00      0.00         3\n",
      "        454       0.00      0.00      0.00         1\n",
      "        455       0.00      0.00      0.00         2\n",
      "        456       0.00      0.00      0.00         1\n",
      "        459       0.00      0.00      0.00         1\n",
      "        460       1.00      0.50      0.67         2\n",
      "        461       0.00      0.00      0.00         1\n",
      "        462       0.94      1.00      0.97        15\n",
      "        463       0.94      0.94      0.94        16\n",
      "        464       1.00      0.30      0.46        10\n",
      "        465       0.00      0.00      0.00         2\n",
      "        466       1.00      1.00      1.00         7\n",
      "        467       1.00      0.25      0.40         4\n",
      "        469       0.67      0.67      0.67         3\n",
      "        470       0.00      0.00      0.00         1\n",
      "        471       1.00      0.60      0.75         5\n",
      "        473       0.00      0.00      0.00         1\n",
      "        474       1.00      1.00      1.00        22\n",
      "        475       0.00      0.00      0.00         3\n",
      "        476       1.00      0.99      0.99        68\n",
      "        477       0.33      0.50      0.40         6\n",
      "        478       0.00      0.00      0.00         2\n",
      "        479       0.33      0.33      0.33         6\n",
      "        480       0.00      0.00      0.00         1\n",
      "        481       0.78      0.70      0.74        10\n",
      "        482       0.90      1.00      0.95        18\n",
      "        483       0.00      0.00      0.00         2\n",
      "        486       0.00      0.00      0.00         1\n",
      "        487       1.00      0.75      0.86         4\n",
      "        488       0.00      0.00      0.00         1\n",
      "        489       0.00      0.00      0.00         1\n",
      "        490       1.00      0.95      0.98        22\n",
      "        491       0.43      0.75      0.55         4\n",
      "        492       0.00      0.00      0.00         2\n",
      "        493       1.00      0.67      0.80         3\n",
      "        494       0.71      0.83      0.77         6\n",
      "        495       1.00      0.50      0.67         2\n",
      "        496       0.80      0.25      0.38        16\n",
      "        497       0.00      0.00      0.00         1\n",
      "        498       0.00      0.00      0.00         2\n",
      "        499       0.00      0.00      0.00         4\n",
      "        500       0.00      0.00      0.00         1\n",
      "        501       0.78      0.78      0.78        32\n",
      "        502       0.53      0.57      0.55        14\n",
      "        503       1.00      0.96      0.98        27\n",
      "        504       0.79      1.00      0.88        11\n",
      "        505       1.00      0.60      0.75         5\n",
      "        506       0.00      0.00      0.00         1\n",
      "        507       0.00      0.00      0.00         6\n",
      "        508       1.00      1.00      1.00         3\n",
      "        509       0.89      1.00      0.94         8\n",
      "        510       0.00      0.00      0.00         3\n",
      "        511       0.89      0.67      0.76        12\n",
      "        512       1.00      1.00      1.00       153\n",
      "        513       0.00      0.00      0.00         1\n",
      "        514       1.00      1.00      1.00         9\n",
      "        515       0.65      0.92      0.76        12\n",
      "        516       1.00      0.50      0.67         4\n",
      "        517       0.96      1.00      0.98        44\n",
      "        518       1.00      0.71      0.83         7\n",
      "        519       1.00      1.00      1.00         2\n",
      "        520       0.00      0.00      0.00         2\n",
      "        521       1.00      0.95      0.98        22\n",
      "        522       0.00      0.00      0.00         2\n",
      "        524       0.00      0.00      0.00         1\n",
      "        525       0.80      0.67      0.73        18\n",
      "        526       1.00      1.00      1.00         3\n",
      "        527       0.75      1.00      0.86         3\n",
      "        528       0.71      1.00      0.83         5\n",
      "        529       0.00      0.00      0.00         3\n",
      "        530       1.00      0.33      0.50         6\n",
      "        531       0.00      0.00      0.00         2\n",
      "        532       0.75      0.90      0.82        10\n",
      "        533       1.00      1.00      1.00         2\n",
      "        534       0.67      0.73      0.70        11\n",
      "        535       0.00      0.00      0.00         2\n",
      "        537       1.00      0.50      0.67         4\n",
      "        538       0.00      0.00      0.00         1\n",
      "        539       0.88      0.88      0.88         8\n",
      "        541       0.88      0.67      0.76        21\n",
      "        542       0.00      0.00      0.00         1\n",
      "        543       0.00      0.00      0.00         1\n",
      "        544       0.00      0.00      0.00         6\n",
      "        545       0.00      0.00      0.00         1\n",
      "        547       0.00      0.00      0.00         3\n",
      "        548       0.86      1.00      0.92         6\n",
      "        549       0.50      0.60      0.55        10\n",
      "        550       1.00      1.00      1.00         4\n",
      "        552       0.00      0.00      0.00         1\n",
      "        553       0.95      0.76      0.84        25\n",
      "        555       0.80      1.00      0.89         4\n",
      "        556       0.00      0.00      0.00         1\n",
      "        557       1.00      1.00      1.00        80\n",
      "        558       0.67      0.40      0.50         5\n",
      "        559       0.78      0.88      0.82         8\n",
      "        560       0.00      0.00      0.00         2\n",
      "        561       1.00      1.00      1.00         5\n",
      "        564       1.00      0.75      0.86         4\n",
      "        566       0.00      0.00      0.00         1\n",
      "        567       0.00      0.00      0.00         3\n",
      "        568       0.89      1.00      0.94         8\n",
      "        569       0.00      0.00      0.00         1\n",
      "        570       0.83      1.00      0.91         5\n",
      "        572       1.00      1.00      1.00         6\n",
      "        573       0.00      0.00      0.00         2\n",
      "        574       0.00      0.00      0.00         2\n",
      "        575       0.60      0.43      0.50         7\n",
      "        576       0.00      0.00      0.00         1\n",
      "        577       0.00      0.00      0.00         3\n",
      "        579       0.00      0.00      0.00         2\n",
      "        580       1.00      1.00      1.00        11\n",
      "        582       1.00      0.33      0.50         3\n",
      "        586       1.00      0.83      0.91         6\n",
      "        587       0.38      1.00      0.55         3\n",
      "        588       0.80      1.00      0.89         4\n",
      "        589       0.00      0.00      0.00         5\n",
      "        590       0.00      0.00      0.00         4\n",
      "        591       1.00      0.67      0.80         3\n",
      "        592       0.50      1.00      0.67         2\n",
      "        593       1.00      0.40      0.57         5\n",
      "        594       1.00      1.00      1.00        17\n",
      "        597       0.98      0.92      0.95        53\n",
      "        598       0.00      0.00      0.00         4\n",
      "        599       0.67      0.40      0.50         5\n",
      "        600       1.00      1.00      1.00         5\n",
      "        601       0.00      0.00      0.00         1\n",
      "        602       0.71      0.83      0.77         6\n",
      "        603       1.00      1.00      1.00         2\n",
      "        604       0.00      0.00      0.00         4\n",
      "        605       0.00      0.00      0.00         2\n",
      "        607       0.00      0.00      0.00         1\n",
      "        608       0.00      0.00      0.00         1\n",
      "        609       1.00      1.00      1.00         2\n",
      "        610       1.00      0.50      0.67         2\n",
      "        611       0.00      0.00      0.00         1\n",
      "        612       0.67      0.75      0.71         8\n",
      "        613       0.00      0.00      0.00         1\n",
      "        614       0.77      0.71      0.74        14\n",
      "        615       0.75      1.00      0.86         3\n",
      "        616       0.64      0.70      0.67        10\n",
      "        617       0.00      0.00      0.00         1\n",
      "        618       0.00      0.00      0.00         1\n",
      "        620       1.00      0.50      0.67         4\n",
      "        624       1.00      1.00      1.00         3\n",
      "        625       0.00      0.00      0.00         1\n",
      "        626       0.50      1.00      0.67         2\n",
      "        628       0.00      0.00      0.00         1\n",
      "        629       0.00      0.00      0.00         1\n",
      "        630       0.00      0.00      0.00         1\n",
      "        631       0.75      0.75      0.75         4\n",
      "        632       0.00      0.00      0.00         1\n",
      "        633       1.00      1.00      1.00         4\n",
      "        634       0.00      0.00      0.00         2\n",
      "        636       0.00      0.00      0.00         1\n",
      "        637       1.00      1.00      1.00         2\n",
      "        638       1.00      0.17      0.29         6\n",
      "        641       0.00      0.00      0.00         5\n",
      "        643       0.00      0.00      0.00         2\n",
      "        644       1.00      0.78      0.88         9\n",
      "        645       0.90      0.86      0.88        21\n",
      "        646       0.00      0.00      0.00         1\n",
      "        647       0.00      0.00      0.00         4\n",
      "        649       0.00      0.00      0.00         1\n",
      "        650       1.00      0.80      0.89         5\n",
      "        651       1.00      0.90      0.95        10\n",
      "        652       0.00      0.00      0.00        19\n",
      "        655       0.00      0.00      0.00         4\n",
      "        656       0.00      0.00      0.00         2\n",
      "        657       0.00      0.00      0.00         2\n",
      "        658       1.00      1.00      1.00         2\n",
      "        659       0.00      0.00      0.00         1\n",
      "        660       0.80      0.80      0.80         5\n",
      "        662       0.00      0.00      0.00         1\n",
      "        663       1.00      0.71      0.83         7\n",
      "        665       1.00      0.75      0.86         4\n",
      "        667       0.80      1.00      0.89         8\n",
      "        668       1.00      1.00      1.00         4\n",
      "        669       0.00      0.00      0.00         5\n",
      "        670       1.00      1.00      1.00         3\n",
      "        671       0.89      0.80      0.84        10\n",
      "        672       1.00      1.00      1.00         2\n",
      "        674       1.00      1.00      1.00         2\n",
      "        675       0.83      0.83      0.83         6\n",
      "        677       0.00      0.00      0.00         1\n",
      "        678       0.00      0.00      0.00         1\n",
      "        679       0.00      0.00      0.00         1\n",
      "        680       0.00      0.00      0.00         1\n",
      "        682       0.67      0.86      0.75         7\n",
      "        683       0.00      0.00      0.00         2\n",
      "        685       1.00      1.00      1.00         2\n",
      "        688       1.00      1.00      1.00         7\n",
      "        690       0.00      0.00      0.00         2\n",
      "        692       1.00      1.00      1.00         3\n",
      "        693       0.00      0.00      0.00         4\n",
      "        694       0.92      0.65      0.76        17\n",
      "        695       0.25      0.33      0.29         3\n",
      "        696       0.75      1.00      0.86         3\n",
      "        697       0.00      0.00      0.00         1\n",
      "        698       0.88      1.00      0.93         7\n",
      "        699       0.94      1.00      0.97        16\n",
      "        701       0.33      0.20      0.25         5\n",
      "        702       0.00      0.00      0.00         1\n",
      "        703       0.00      0.00      0.00         1\n",
      "        705       0.00      0.00      0.00         3\n",
      "        706       0.50      0.62      0.56         8\n",
      "        707       0.00      0.00      0.00         1\n",
      "        708       1.00      1.00      1.00         5\n",
      "        714       1.00      1.00      1.00         4\n",
      "        715       0.00      0.00      0.00         1\n",
      "        716       0.00      0.00      0.00         1\n",
      "        717       0.00      0.00      0.00         1\n",
      "        718       1.00      1.00      1.00         3\n",
      "        719       0.00      0.00      0.00         3\n",
      "        720       1.00      1.00      1.00         5\n",
      "        721       1.00      0.60      0.75         5\n",
      "        722       0.00      0.00      0.00         4\n",
      "        723       0.00      0.00      0.00         1\n",
      "        724       0.00      0.00      0.00         1\n",
      "        725       0.00      0.00      0.00         3\n",
      "        726       1.00      1.00      1.00        16\n",
      "        728       0.75      1.00      0.86         6\n",
      "        730       0.00      0.00      0.00         1\n",
      "        731       0.00      0.00      0.00         1\n",
      "        732       1.00      1.00      1.00         5\n",
      "        733       0.00      0.00      0.00         1\n",
      "        734       1.00      0.50      0.67         4\n",
      "        735       0.00      0.00      0.00         1\n",
      "        736       0.60      0.30      0.40        10\n",
      "        737       0.56      0.42      0.48        12\n",
      "        739       0.00      0.00      0.00         1\n",
      "        741       0.00      0.00      0.00         3\n",
      "        742       0.75      1.00      0.86         3\n",
      "        743       0.00      0.00      0.00         1\n",
      "        745       0.00      0.00      0.00         1\n",
      "        748       0.00      0.00      0.00         1\n",
      "        749       0.00      0.00      0.00         2\n",
      "        750       0.50      0.50      0.50         2\n",
      "        753       0.00      0.00      0.00         1\n",
      "        755       0.00      0.00      0.00         2\n",
      "        756       0.00      0.00      0.00         7\n",
      "        757       0.40      1.00      0.57         2\n",
      "        758       0.00      0.00      0.00         1\n",
      "        760       0.33      0.20      0.25         5\n",
      "        761       1.00      1.00      1.00         3\n",
      "        762       0.00      0.00      0.00         4\n",
      "        764       0.00      0.00      0.00         1\n",
      "        765       0.00      0.00      0.00         2\n",
      "        766       0.80      0.80      0.80        10\n",
      "        767       0.00      0.00      0.00         1\n",
      "        768       1.00      1.00      1.00         2\n",
      "        769       1.00      1.00      1.00         3\n",
      "        770       0.00      0.00      0.00         2\n",
      "        772       0.00      0.00      0.00         2\n",
      "        773       0.00      0.00      0.00         1\n",
      "        774       1.00      0.50      0.67         8\n",
      "        775       0.00      0.00      0.00         1\n",
      "        776       1.00      1.00      1.00         2\n",
      "        782       0.00      0.00      0.00         1\n",
      "        783       0.00      0.00      0.00         4\n",
      "        785       0.00      0.00      0.00         2\n",
      "        786       0.80      1.00      0.89         4\n",
      "        787       0.00      0.00      0.00         1\n",
      "        788       0.73      0.62      0.67        13\n",
      "        789       0.00      0.00      0.00         4\n",
      "        792       0.73      0.80      0.76        10\n",
      "        795       0.00      0.00      0.00         1\n",
      "        799       0.00      0.00      0.00         2\n",
      "        800       0.75      1.00      0.86         3\n",
      "        801       0.50      1.00      0.67         3\n",
      "        803       0.00      0.00      0.00         1\n",
      "        804       0.00      0.00      0.00         4\n",
      "        806       0.00      0.00      0.00         1\n",
      "        808       0.00      0.00      0.00         2\n",
      "        810       0.00      0.00      0.00         4\n",
      "        811       0.00      0.00      0.00         1\n",
      "        814       1.00      1.00      1.00         2\n",
      "        815       0.00      0.00      0.00         1\n",
      "        816       0.00      0.00      0.00         1\n",
      "        817       0.50      0.50      0.50         4\n",
      "        823       0.67      0.67      0.67         6\n",
      "        828       1.00      0.75      0.86         8\n",
      "        829       0.75      1.00      0.86         3\n",
      "        830       0.00      0.00      0.00         4\n",
      "        831       0.00      0.00      0.00         1\n",
      "        832       0.00      0.00      0.00         1\n",
      "        833       0.80      0.44      0.57         9\n",
      "        835       0.00      0.00      0.00         2\n",
      "        836       1.00      0.71      0.83         7\n",
      "        837       1.00      1.00      1.00         3\n",
      "        839       0.00      0.00      0.00         1\n",
      "        842       1.00      1.00      1.00         6\n",
      "        843       0.86      1.00      0.92         6\n",
      "        844       0.00      0.00      0.00         2\n",
      "        846       0.00      0.00      0.00         1\n",
      "        847       0.75      0.60      0.67         5\n",
      "        848       0.00      0.00      0.00         1\n",
      "        849       0.00      0.00      0.00         3\n",
      "        852       0.00      0.00      0.00         4\n",
      "        853       0.00      0.00      0.00         1\n",
      "        855       0.00      0.00      0.00         1\n",
      "        856       0.00      0.00      0.00         1\n",
      "        857       0.00      0.00      0.00         1\n",
      "        858       1.00      0.60      0.75         5\n",
      "        861       0.00      0.00      0.00         1\n",
      "        862       1.00      0.50      0.67         6\n",
      "        863       0.00      0.00      0.00         1\n",
      "        865       0.00      0.00      0.00         1\n",
      "        867       0.00      0.00      0.00         1\n",
      "        868       0.00      0.00      0.00         2\n",
      "        871       0.00      0.00      0.00         1\n",
      "        872       1.00      1.00      1.00         5\n",
      "        874       0.75      0.75      0.75         4\n",
      "        875       0.67      0.67      0.67         6\n",
      "        878       0.00      0.00      0.00         2\n",
      "        879       0.00      0.00      0.00         1\n",
      "        882       0.40      0.50      0.44         4\n",
      "        883       0.00      0.00      0.00         1\n",
      "        885       0.83      1.00      0.91         5\n",
      "        886       0.00      0.00      0.00         4\n",
      "        888       0.00      0.00      0.00         1\n",
      "        890       1.00      1.00      1.00         2\n",
      "        892       0.00      0.00      0.00         4\n",
      "        893       0.75      1.00      0.86         3\n",
      "        895       1.00      1.00      1.00         2\n",
      "        896       0.00      0.00      0.00         2\n",
      "        897       0.83      1.00      0.91         5\n",
      "        901       0.00      0.00      0.00         2\n",
      "        902       0.33      0.33      0.33         6\n",
      "        903       0.00      0.00      0.00         1\n",
      "        904       0.00      0.00      0.00         1\n",
      "        905       0.00      0.00      0.00         1\n",
      "        907       0.00      0.00      0.00         1\n",
      "        908       0.00      0.00      0.00         2\n",
      "        909       1.00      1.00      1.00         2\n",
      "        910       0.00      0.00      0.00         1\n",
      "        914       1.00      1.00      1.00         4\n",
      "        915       0.00      0.00      0.00         2\n",
      "        916       1.00      1.00      1.00         3\n",
      "        917       0.00      0.00      0.00         1\n",
      "        920       1.00      0.40      0.57         5\n",
      "        921       0.00      0.00      0.00         1\n",
      "        922       0.00      0.00      0.00         2\n",
      "        930       0.00      0.00      0.00         2\n",
      "        933       0.00      0.00      0.00         2\n",
      "        935       0.00      0.00      0.00         2\n",
      "        936       0.00      0.00      0.00         1\n",
      "        937       1.00      1.00      1.00         3\n",
      "        938       0.00      0.00      0.00         3\n",
      "        939       1.00      1.00      1.00         4\n",
      "        940       0.00      0.00      0.00         1\n",
      "        944       0.00      0.00      0.00         2\n",
      "        945       0.00      0.00      0.00         1\n",
      "        946       0.00      0.00      0.00         1\n",
      "        947       0.00      0.00      0.00         1\n",
      "        948       0.00      0.00      0.00         2\n",
      "        949       1.00      0.25      0.40         4\n",
      "        951       0.00      0.00      0.00         1\n",
      "        954       0.00      0.00      0.00         2\n",
      "        955       0.00      0.00      0.00         1\n",
      "        956       0.75      0.60      0.67         5\n",
      "        957       0.00      0.00      0.00         4\n",
      "        959       1.00      1.00      1.00         8\n",
      "        960       0.00      0.00      0.00         1\n",
      "        961       0.00      0.00      0.00         2\n",
      "        962       1.00      1.00      1.00        21\n",
      "        964       0.83      0.83      0.83        12\n",
      "        969       0.00      0.00      0.00         1\n",
      "        970       0.50      0.33      0.40         3\n",
      "        971       0.00      0.00      0.00         1\n",
      "        973       0.00      0.00      0.00         1\n",
      "        974       0.00      0.00      0.00         1\n",
      "        976       0.00      0.00      0.00         1\n",
      "        977       0.00      0.00      0.00         2\n",
      "        978       0.00      0.00      0.00         1\n",
      "        979       0.00      0.00      0.00         1\n",
      "        980       0.00      0.00      0.00         2\n",
      "        982       0.00      0.00      0.00         1\n",
      "        983       0.00      0.00      0.00         1\n",
      "        984       1.00      1.00      1.00         2\n",
      "        985       0.00      0.00      0.00         1\n",
      "        987       1.00      1.00      1.00         2\n",
      "        988       0.50      0.50      0.50         4\n",
      "        989       0.00      0.00      0.00         1\n",
      "        990       0.00      0.00      0.00         1\n",
      "        991       0.00      0.00      0.00         6\n",
      "        992       0.00      0.00      0.00         3\n",
      "        993       0.00      0.00      0.00         1\n",
      "        994       0.00      0.00      0.00         3\n",
      "        995       0.00      0.00      0.00         1\n",
      "        997       0.00      0.00      0.00         1\n",
      "        998       0.00      0.00      0.00         1\n",
      "        999       0.00      0.00      0.00         1\n",
      "       1000       0.75      0.67      0.71         9\n",
      "       1004       0.00      0.00      0.00         1\n",
      "       1007       1.00      0.83      0.91         6\n",
      "       1010       0.60      1.00      0.75         3\n",
      "       1011       0.00      0.00      0.00         1\n",
      "       1012       0.00      0.00      0.00         1\n",
      "       1013       1.00      1.00      1.00         6\n",
      "       1014       0.00      0.00      0.00         1\n",
      "       1015       0.62      1.00      0.77         5\n",
      "       1016       0.00      0.00      0.00         1\n",
      "       1017       0.00      0.00      0.00         1\n",
      "       1019       0.00      0.00      0.00         1\n",
      "       1020       0.00      0.00      0.00         2\n",
      "       1021       0.00      0.00      0.00         1\n",
      "       1023       1.00      0.91      0.95        11\n",
      "       1024       0.00      0.00      0.00         1\n",
      "       1025       0.00      0.00      0.00         1\n",
      "       1027       0.00      0.00      0.00         1\n",
      "       1028       0.00      0.00      0.00         1\n",
      "       1031       0.00      0.00      0.00         1\n",
      "       1032       0.00      0.00      0.00         1\n",
      "       1033       0.00      0.00      0.00         1\n",
      "       1034       0.00      0.00      0.00         2\n",
      "       1035       0.00      0.00      0.00         1\n",
      "       1038       0.00      0.00      0.00         3\n",
      "       1039       0.00      0.00      0.00         2\n",
      "       1040       0.00      0.00      0.00         1\n",
      "       1044       1.00      1.00      1.00         2\n",
      "       1045       0.00      0.00      0.00         1\n",
      "       1046       0.00      0.00      0.00         1\n",
      "       1047       0.00      0.00      0.00         3\n",
      "       1048       0.00      0.00      0.00         1\n",
      "       1049       0.00      0.00      0.00         1\n",
      "       1050       0.67      1.00      0.80         2\n",
      "       1051       0.86      0.86      0.86         7\n",
      "       1053       0.00      0.00      0.00         1\n",
      "       1055       0.00      0.00      0.00         3\n",
      "       1057       1.00      0.33      0.50         3\n",
      "       1058       0.00      0.00      0.00         1\n",
      "       1059       0.00      0.00      0.00         1\n",
      "       1061       0.00      0.00      0.00         1\n",
      "       1062       0.00      0.00      0.00         1\n",
      "       1063       0.00      0.00      0.00         1\n",
      "       1064       0.00      0.00      0.00         2\n",
      "       1065       1.00      1.00      1.00         2\n",
      "       1066       0.00      0.00      0.00         1\n",
      "       1068       0.00      0.00      0.00         1\n",
      "       1069       0.00      0.00      0.00         1\n",
      "       1070       1.00      0.80      0.89         5\n",
      "       1071       0.00      0.00      0.00         1\n",
      "       1073       0.00      0.00      0.00         1\n",
      "       1075       0.00      0.00      0.00         2\n",
      "       1077       0.00      0.00      0.00         2\n",
      "       1078       0.50      0.67      0.57         3\n",
      "       1079       0.00      0.00      0.00         2\n",
      "       1080       1.00      0.67      0.80         3\n",
      "       1081       0.00      0.00      0.00         2\n",
      "       1082       1.00      1.00      1.00         7\n",
      "       1083       0.00      0.00      0.00         1\n",
      "       1084       0.00      0.00      0.00         1\n",
      "       1087       0.00      0.00      0.00         1\n",
      "       1089       0.00      0.00      0.00         1\n",
      "       1093       0.00      0.00      0.00         3\n",
      "       1096       0.00      0.00      0.00         2\n",
      "       1097       0.00      0.00      0.00         1\n",
      "       1099       0.00      0.00      0.00         1\n",
      "       1100       1.00      1.00      1.00         2\n",
      "       1101       0.67      1.00      0.80         4\n",
      "       1102       0.00      0.00      0.00         1\n",
      "       1103       1.00      1.00      1.00         4\n",
      "       1104       0.00      0.00      0.00         1\n",
      "       1106       1.00      0.50      0.67         2\n",
      "       1107       0.00      0.00      0.00         4\n",
      "       1108       0.00      0.00      0.00         1\n",
      "       1110       0.00      0.00      0.00         2\n",
      "       1111       0.00      0.00      0.00         1\n",
      "       1113       0.00      0.00      0.00         3\n",
      "       1114       0.00      0.00      0.00         1\n",
      "       1115       0.00      0.00      0.00         1\n",
      "       1116       0.00      0.00      0.00         2\n",
      "       1120       0.00      0.00      0.00         1\n",
      "       1121       1.00      1.00      1.00         2\n",
      "       1124       0.00      0.00      0.00         1\n",
      "       1126       0.00      0.00      0.00         2\n",
      "       1127       0.50      0.80      0.62         5\n",
      "       1128       0.00      0.00      0.00         1\n",
      "       1130       1.00      1.00      1.00         2\n",
      "       1131       0.00      0.00      0.00         2\n",
      "       1132       0.00      0.00      0.00         2\n",
      "       1136       0.00      0.00      0.00         1\n",
      "       1139       1.00      1.00      1.00         3\n",
      "       1141       0.00      0.00      0.00         1\n",
      "       1142       0.00      0.00      0.00         1\n",
      "       1143       0.00      0.00      0.00         2\n",
      "       1146       0.75      1.00      0.86         3\n",
      "       1149       0.00      0.00      0.00         1\n",
      "       1152       0.00      0.00      0.00         2\n",
      "       1153       1.00      0.33      0.50         6\n",
      "       1154       1.00      1.00      1.00         4\n",
      "       1155       0.00      0.00      0.00         1\n",
      "       1157       0.00      0.00      0.00         1\n",
      "       1158       0.00      0.00      0.00         1\n",
      "       1159       1.00      1.00      1.00         4\n",
      "       1161       0.00      0.00      0.00         1\n",
      "       1162       0.00      0.00      0.00         2\n",
      "       1165       0.00      0.00      0.00         1\n",
      "       1166       0.00      0.00      0.00         1\n",
      "       1167       1.00      0.67      0.80         3\n",
      "       1168       0.00      0.00      0.00         2\n",
      "       1170       0.00      0.00      0.00         1\n",
      "       1171       0.00      0.00      0.00         1\n",
      "       1174       0.00      0.00      0.00         1\n",
      "       1175       1.00      1.00      1.00         3\n",
      "       1178       0.00      0.00      0.00         1\n",
      "       1179       0.00      0.00      0.00         1\n",
      "       1180       0.00      0.00      0.00         1\n",
      "       1183       0.67      0.67      0.67         6\n",
      "       1186       0.00      0.00      0.00         1\n",
      "       1188       0.75      1.00      0.86         3\n",
      "       1190       0.00      0.00      0.00         1\n",
      "       1193       0.00      0.00      0.00         1\n",
      "       1195       0.00      0.00      0.00         1\n",
      "       1196       0.33      0.50      0.40         2\n",
      "       1197       0.25      0.25      0.25         4\n",
      "       1198       0.00      0.00      0.00         1\n",
      "       1199       0.00      0.00      0.00         1\n",
      "       1201       0.00      0.00      0.00         1\n",
      "       1202       0.00      0.00      0.00         1\n",
      "       1206       0.00      0.00      0.00         1\n",
      "       1209       0.00      0.00      0.00         1\n",
      "       1210       0.00      0.00      0.00         4\n",
      "       1211       0.00      0.00      0.00         1\n",
      "       1213       0.00      0.00      0.00         2\n",
      "       1215       0.00      0.00      0.00         1\n",
      "       1219       0.00      0.00      0.00         1\n",
      "       1220       1.00      1.00      1.00         2\n",
      "       1221       0.67      0.67      0.67         3\n",
      "       1223       0.00      0.00      0.00         4\n",
      "       1224       0.00      0.00      0.00         1\n",
      "       1226       0.00      0.00      0.00         1\n",
      "       1229       1.00      1.00      1.00         2\n",
      "       1230       0.00      0.00      0.00         1\n",
      "       1234       0.67      1.00      0.80         2\n",
      "       1236       0.00      0.00      0.00         1\n",
      "       1238       0.00      0.00      0.00         1\n",
      "       1239       0.00      0.00      0.00         1\n",
      "       1240       0.00      0.00      0.00         1\n",
      "       1244       0.00      0.00      0.00         1\n",
      "       1245       0.00      0.00      0.00         1\n",
      "       1247       0.00      0.00      0.00         1\n",
      "       1251       0.00      0.00      0.00         2\n",
      "       1255       0.00      0.00      0.00         1\n",
      "       1258       1.00      1.00      1.00         2\n",
      "       1260       0.00      0.00      0.00         1\n",
      "       1263       0.00      0.00      0.00         1\n",
      "       1264       0.00      0.00      0.00         2\n",
      "       1266       0.00      0.00      0.00         1\n",
      "       1268       0.00      0.00      0.00         2\n",
      "       1269       0.50      0.50      0.50         2\n",
      "       1270       0.00      0.00      0.00         1\n",
      "       1271       1.00      1.00      1.00         3\n",
      "       1275       0.00      0.00      0.00         1\n",
      "       1277       0.00      0.00      0.00         1\n",
      "       1278       0.00      0.00      0.00         2\n",
      "       1279       0.00      0.00      0.00         2\n",
      "       1280       0.50      1.00      0.67         2\n",
      "       1281       0.00      0.00      0.00         1\n",
      "       1283       0.00      0.00      0.00         2\n",
      "       1286       0.00      0.00      0.00         2\n",
      "       1289       0.00      0.00      0.00         1\n",
      "       1290       0.00      0.00      0.00         1\n",
      "       1291       0.00      0.00      0.00         2\n",
      "       1292       0.00      0.00      0.00         1\n",
      "       1293       0.00      0.00      0.00         1\n",
      "       1294       0.00      0.00      0.00         1\n",
      "       1295       0.00      0.00      0.00         1\n",
      "       1298       0.00      0.00      0.00         5\n",
      "       1299       0.00      0.00      0.00         3\n",
      "       1300       0.00      0.00      0.00         4\n",
      "       1304       1.00      0.67      0.80         3\n",
      "       1305       0.00      0.00      0.00         3\n",
      "       1306       0.00      0.00      0.00         2\n",
      "       1307       0.00      0.00      0.00         1\n",
      "       1308       0.00      0.00      0.00         1\n",
      "       1309       0.00      0.00      0.00         1\n",
      "       1310       0.50      0.25      0.33         4\n",
      "       1311       0.00      0.00      0.00         2\n",
      "       1313       0.00      0.00      0.00         2\n",
      "       1316       0.00      0.00      0.00         1\n",
      "       1317       0.00      0.00      0.00         1\n",
      "       1318       1.00      1.00      1.00         3\n",
      "       1321       0.00      0.00      0.00         1\n",
      "       1328       0.00      0.00      0.00         1\n",
      "       1330       0.00      0.00      0.00         1\n",
      "       1331       0.00      0.00      0.00         2\n",
      "       1332       0.00      0.00      0.00         1\n",
      "       1333       0.00      0.00      0.00         1\n",
      "       1336       0.00      0.00      0.00         2\n",
      "       1338       0.00      0.00      0.00         1\n",
      "       1339       0.00      0.00      0.00         1\n",
      "       1341       0.00      0.00      0.00         1\n",
      "       1342       1.00      1.00      1.00         2\n",
      "       1343       1.00      1.00      1.00         4\n",
      "       1345       0.00      0.00      0.00         1\n",
      "       1347       0.00      0.00      0.00         1\n",
      "       1351       0.00      0.00      0.00         1\n",
      "       1352       0.00      0.00      0.00         1\n",
      "       1353       0.00      0.00      0.00         1\n",
      "       1355       0.00      0.00      0.00         2\n",
      "       1359       0.00      0.00      0.00         1\n",
      "       1360       0.00      0.00      0.00         1\n",
      "       1365       1.00      1.00      1.00         3\n",
      "       1366       0.00      0.00      0.00         1\n",
      "       1367       0.00      0.00      0.00         1\n",
      "       1371       0.00      0.00      0.00         2\n",
      "       1373       0.00      0.00      0.00         1\n",
      "       1376       0.75      1.00      0.86         3\n",
      "       1377       1.00      0.50      0.67         4\n",
      "       1382       0.00      0.00      0.00         3\n",
      "       1383       0.00      0.00      0.00         1\n",
      "       1386       0.00      0.00      0.00         1\n",
      "       1389       0.00      0.00      0.00         1\n",
      "       1393       0.00      0.00      0.00         1\n",
      "       1394       0.00      0.00      0.00         1\n",
      "       1398       0.00      0.00      0.00         1\n",
      "       1402       0.00      0.00      0.00         1\n",
      "       1403       0.00      0.00      0.00         2\n",
      "       1404       0.00      0.00      0.00         2\n",
      "       1410       0.00      0.00      0.00         1\n",
      "       1414       0.00      0.00      0.00         1\n",
      "       1416       0.00      0.00      0.00         1\n",
      "       1417       0.00      0.00      0.00         3\n",
      "       1418       0.00      0.00      0.00         1\n",
      "       1419       1.00      1.00      1.00         2\n",
      "       1420       0.00      0.00      0.00         1\n",
      "       1422       0.00      0.00      0.00         2\n",
      "       1423       0.00      0.00      0.00         2\n",
      "       1424       0.00      0.00      0.00         1\n",
      "       1425       0.00      0.00      0.00         1\n",
      "       1428       0.00      0.00      0.00         1\n",
      "       1429       1.00      1.00      1.00         2\n",
      "       1431       0.00      0.00      0.00         1\n",
      "       1434       1.00      0.75      0.86         4\n",
      "       1435       0.00      0.00      0.00         1\n",
      "       1439       0.00      0.00      0.00         1\n",
      "       1440       0.00      0.00      0.00         1\n",
      "       1442       0.00      0.00      0.00         1\n",
      "       1443       0.00      0.00      0.00         1\n",
      "       1444       0.00      0.00      0.00         1\n",
      "       1445       0.00      0.00      0.00         1\n",
      "       1447       0.00      0.00      0.00         1\n",
      "       1448       0.00      0.00      0.00         1\n",
      "       1451       0.00      0.00      0.00         1\n",
      "       1455       0.00      0.00      0.00         1\n",
      "       1456       0.00      0.00      0.00         1\n",
      "       1457       0.00      0.00      0.00         4\n",
      "       1458       0.00      0.00      0.00         1\n",
      "       1460       0.00      0.00      0.00         1\n",
      "       1461       1.00      0.67      0.80         3\n",
      "       1462       0.00      0.00      0.00         1\n",
      "       1464       0.29      0.67      0.40         3\n",
      "       1465       0.00      0.00      0.00         2\n",
      "       1467       0.00      0.00      0.00         1\n",
      "       1468       0.00      0.00      0.00         1\n",
      "       1469       1.00      0.67      0.80         3\n",
      "       1470       0.00      0.00      0.00         2\n",
      "       1474       0.00      0.00      0.00         1\n",
      "       1478       1.00      1.00      1.00         4\n",
      "       1483       0.00      0.00      0.00         1\n",
      "       1491       0.00      0.00      0.00         1\n",
      "       1493       0.00      0.00      0.00         1\n",
      "       1494       0.00      0.00      0.00         1\n",
      "       1496       0.00      0.00      0.00         1\n",
      "       1498       0.00      0.00      0.00         3\n",
      "       1499       0.00      0.00      0.00         1\n",
      "       1501       0.00      0.00      0.00         1\n",
      "       1503       0.00      0.00      0.00         1\n",
      "       1504       0.00      0.00      0.00         1\n",
      "       1505       0.00      0.00      0.00         1\n",
      "       1511       1.00      1.00      1.00         3\n",
      "       1512       0.00      0.00      0.00         1\n",
      "       1513       1.00      1.00      1.00         2\n",
      "       1515       0.00      0.00      0.00         1\n",
      "       1521       0.00      0.00      0.00         1\n",
      "       1522       1.00      1.00      1.00         2\n",
      "       1524       0.00      0.00      0.00         1\n",
      "       1525       0.00      0.00      0.00         2\n",
      "       1526       0.00      0.00      0.00         1\n",
      "       1528       0.00      0.00      0.00         1\n",
      "       1532       0.00      0.00      0.00         1\n",
      "       1541       0.00      0.00      0.00         1\n",
      "       1548       0.00      0.00      0.00         1\n",
      "       1549       0.00      0.00      0.00         1\n",
      "       1555       0.00      0.00      0.00         1\n",
      "       1558       0.00      0.00      0.00         1\n",
      "       1563       1.00      0.33      0.50         3\n",
      "       1567       0.00      0.00      0.00         1\n",
      "       1569       0.00      0.00      0.00         1\n",
      "       1571       0.00      0.00      0.00         1\n",
      "       1572       0.00      0.00      0.00         1\n",
      "       1575       0.00      0.00      0.00         1\n",
      "       1576       0.00      0.00      0.00         1\n",
      "       1578       0.00      0.00      0.00         1\n",
      "       1579       0.00      0.00      0.00         1\n",
      "       1580       0.00      0.00      0.00         1\n",
      "       1581       0.00      0.00      0.00         1\n",
      "       1587       0.00      0.00      0.00         1\n",
      "       1588       0.00      0.00      0.00         1\n",
      "       1590       0.00      0.00      0.00         1\n",
      "       1594       0.00      0.00      0.00         1\n",
      "       1595       0.00      0.00      0.00         1\n",
      "       1597       0.00      0.00      0.00         1\n",
      "       1598       0.00      0.00      0.00         1\n",
      "       1600       0.00      0.00      0.00         1\n",
      "       1603       1.00      1.00      1.00         2\n",
      "       1604       0.00      0.00      0.00         1\n",
      "       1607       0.00      0.00      0.00         1\n",
      "       1609       0.00      0.00      0.00         1\n",
      "       1610       0.00      0.00      0.00         1\n",
      "       1611       1.00      1.00      1.00         2\n",
      "       1615       0.00      0.00      0.00         1\n",
      "       1621       0.00      0.00      0.00         1\n",
      "       1625       0.00      0.00      0.00         1\n",
      "       1626       0.00      0.00      0.00         1\n",
      "       1629       0.00      0.00      0.00         1\n",
      "       1630       0.00      0.00      0.00         2\n",
      "       1632       0.00      0.00      0.00         3\n",
      "       1633       0.00      0.00      0.00         1\n",
      "       1637       0.00      0.00      0.00         1\n",
      "       1639       0.00      0.00      0.00         2\n",
      "       1643       0.00      0.00      0.00         1\n",
      "       1646       0.00      0.00      0.00         1\n",
      "       1647       0.00      0.00      0.00         2\n",
      "       1648       0.00      0.00      0.00         1\n",
      "       1649       0.00      0.00      0.00         2\n",
      "       1650       0.00      0.00      0.00         1\n",
      "       1653       0.00      0.00      0.00         2\n",
      "       1657       0.00      0.00      0.00         1\n",
      "       1661       0.00      0.00      0.00         2\n",
      "       1662       0.00      0.00      0.00         1\n",
      "       1667       0.00      0.00      0.00         1\n",
      "       1668       0.00      0.00      0.00         1\n",
      "       1669       0.00      0.00      0.00         1\n",
      "       1670       0.00      0.00      0.00         1\n",
      "       1672       0.00      0.00      0.00         2\n",
      "       1673       0.00      0.00      0.00         1\n",
      "       1678       0.00      0.00      0.00         1\n",
      "       1682       0.00      0.00      0.00         1\n",
      "       1686       0.67      1.00      0.80         2\n",
      "       1691       0.00      0.00      0.00         1\n",
      "       1695       0.00      0.00      0.00         1\n",
      "       1697       0.00      0.00      0.00         1\n",
      "       1706       0.67      1.00      0.80         2\n",
      "       1707       0.00      0.00      0.00         1\n",
      "       1710       0.00      0.00      0.00         1\n",
      "       1711       0.00      0.00      0.00         1\n",
      "       1713       0.00      0.00      0.00         1\n",
      "       1714       0.00      0.00      0.00         1\n",
      "       1715       0.00      0.00      0.00         1\n",
      "       1717       0.00      0.00      0.00         1\n",
      "       1718       0.00      0.00      0.00         1\n",
      "       1720       0.00      0.00      0.00         1\n",
      "       1722       0.00      0.00      0.00         1\n",
      "       1723       0.00      0.00      0.00         1\n",
      "       1727       0.00      0.00      0.00         1\n",
      "       1730       0.00      0.00      0.00         1\n",
      "       1731       0.00      0.00      0.00         1\n",
      "       1739       0.00      0.00      0.00         1\n",
      "       1740       0.00      0.00      0.00         2\n",
      "       1748       0.00      0.00      0.00         1\n",
      "       1751       0.00      0.00      0.00         1\n",
      "       1756       0.00      0.00      0.00         1\n",
      "       1758       0.00      0.00      0.00         2\n",
      "       1759       0.00      0.00      0.00         1\n",
      "       1767       0.00      0.00      0.00         1\n",
      "       1768       0.00      0.00      0.00         1\n",
      "       1772       0.00      0.00      0.00         1\n",
      "       1773       0.00      0.00      0.00         1\n",
      "       1775       0.00      0.00      0.00         1\n",
      "       1776       0.00      0.00      0.00         1\n",
      "       1777       0.00      0.00      0.00         1\n",
      "       1778       0.00      0.00      0.00         1\n",
      "       1781       0.00      0.00      0.00         1\n",
      "       1783       0.00      0.00      0.00         2\n",
      "       1785       0.00      0.00      0.00         1\n",
      "       1791       1.00      1.00      1.00         2\n",
      "       1793       0.00      0.00      0.00         1\n",
      "       1795       0.00      0.00      0.00         2\n",
      "       1796       0.00      0.00      0.00         1\n",
      "       1798       0.00      0.00      0.00         1\n",
      "       1799       0.00      0.00      0.00         1\n",
      "       1800       0.00      0.00      0.00         1\n",
      "       1802       0.00      0.00      0.00         1\n",
      "       1803       0.00      0.00      0.00         1\n",
      "       1808       0.00      0.00      0.00         1\n",
      "       1810       0.00      0.00      0.00         1\n",
      "       1812       0.00      0.00      0.00         1\n",
      "       1814       0.00      0.00      0.00         2\n",
      "       1820       0.00      0.00      0.00         1\n",
      "       1823       0.00      0.00      0.00         1\n",
      "       1828       0.00      0.00      0.00         1\n",
      "       1829       0.00      0.00      0.00         1\n",
      "       1831       0.00      0.00      0.00         1\n",
      "       1833       0.00      0.00      0.00         1\n",
      "       1837       0.00      0.00      0.00         1\n",
      "       1845       0.00      0.00      0.00         1\n",
      "       1846       0.00      0.00      0.00         1\n",
      "       1848       0.00      0.00      0.00         1\n",
      "       1849       0.00      0.00      0.00         1\n",
      "       1850       0.00      0.00      0.00         1\n",
      "       1852       0.00      0.00      0.00         1\n",
      "       1853       0.00      0.00      0.00         1\n",
      "       1855       0.00      0.00      0.00         1\n",
      "       1859       0.00      0.00      0.00         1\n",
      "       1864       0.00      0.00      0.00         1\n",
      "       1867       0.00      0.00      0.00         1\n",
      "       1870       0.00      0.00      0.00         2\n",
      "       1871       0.00      0.00      0.00         1\n",
      "       1872       0.00      0.00      0.00         1\n",
      "       1874       0.00      0.00      0.00         1\n",
      "       1875       0.00      0.00      0.00         1\n",
      "       1876       0.00      0.00      0.00         1\n",
      "       1880       0.00      0.00      0.00         1\n",
      "       1884       0.00      0.00      0.00         1\n",
      "       1885       0.00      0.00      0.00         1\n",
      "       1886       0.00      0.00      0.00         1\n",
      "       1889       0.00      0.00      0.00         1\n",
      "       1890       0.00      0.00      0.00         1\n",
      "       1891       0.00      0.00      0.00         1\n",
      "       1893       0.00      0.00      0.00         1\n",
      "       1895       0.00      0.00      0.00         1\n",
      "       1899       0.00      0.00      0.00         1\n",
      "       1903       0.00      0.00      0.00         1\n",
      "       1907       0.00      0.00      0.00         1\n",
      "       1909       0.00      0.00      0.00         1\n",
      "       1911       0.00      0.00      0.00         1\n",
      "       1913       0.00      0.00      0.00         1\n",
      "       1915       0.00      0.00      0.00         1\n",
      "       1918       0.00      0.00      0.00         1\n",
      "       1920       0.00      0.00      0.00         1\n",
      "       1923       0.00      0.00      0.00         1\n",
      "       1930       0.00      0.00      0.00         1\n",
      "       1933       0.00      0.00      0.00         1\n",
      "       1934       0.00      0.00      0.00         1\n",
      "       1935       0.00      0.00      0.00         1\n",
      "       1942       0.00      0.00      0.00         1\n",
      "       1944       0.00      0.00      0.00         1\n",
      "       1948       0.00      0.00      0.00         1\n",
      "       1952       0.00      0.00      0.00         1\n",
      "       1957       0.00      0.00      0.00         1\n",
      "       1958       0.00      0.00      0.00         1\n",
      "       1965       0.00      0.00      0.00         1\n",
      "       1966       0.00      0.00      0.00         1\n",
      "       1968       0.00      0.00      0.00         1\n",
      "       1972       0.00      0.00      0.00         1\n",
      "       1975       0.00      0.00      0.00         1\n",
      "       1976       0.00      0.00      0.00         1\n",
      "       1977       0.00      0.00      0.00         1\n",
      "       1978       0.00      0.00      0.00         1\n",
      "       1981       0.00      0.00      0.00         1\n",
      "       1982       0.00      0.00      0.00         1\n",
      "       1989       0.00      0.00      0.00         1\n",
      "       1990       0.00      0.00      0.00         1\n",
      "       1996       0.00      0.00      0.00         1\n",
      "       1998       0.00      0.00      0.00         1\n",
      "       2001       0.00      0.00      0.00         1\n",
      "       2005       0.00      0.00      0.00         1\n",
      "       2011       0.00      0.00      0.00         1\n",
      "       2012       0.00      0.00      0.00         1\n",
      "       2015       0.00      0.00      0.00         1\n",
      "       2021       0.00      0.00      0.00         1\n",
      "       2022       0.00      0.00      0.00         1\n",
      "       2027       0.00      0.00      0.00         1\n",
      "       2032       0.00      0.00      0.00         1\n",
      "       2033       0.00      0.00      0.00         1\n",
      "       2036       0.00      0.00      0.00         1\n",
      "       2038       0.00      0.00      0.00         1\n",
      "       2039       0.00      0.00      0.00         1\n",
      "       2041       0.00      0.00      0.00         1\n",
      "       2043       0.00      0.00      0.00         1\n",
      "       2044       0.00      0.00      0.00         1\n",
      "       2045       0.00      0.00      0.00         1\n",
      "       2046       0.00      0.00      0.00         1\n",
      "       2047       0.00      0.00      0.00         1\n",
      "       2048       0.00      0.00      0.00         1\n",
      "       2050       0.00      0.00      0.00         1\n",
      "       2053       0.00      0.00      0.00         1\n",
      "       2054       0.00      0.00      0.00         1\n",
      "\n",
      "avg / total       0.90      0.91      0.90     54911\n",
      "\n",
      "score = 0.912\n"
     ]
    },
    {
     "ename": "IOError",
     "evalue": "[Errno 2] No such file or directory: 'data/data/confusion-matrix-xtratrees-vs251.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIOError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-ff4fcdc8a68e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;32mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"score = {:.3f}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maccuracy_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpred1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[0mcm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconfusion_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpred1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m \u001b[0mwrite_confusion_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'data/confusion-matrix-xtratrees-vs251.txt'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-2-61b6a113b7ff>\u001b[0m in \u001b[0;36mwrite_confusion_matrix\u001b[1;34m(cm, out_file_name)\u001b[0m\n\u001b[0;32m     71\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mwrite_confusion_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout_file_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m     \u001b[1;31m#fop = open('data/confusion-matrix-apt.txt','w')\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 73\u001b[1;33m     \u001b[0mfop\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'data/'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mout_file_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'w'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     74\u001b[0m     \u001b[1;31m# this is rubbish ->  cm.tofile(fop, \",\")\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0ma_idx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIOError\u001b[0m: [Errno 2] No such file or directory: 'data/data/confusion-matrix-xtratrees-vs251.txt'"
     ]
    }
   ],
   "source": [
    "# Getting memory exhaustion, try a small subset of features.\n",
    "#X = combined_train_features.iloc[:,1:100]\n",
    "#y = train_labels['label']\n",
    "#print(\"Length of y: {:d}\".format(len(y)))\n",
    "#print(\"Shape of X: {:d} {:d}\".format(X.shape[0], X.shape[1]))\n",
    "\n",
    "clf1 = ExtraTreesClassifier(n_estimators=100, max_features=None, min_samples_leaf=1, min_samples_split=9, n_jobs=4, criterion='gini')\n",
    "pred1 = run_vs_cv(X, y, clf1, 10)\n",
    "print(\" \")\n",
    "print(\"Classification report:\")\n",
    "print(classification_report(y, pred1))\n",
    "print(\"score = {:.3f}\".format(accuracy_score(y, pred1)))\n",
    "cm = confusion_matrix(y, pred1)\n",
    "write_confusion_matrix(cm, 'confusion-matrix-xtratrees-vs251.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "write_confusion_matrix(cm, 'confusion-matrix-xtratrees-vs251.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# DEPRECATED: use the LightGBM sklearn API class.\n",
    "def run_lightgbm_cv(X, y, params, num_iters):\n",
    "\n",
    "    # Construct a kfolds object:\n",
    "    # For softprob prediction this will only work if the distribution of \n",
    "    # label values is even throughout each sub-sample, so only large sample\n",
    "    # sizes will generally work, small sample sizes with a large number of\n",
    "    # label values will generate errors when doing the softprob assignment\n",
    "    # to y_prob because the training set will likely have a different number\n",
    "    # of unique label values from the full sample set.\n",
    "    # In this case comment out the softprob assignment and y_prob return\n",
    "    # value.\n",
    "    \n",
    "    len_y = len(y)\n",
    "    num_labels = y.nunique()\n",
    "    \n",
    "    kf = KFold(len_y, n_folds=num_iters, shuffle=True)\n",
    "    y_prob = np.zeros((len_y, num_labels))\n",
    "    y_pred = np.zeros(len_y)\n",
    "    \n",
    "    # Iterate through folds\n",
    "    for train_index, test_index in kf:\n",
    "        print(test_index, train_index)\n",
    "        \n",
    "        X_train = X.loc[train_index,:]\n",
    "        X_test = X.loc[test_index,:]\n",
    "        y_train = y[train_index]\n",
    "\n",
    "        train_data = lgb.Dataset(X_train, label=y_train)\n",
    "        clf = lgb.train(params, train_data, 10)\n",
    "        \n",
    "        y_prob[test_index] = clf.predict_proba(X_test)\n",
    "        y_pred[test_index] = clf.predict(X_test)\n",
    "        \n",
    "    \n",
    "    return y_prob, y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data = lgb.Dataset(X, label=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "params = {'num_leaves':31, 'num_trees':100, 'objective':'multiclass', 'num_threads':4 }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "prob1, pred1 = run_lightgbm_cv(X, y, params, 10)\n",
    "print(\" \")\n",
    "print(\"Classification report:\")\n",
    "print(classification_report(y, pred1))\n",
    "print(\"logloss = {:.3f}\".format(log_loss(y, prob1)))\n",
    "print(\"score = {:.3f}\".format(accuracy_score(y, pred1)))\n",
    "cm = confusion_matrix(y, pred1)\n",
    "write_confusion_matrix(cm, 'data/confusion-matrix-xtratrees-vs251.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([    0,     5,     8, ..., 54874, 54882, 54902]), array([    1,     2,     3, ..., 54908, 54909, 54910]))\n",
      "(array([   11,    14,    20, ..., 54880, 54885, 54908]), array([    0,     1,     2, ..., 54907, 54909, 54910]))\n",
      "(array([   26,    30,    36, ..., 54891, 54895, 54896]), array([    0,     1,     2, ..., 54908, 54909, 54910]))\n",
      "(array([    9,    17,    25, ..., 54867, 54884, 54893]), array([    0,     1,     2, ..., 54908, 54909, 54910]))\n",
      "(array([    2,     4,    16, ..., 54900, 54903, 54906]), array([    0,     1,     3, ..., 54908, 54909, 54910]))\n",
      "(array([   27,    35,    42, ..., 54897, 54898, 54899]), array([    0,     1,     2, ..., 54908, 54909, 54910]))\n",
      "(array([   10,    13,    21, ..., 54889, 54904, 54907]), array([    0,     1,     2, ..., 54908, 54909, 54910]))\n",
      "(array([    1,     7,    15, ..., 54850, 54888, 54890]), array([    0,     2,     3, ..., 54908, 54909, 54910]))\n",
      "(array([    3,    18,    29, ..., 54871, 54905, 54910]), array([    0,     1,     2, ..., 54907, 54908, 54909]))\n",
      "(array([    6,    12,    33, ..., 54894, 54901, 54909]), array([    0,     1,     2, ..., 54907, 54908, 54910]))\n",
      " \n",
      "Classification report:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.65      0.87      0.74      5079\n",
      "          1       0.91      0.90      0.91       197\n",
      "          2       0.99      1.00      1.00       158\n",
      "          3       1.00      0.92      0.96        12\n",
      "          5       0.98      1.00      0.99       181\n",
      "          6       0.99      1.00      1.00       960\n",
      "          8       1.00      0.98      0.99       222\n",
      "          9       0.78      0.72      0.75        78\n",
      "         10       0.72      0.86      0.78       400\n",
      "         11       1.00      1.00      1.00      3779\n",
      "         12       0.96      0.99      0.98       699\n",
      "         13       0.00      0.00      0.00         2\n",
      "         14       0.86      0.92      0.89       276\n",
      "         15       0.93      0.95      0.94       998\n",
      "         16       0.00      0.00      0.00         6\n",
      "         17       1.00      0.50      0.67        18\n",
      "         19       0.99      1.00      0.99      1938\n",
      "         20       0.92      0.94      0.93       450\n",
      "         21       0.99      1.00      0.99       768\n",
      "         22       1.00      0.17      0.29         6\n",
      "         23       0.97      0.98      0.98      3316\n",
      "         24       0.97      1.00      0.98       287\n",
      "         25       0.98      0.98      0.98       295\n",
      "         26       0.42      0.58      0.48      1084\n",
      "         27       0.78      0.79      0.78       391\n",
      "         29       1.00      1.00      1.00      6134\n",
      "         30       0.77      0.67      0.72        49\n",
      "         31       1.00      1.00      1.00      1548\n",
      "         33       0.39      0.37      0.38        73\n",
      "         34       0.00      0.00      0.00         8\n",
      "         35       1.00      1.00      1.00       226\n",
      "         36       0.82      1.00      0.90        18\n",
      "         37       0.94      0.75      0.83       259\n",
      "         38       1.00      1.00      1.00       516\n",
      "         39       0.93      0.89      0.91       148\n",
      "         40       0.90      0.90      0.90        10\n",
      "         41       0.60      0.52      0.56       251\n",
      "         42       0.97      0.91      0.94        33\n",
      "         43       0.92      0.67      0.77        18\n",
      "         44       0.00      0.00      0.00         1\n",
      "         45       1.00      1.00      1.00       311\n",
      "         46       1.00      1.00      1.00       204\n",
      "         47       0.00      0.00      0.00         1\n",
      "         48       0.69      0.37      0.48        65\n",
      "         49       0.69      0.87      0.77        23\n",
      "         50       0.00      0.00      0.00         4\n",
      "         51       0.93      0.98      0.96       144\n",
      "         52       0.90      1.00      0.95        53\n",
      "         53       0.00      0.00      0.00        10\n",
      "         54       0.97      0.97      0.97        37\n",
      "         55       0.00      0.00      0.00         3\n",
      "         56       0.98      0.97      0.97        94\n",
      "         57       0.00      0.00      0.00         4\n",
      "         58       0.30      0.24      0.27        93\n",
      "         59       0.90      0.50      0.64        18\n",
      "         60       0.00      0.00      0.00         3\n",
      "         61       0.33      0.20      0.25       249\n",
      "         62       0.00      0.00      0.00         4\n",
      "         63       0.00      0.00      0.00         1\n",
      "         64       0.91      0.84      0.87        37\n",
      "         66       0.00      0.00      0.00         1\n",
      "         67       1.00      1.00      1.00        12\n",
      "         68       0.85      0.88      0.87        26\n",
      "         70       0.00      0.00      0.00         4\n",
      "         71       1.00      0.40      0.57         5\n",
      "         72       0.67      0.46      0.55        13\n",
      "         73       0.99      1.00      0.99      3767\n",
      "         75       0.99      1.00      1.00       275\n",
      "         76       1.00      1.00      1.00       313\n",
      "         77       0.56      0.41      0.48        58\n",
      "         78       0.99      1.00      0.99      4035\n",
      "         79       0.88      0.90      0.89        39\n",
      "         80       0.74      0.50      0.60        28\n",
      "         81       0.82      0.95      0.88        44\n",
      "         82       0.90      0.96      0.93        56\n",
      "         83       0.99      0.99      0.99       218\n",
      "         84       0.00      0.00      0.00         1\n",
      "         85       0.00      0.00      0.00         2\n",
      "         86       0.00      0.00      0.00         1\n",
      "         88       0.77      0.82      0.80        67\n",
      "         90       0.89      0.88      0.89       116\n",
      "         91       0.50      0.18      0.27        38\n",
      "         92       0.00      0.00      0.00         7\n",
      "         94       0.87      0.93      0.90       118\n",
      "         95       0.93      0.96      0.95       117\n",
      "         96       0.69      0.66      0.68       175\n",
      "         97       0.49      0.54      0.52        59\n",
      "         99       0.93      0.96      0.94       112\n",
      "        100       0.88      0.88      0.88        16\n",
      "        101       0.89      0.96      0.93        26\n",
      "        102       0.40      0.18      0.25        11\n",
      "        103       1.00      0.90      0.95        10\n",
      "        104       0.90      1.00      0.95        19\n",
      "        105       0.00      0.00      0.00         6\n",
      "        106       0.08      0.03      0.05        29\n",
      "        107       0.78      0.92      0.85        98\n",
      "        108       0.50      0.33      0.40         9\n",
      "        110       0.95      0.97      0.96        37\n",
      "        111       0.44      0.45      0.45        42\n",
      "        112       0.00      0.00      0.00         1\n",
      "        113       0.97      1.00      0.98        32\n",
      "        114       0.64      0.60      0.62        15\n",
      "        115       0.77      0.81      0.79        80\n",
      "        116       1.00      0.50      0.67         6\n",
      "        119       0.20      0.05      0.08        20\n",
      "        121       0.00      0.00      0.00        11\n",
      "        122       0.67      0.33      0.44         6\n",
      "        123       0.00      0.00      0.00         6\n",
      "        124       1.00      1.00      1.00       194\n",
      "        125       0.98      1.00      0.99        60\n",
      "        126       0.90      0.92      0.91        65\n",
      "        127       0.41      0.45      0.43       135\n",
      "        128       0.75      0.64      0.69       129\n",
      "        130       0.00      0.00      0.00         1\n",
      "        131       0.85      0.97      0.91        30\n",
      "        132       0.00      0.00      0.00         3\n",
      "        133       0.91      0.94      0.92        32\n",
      "        134       0.68      0.59      0.63       150\n",
      "        135       0.87      0.93      0.90        14\n",
      "        136       0.00      0.00      0.00        10\n",
      "        137       1.00      0.99      0.99       156\n",
      "        138       1.00      1.00      1.00        16\n",
      "        139       0.00      0.00      0.00         9\n",
      "        140       0.92      0.99      0.95       264\n",
      "        141       0.00      0.00      0.00         3\n",
      "        142       0.00      0.00      0.00         2\n",
      "        143       0.00      0.00      0.00         3\n",
      "        144       0.76      0.76      0.76        42\n",
      "        145       1.00      0.40      0.57         5\n",
      "        146       0.69      0.53      0.60       100\n",
      "        147       0.87      1.00      0.93        46\n",
      "        148       1.00      0.20      0.33         5\n",
      "        149       0.93      0.93      0.93        15\n",
      "        150       1.00      1.00      1.00        23\n",
      "        151       0.00      0.00      0.00         4\n",
      "        152       0.57      0.45      0.50       135\n",
      "        153       0.83      0.94      0.88        32\n",
      "        154       0.97      0.92      0.94        64\n",
      "        155       0.99      1.00      0.99        91\n",
      "        156       0.93      1.00      0.96        27\n",
      "        157       0.00      0.00      0.00         1\n",
      "        158       0.50      0.35      0.41        23\n",
      "        159       0.00      0.00      0.00         6\n",
      "        160       0.40      0.40      0.40         5\n",
      "        161       0.82      0.85      0.84        87\n",
      "        163       0.97      0.94      0.95       132\n",
      "        164       0.98      0.90      0.94        51\n",
      "        165       0.89      0.87      0.88       171\n",
      "        166       0.00      0.00      0.00         1\n",
      "        167       0.76      0.81      0.79        27\n",
      "        168       0.43      0.38      0.40        16\n",
      "        169       0.00      0.00      0.00         3\n",
      "        170       0.89      0.80      0.84        10\n",
      "        171       0.00      0.00      0.00         7\n",
      "        172       0.70      0.75      0.73        44\n",
      "        173       0.83      1.00      0.91        24\n",
      "        174       0.98      0.96      0.97       156\n",
      "        175       0.00      0.00      0.00         3\n",
      "        176       0.72      0.76      0.74        50\n",
      "        177       0.93      0.89      0.91        57\n",
      "        178       0.95      0.95      0.95        19\n",
      "        179       0.00      0.00      0.00         1\n",
      "        180       0.00      0.00      0.00         5\n",
      "        181       1.00      0.09      0.16        23\n",
      "        182       0.00      0.00      0.00         1\n",
      "        183       0.50      0.07      0.12        14\n",
      "        184       0.76      0.78      0.77        37\n",
      "        185       0.47      0.47      0.47        57\n",
      "        186       0.00      0.00      0.00         2\n",
      "        187       1.00      1.00      1.00       172\n",
      "        188       0.98      1.00      0.99        59\n",
      "        189       0.00      0.00      0.00         2\n",
      "        190       0.00      0.00      0.00         1\n",
      "        191       0.95      0.91      0.93       162\n",
      "        192       0.75      0.18      0.29        17\n",
      "        193       0.82      0.82      0.82        22\n",
      "        196       0.50      0.43      0.46         7\n",
      "        197       0.00      0.00      0.00         4\n",
      "        198       0.71      0.42      0.53        12\n",
      "        199       0.00      0.00      0.00         3\n",
      "        200       0.72      0.72      0.72        39\n",
      "        201       0.21      0.13      0.16        78\n",
      "        203       0.00      0.00      0.00         9\n",
      "        204       1.00      0.62      0.77         8\n",
      "        205       1.00      0.97      0.98        33\n",
      "        207       0.00      0.00      0.00         2\n",
      "        208       0.68      0.60      0.64        25\n",
      "        209       0.00      0.00      0.00         1\n",
      "        210       0.89      0.67      0.76        24\n",
      "        211       0.49      0.53      0.51        49\n",
      "        212       0.80      0.50      0.62         8\n",
      "        213       0.89      1.00      0.94        16\n",
      "        214       0.95      1.00      0.97        39\n",
      "        215       1.00      0.87      0.93        15\n",
      "        216       0.74      0.94      0.83        18\n",
      "        217       0.00      0.00      0.00         1\n",
      "        218       0.93      1.00      0.96        82\n",
      "        219       0.50      0.45      0.47        29\n",
      "        221       0.50      0.05      0.10        19\n",
      "        222       0.00      0.00      0.00         1\n",
      "        223       0.67      0.74      0.70        46\n",
      "        224       0.67      0.86      0.75         7\n",
      "        225       0.99      1.00      1.00       197\n",
      "        226       0.85      0.87      0.86        46\n",
      "        228       1.00      0.92      0.96        39\n",
      "        229       1.00      1.00      1.00        43\n",
      "        230       0.75      0.54      0.63        28\n",
      "        231       0.93      0.81      0.87        16\n",
      "        232       1.00      1.00      1.00        12\n",
      "        233       0.00      0.00      0.00         5\n",
      "        234       0.00      0.00      0.00         5\n",
      "        235       0.93      0.82      0.87        34\n",
      "        237       1.00      0.73      0.85        15\n",
      "        239       0.00      0.00      0.00         4\n",
      "        240       0.76      0.85      0.81        34\n",
      "        241       0.73      0.70      0.72       183\n",
      "        244       0.95      1.00      0.97        18\n",
      "        245       0.00      0.00      0.00         1\n",
      "        248       0.83      0.71      0.77        14\n",
      "        249       0.61      0.93      0.74        15\n",
      "        250       0.25      0.18      0.21        17\n",
      "        251       0.90      0.92      0.91        49\n",
      "        254       0.00      0.00      0.00         5\n",
      "        255       0.00      0.00      0.00         2\n",
      "        256       0.98      1.00      0.99       147\n",
      "        257       0.00      0.00      0.00         4\n",
      "        258       0.00      0.00      0.00         4\n",
      "        259       0.85      0.70      0.77        71\n",
      "        260       0.00      0.00      0.00         1\n",
      "        261       0.44      0.50      0.47        34\n",
      "        262       0.80      0.72      0.76        92\n",
      "        264       0.00      0.00      0.00         2\n",
      "        265       0.83      0.80      0.81        74\n",
      "        266       1.00      0.70      0.82        10\n",
      "        268       0.53      0.65      0.59        26\n",
      "        269       0.14      0.08      0.10        13\n",
      "        270       0.95      0.94      0.94        62\n",
      "        271       0.60      0.29      0.39        21\n",
      "        272       0.96      1.00      0.98        70\n",
      "        273       0.00      0.00      0.00         3\n",
      "        274       0.00      0.00      0.00         2\n",
      "        275       0.00      0.00      0.00         1\n",
      "        276       0.00      0.00      0.00         1\n",
      "        277       0.97      0.97      0.97       178\n",
      "        278       0.00      0.00      0.00         4\n",
      "        280       0.00      0.00      0.00         3\n",
      "        281       0.06      0.03      0.04        33\n",
      "        282       0.00      0.00      0.00         7\n",
      "        285       0.00      0.00      0.00         1\n",
      "        286       0.85      0.65      0.73        17\n",
      "        288       0.91      0.83      0.87        12\n",
      "        289       0.00      0.00      0.00         8\n",
      "        290       0.00      0.00      0.00         5\n",
      "        291       1.00      1.00      1.00        30\n",
      "        292       0.85      0.81      0.83        21\n",
      "        293       0.00      0.00      0.00         1\n",
      "        295       0.71      0.66      0.69        53\n",
      "        297       0.00      0.00      0.00         3\n",
      "        298       0.00      0.00      0.00         1\n",
      "        299       0.91      0.83      0.87        12\n",
      "        300       0.00      0.00      0.00         3\n",
      "        301       0.00      0.00      0.00         5\n",
      "        303       0.73      0.92      0.81        12\n",
      "        304       0.00      0.00      0.00         2\n",
      "        305       0.92      1.00      0.96        11\n",
      "        306       0.00      0.00      0.00        10\n",
      "        307       0.00      0.00      0.00         2\n",
      "        308       1.00      0.93      0.97        91\n",
      "        309       0.00      0.00      0.00         3\n",
      "        310       0.94      0.89      0.91        18\n",
      "        311       0.88      0.64      0.74        11\n",
      "        313       0.00      0.00      0.00         1\n",
      "        314       0.00      0.00      0.00         1\n",
      "        315       0.73      0.36      0.48        22\n",
      "        316       0.52      0.86      0.65        14\n",
      "        317       0.65      0.92      0.76        12\n",
      "        318       0.79      0.84      0.81        49\n",
      "        319       0.00      0.00      0.00         1\n",
      "        320       0.80      0.57      0.67         7\n",
      "        321       0.79      0.92      0.85        12\n",
      "        322       1.00      1.00      1.00         9\n",
      "        323       0.00      0.00      0.00         6\n",
      "        324       1.00      1.00      1.00        36\n",
      "        325       0.89      0.89      0.89        36\n",
      "        326       0.00      0.00      0.00         4\n",
      "        327       0.94      0.94      0.94        17\n",
      "        328       1.00      0.62      0.76        13\n",
      "        329       0.71      0.67      0.69        15\n",
      "        330       1.00      1.00      1.00        41\n",
      "        331       0.83      0.71      0.77        14\n",
      "        333       0.50      0.44      0.47         9\n",
      "        334       0.00      0.00      0.00         4\n",
      "        336       0.41      0.30      0.34        37\n",
      "        337       0.64      0.78      0.70         9\n",
      "        338       0.00      0.00      0.00         1\n",
      "        340       0.00      0.00      0.00         1\n",
      "        341       0.00      0.00      0.00         3\n",
      "        342       0.97      1.00      0.98      2214\n",
      "        344       0.00      0.00      0.00         5\n",
      "        346       0.88      0.96      0.92        23\n",
      "        347       0.71      0.76      0.73        49\n",
      "        348       1.00      1.00      1.00        11\n",
      "        349       0.00      0.00      0.00         1\n",
      "        351       0.83      0.95      0.88        20\n",
      "        352       0.75      0.67      0.71        49\n",
      "        353       1.00      1.00      1.00        24\n",
      "        355       0.00      0.00      0.00         4\n",
      "        356       0.00      0.00      0.00         4\n",
      "        358       0.00      0.00      0.00         5\n",
      "        359       0.97      0.94      0.95        33\n",
      "        360       0.00      0.00      0.00         8\n",
      "        361       0.00      0.00      0.00         4\n",
      "        362       0.00      0.00      0.00         4\n",
      "        363       0.96      0.74      0.84        35\n",
      "        364       0.00      0.00      0.00         1\n",
      "        365       0.99      1.00      0.99        80\n",
      "        367       0.00      0.00      0.00         2\n",
      "        369       0.00      0.00      0.00         4\n",
      "        370       0.88      0.96      0.92        23\n",
      "        372       0.00      0.00      0.00         1\n",
      "        374       1.00      1.00      1.00         7\n",
      "        375       0.00      0.00      0.00         1\n",
      "        376       0.00      0.00      0.00        15\n",
      "        377       0.71      0.42      0.53        12\n",
      "        378       0.00      0.00      0.00         3\n",
      "        381       0.00      0.00      0.00         6\n",
      "        382       0.88      0.97      0.92        30\n",
      "        383       1.00      0.50      0.67         6\n",
      "        384       0.00      0.00      0.00         1\n",
      "        385       0.00      0.00      0.00         5\n",
      "        386       0.50      0.17      0.25         6\n",
      "        388       0.86      0.75      0.80         8\n",
      "        389       1.00      1.00      1.00        79\n",
      "        392       1.00      1.00      1.00        50\n",
      "        393       0.00      0.00      0.00         1\n",
      "        394       1.00      1.00      1.00        22\n",
      "        396       0.00      0.00      0.00         4\n",
      "        397       0.58      0.52      0.55        21\n",
      "        399       0.50      0.38      0.43         8\n",
      "        400       1.00      1.00      1.00        22\n",
      "        401       0.00      0.00      0.00         1\n",
      "        402       0.20      0.08      0.12        12\n",
      "        403       0.00      0.00      0.00         5\n",
      "        404       0.00      0.00      0.00         1\n",
      "        405       1.00      0.88      0.93         8\n",
      "        406       0.00      0.00      0.00         1\n",
      "        407       0.00      0.00      0.00         1\n",
      "        408       0.00      0.00      0.00         2\n",
      "        409       1.00      0.83      0.91        12\n",
      "        410       0.93      0.78      0.85        18\n",
      "        411       0.00      0.00      0.00         2\n",
      "        412       0.00      0.00      0.00         3\n",
      "        413       0.56      0.50      0.53        10\n",
      "        414       0.00      0.00      0.00         7\n",
      "        415       0.82      0.75      0.78        12\n",
      "        416       0.00      0.00      0.00         1\n",
      "        417       0.98      1.00      0.99       218\n",
      "        419       0.00      0.00      0.00         2\n",
      "        420       0.00      0.00      0.00         1\n",
      "        421       0.87      0.54      0.67        24\n",
      "        422       0.50      0.50      0.50         8\n",
      "        423       0.00      0.00      0.00         4\n",
      "        426       0.80      1.00      0.89         8\n",
      "        429       1.00      1.00      1.00        10\n",
      "        430       0.00      0.00      0.00         3\n",
      "        431       0.46      0.25      0.32        24\n",
      "        432       0.86      0.86      0.86         7\n",
      "        433       0.91      1.00      0.95        21\n",
      "        434       1.00      0.93      0.96        14\n",
      "        435       0.53      0.40      0.45        25\n",
      "        438       0.00      0.00      0.00         1\n",
      "        439       1.00      1.00      1.00        11\n",
      "        441       0.00      0.00      0.00         3\n",
      "        442       0.96      1.00      0.98        27\n",
      "        443       0.00      0.00      0.00         4\n",
      "        444       0.00      0.00      0.00         6\n",
      "        445       0.00      0.00      0.00        15\n",
      "        446       0.00      0.00      0.00        10\n",
      "        447       1.00      0.67      0.80        12\n",
      "        448       0.00      0.00      0.00         3\n",
      "        450       0.00      0.00      0.00         3\n",
      "        451       1.00      0.90      0.95        21\n",
      "        452       0.00      0.00      0.00         3\n",
      "        454       0.00      0.00      0.00         1\n",
      "        455       0.00      0.00      0.00         2\n",
      "        456       0.00      0.00      0.00         1\n",
      "        459       0.00      0.00      0.00         1\n",
      "        460       0.00      0.00      0.00         2\n",
      "        461       0.00      0.00      0.00         1\n",
      "        462       1.00      1.00      1.00        15\n",
      "        463       1.00      0.75      0.86        16\n",
      "        464       1.00      0.30      0.46        10\n",
      "        465       0.00      0.00      0.00         2\n",
      "        466       1.00      1.00      1.00         7\n",
      "        467       0.00      0.00      0.00         4\n",
      "        469       0.00      0.00      0.00         3\n",
      "        470       0.00      0.00      0.00         1\n",
      "        471       0.00      0.00      0.00         5\n",
      "        473       0.00      0.00      0.00         1\n",
      "        474       1.00      1.00      1.00        22\n",
      "        475       0.00      0.00      0.00         3\n",
      "        476       1.00      0.97      0.99        68\n",
      "        477       0.00      0.00      0.00         6\n",
      "        478       0.00      0.00      0.00         2\n",
      "        479       0.00      0.00      0.00         6\n",
      "        480       0.00      0.00      0.00         1\n",
      "        481       0.88      0.70      0.78        10\n",
      "        482       0.95      1.00      0.97        18\n",
      "        483       0.00      0.00      0.00         2\n",
      "        486       0.00      0.00      0.00         1\n",
      "        487       0.00      0.00      0.00         4\n",
      "        488       0.00      0.00      0.00         1\n",
      "        489       0.00      0.00      0.00         1\n",
      "        490       0.95      0.95      0.95        22\n",
      "        491       0.00      0.00      0.00         4\n",
      "        492       0.00      0.00      0.00         2\n",
      "        493       0.00      0.00      0.00         3\n",
      "        494       0.17      0.17      0.17         6\n",
      "        495       0.00      0.00      0.00         2\n",
      "        496       0.33      0.06      0.11        16\n",
      "        497       0.00      0.00      0.00         1\n",
      "        498       0.00      0.00      0.00         2\n",
      "        499       0.00      0.00      0.00         4\n",
      "        500       0.00      0.00      0.00         1\n",
      "        501       0.76      0.81      0.79        32\n",
      "        502       0.64      0.50      0.56        14\n",
      "        503       1.00      0.96      0.98        27\n",
      "        504       1.00      0.64      0.78        11\n",
      "        505       0.00      0.00      0.00         5\n",
      "        506       0.00      0.00      0.00         1\n",
      "        507       0.00      0.00      0.00         6\n",
      "        508       0.00      0.00      0.00         3\n",
      "        509       1.00      1.00      1.00         8\n",
      "        510       0.00      0.00      0.00         3\n",
      "        511       0.89      0.67      0.76        12\n",
      "        512       1.00      1.00      1.00       153\n",
      "        513       0.00      0.00      0.00         1\n",
      "        514       1.00      1.00      1.00         9\n",
      "        515       0.71      1.00      0.83        12\n",
      "        516       0.00      0.00      0.00         4\n",
      "        517       0.94      1.00      0.97        44\n",
      "        518       1.00      0.14      0.25         7\n",
      "        519       0.00      0.00      0.00         2\n",
      "        520       0.00      0.00      0.00         2\n",
      "        521       1.00      0.91      0.95        22\n",
      "        522       0.00      0.00      0.00         2\n",
      "        524       0.00      0.00      0.00         1\n",
      "        525       0.72      0.72      0.72        18\n",
      "        526       0.00      0.00      0.00         3\n",
      "        527       0.00      0.00      0.00         3\n",
      "        528       0.00      0.00      0.00         5\n",
      "        529       0.00      0.00      0.00         3\n",
      "        530       0.00      0.00      0.00         6\n",
      "        531       0.00      0.00      0.00         2\n",
      "        532       0.75      0.90      0.82        10\n",
      "        533       0.00      0.00      0.00         2\n",
      "        534       0.25      0.09      0.13        11\n",
      "        535       0.00      0.00      0.00         2\n",
      "        537       0.00      0.00      0.00         4\n",
      "        538       0.00      0.00      0.00         1\n",
      "        539       0.60      0.38      0.46         8\n",
      "        541       0.87      0.62      0.72        21\n",
      "        542       0.00      0.00      0.00         1\n",
      "        543       0.00      0.00      0.00         1\n",
      "        544       0.00      0.00      0.00         6\n",
      "        545       0.00      0.00      0.00         1\n",
      "        547       0.00      0.00      0.00         3\n",
      "        548       0.00      0.00      0.00         6\n",
      "        549       0.67      0.20      0.31        10\n",
      "        550       0.00      0.00      0.00         4\n",
      "        552       0.00      0.00      0.00         1\n",
      "        553       0.83      0.76      0.79        25\n",
      "        555       0.00      0.00      0.00         4\n",
      "        556       0.00      0.00      0.00         1\n",
      "        557       1.00      1.00      1.00        80\n",
      "        558       0.00      0.00      0.00         5\n",
      "        559       1.00      0.25      0.40         8\n",
      "        560       0.00      0.00      0.00         2\n",
      "        561       1.00      0.80      0.89         5\n",
      "        564       0.00      0.00      0.00         4\n",
      "        566       0.00      0.00      0.00         1\n",
      "        567       0.00      0.00      0.00         3\n",
      "        568       1.00      0.75      0.86         8\n",
      "        569       0.00      0.00      0.00         1\n",
      "        570       1.00      0.60      0.75         5\n",
      "        572       1.00      0.33      0.50         6\n",
      "        573       0.00      0.00      0.00         2\n",
      "        574       0.00      0.00      0.00         2\n",
      "        575       0.00      0.00      0.00         7\n",
      "        576       0.00      0.00      0.00         1\n",
      "        577       0.00      0.00      0.00         3\n",
      "        579       0.00      0.00      0.00         2\n",
      "        580       1.00      1.00      1.00        11\n",
      "        582       0.00      0.00      0.00         3\n",
      "        586       1.00      0.67      0.80         6\n",
      "        587       0.00      0.00      0.00         3\n",
      "        588       0.00      0.00      0.00         4\n",
      "        589       0.00      0.00      0.00         5\n",
      "        590       0.00      0.00      0.00         4\n",
      "        591       0.00      0.00      0.00         3\n",
      "        592       0.00      0.00      0.00         2\n",
      "        593       0.00      0.00      0.00         5\n",
      "        594       1.00      0.82      0.90        17\n",
      "        597       0.91      0.92      0.92        53\n",
      "        598       0.00      0.00      0.00         4\n",
      "        599       0.00      0.00      0.00         5\n",
      "        600       0.00      0.00      0.00         5\n",
      "        601       0.00      0.00      0.00         1\n",
      "        602       0.80      0.67      0.73         6\n",
      "        603       0.00      0.00      0.00         2\n",
      "        604       0.00      0.00      0.00         4\n",
      "        605       0.00      0.00      0.00         2\n",
      "        607       0.00      0.00      0.00         1\n",
      "        608       0.00      0.00      0.00         1\n",
      "        609       0.00      0.00      0.00         2\n",
      "        610       0.00      0.00      0.00         2\n",
      "        611       0.00      0.00      0.00         1\n",
      "        612       0.50      0.38      0.43         8\n",
      "        613       0.00      0.00      0.00         1\n",
      "        614       0.73      0.57      0.64        14\n",
      "        615       0.00      0.00      0.00         3\n",
      "        616       0.44      0.80      0.57        10\n",
      "        617       0.00      0.00      0.00         1\n",
      "        618       0.00      0.00      0.00         1\n",
      "        620       0.00      0.00      0.00         4\n",
      "        624       0.00      0.00      0.00         3\n",
      "        625       0.00      0.00      0.00         1\n",
      "        626       0.00      0.00      0.00         2\n",
      "        628       0.00      0.00      0.00         1\n",
      "        629       0.00      0.00      0.00         1\n",
      "        630       0.00      0.00      0.00         1\n",
      "        631       0.00      0.00      0.00         4\n",
      "        632       0.00      0.00      0.00         1\n",
      "        633       0.00      0.00      0.00         4\n",
      "        634       0.00      0.00      0.00         2\n",
      "        636       0.00      0.00      0.00         1\n",
      "        637       0.00      0.00      0.00         2\n",
      "        638       0.00      0.00      0.00         6\n",
      "        641       0.00      0.00      0.00         5\n",
      "        643       0.00      0.00      0.00         2\n",
      "        644       1.00      0.22      0.36         9\n",
      "        645       1.00      0.71      0.83        21\n",
      "        646       0.00      0.00      0.00         1\n",
      "        647       0.00      0.00      0.00         4\n",
      "        649       0.00      0.00      0.00         1\n",
      "        650       1.00      0.20      0.33         5\n",
      "        651       1.00      0.90      0.95        10\n",
      "        652       0.00      0.00      0.00        19\n",
      "        655       0.00      0.00      0.00         4\n",
      "        656       0.00      0.00      0.00         2\n",
      "        657       0.00      0.00      0.00         2\n",
      "        658       0.00      0.00      0.00         2\n",
      "        659       0.00      0.00      0.00         1\n",
      "        660       0.00      0.00      0.00         5\n",
      "        662       0.00      0.00      0.00         1\n",
      "        663       1.00      0.29      0.44         7\n",
      "        665       0.00      0.00      0.00         4\n",
      "        667       0.75      0.75      0.75         8\n",
      "        668       0.00      0.00      0.00         4\n",
      "        669       0.00      0.00      0.00         5\n",
      "        670       0.00      0.00      0.00         3\n",
      "        671       0.67      0.20      0.31        10\n",
      "        672       0.00      0.00      0.00         2\n",
      "        674       0.00      0.00      0.00         2\n",
      "        675       0.80      0.67      0.73         6\n",
      "        677       0.00      0.00      0.00         1\n",
      "        678       0.00      0.00      0.00         1\n",
      "        679       0.00      0.00      0.00         1\n",
      "        680       0.00      0.00      0.00         1\n",
      "        682       0.00      0.00      0.00         7\n",
      "        683       0.00      0.00      0.00         2\n",
      "        685       0.00      0.00      0.00         2\n",
      "        688       1.00      0.86      0.92         7\n",
      "        690       0.00      0.00      0.00         2\n",
      "        692       0.00      0.00      0.00         3\n",
      "        693       0.00      0.00      0.00         4\n",
      "        694       1.00      0.71      0.83        17\n",
      "        695       0.00      0.00      0.00         3\n",
      "        696       0.00      0.00      0.00         3\n",
      "        697       0.00      0.00      0.00         1\n",
      "        698       0.75      0.43      0.55         7\n",
      "        699       0.94      1.00      0.97        16\n",
      "        701       0.25      0.20      0.22         5\n",
      "        702       0.00      0.00      0.00         1\n",
      "        703       0.00      0.00      0.00         1\n",
      "        705       0.00      0.00      0.00         3\n",
      "        706       1.00      0.50      0.67         8\n",
      "        707       0.00      0.00      0.00         1\n",
      "        708       1.00      0.20      0.33         5\n",
      "        714       0.00      0.00      0.00         4\n",
      "        715       0.00      0.00      0.00         1\n",
      "        716       0.00      0.00      0.00         1\n",
      "        717       0.00      0.00      0.00         1\n",
      "        718       0.00      0.00      0.00         3\n",
      "        719       0.00      0.00      0.00         3\n",
      "        720       0.00      0.00      0.00         5\n",
      "        721       0.00      0.00      0.00         5\n",
      "        722       0.00      0.00      0.00         4\n",
      "        723       0.00      0.00      0.00         1\n",
      "        724       0.00      0.00      0.00         1\n",
      "        725       0.00      0.00      0.00         3\n",
      "        726       1.00      1.00      1.00        16\n",
      "        728       1.00      0.67      0.80         6\n",
      "        730       0.00      0.00      0.00         1\n",
      "        731       0.00      0.00      0.00         1\n",
      "        732       1.00      0.80      0.89         5\n",
      "        733       0.00      0.00      0.00         1\n",
      "        734       0.00      0.00      0.00         4\n",
      "        735       0.00      0.00      0.00         1\n",
      "        736       0.00      0.00      0.00        10\n",
      "        737       0.55      0.50      0.52        12\n",
      "        739       0.00      0.00      0.00         1\n",
      "        741       0.00      0.00      0.00         3\n",
      "        742       0.00      0.00      0.00         3\n",
      "        743       0.00      0.00      0.00         1\n",
      "        745       0.00      0.00      0.00         1\n",
      "        748       0.00      0.00      0.00         1\n",
      "        749       0.00      0.00      0.00         2\n",
      "        750       0.00      0.00      0.00         2\n",
      "        753       0.00      0.00      0.00         1\n",
      "        755       0.00      0.00      0.00         2\n",
      "        756       0.00      0.00      0.00         7\n",
      "        757       0.00      0.00      0.00         2\n",
      "        758       0.00      0.00      0.00         1\n",
      "        760       0.00      0.00      0.00         5\n",
      "        761       0.00      0.00      0.00         3\n",
      "        762       0.00      0.00      0.00         4\n",
      "        764       0.00      0.00      0.00         1\n",
      "        765       0.00      0.00      0.00         2\n",
      "        766       0.88      0.70      0.78        10\n",
      "        767       0.00      0.00      0.00         1\n",
      "        768       0.00      0.00      0.00         2\n",
      "        769       0.00      0.00      0.00         3\n",
      "        770       0.00      0.00      0.00         2\n",
      "        772       0.00      0.00      0.00         2\n",
      "        773       0.00      0.00      0.00         1\n",
      "        774       0.75      0.38      0.50         8\n",
      "        775       0.00      0.00      0.00         1\n",
      "        776       0.00      0.00      0.00         2\n",
      "        782       0.00      0.00      0.00         1\n",
      "        783       0.00      0.00      0.00         4\n",
      "        785       0.00      0.00      0.00         2\n",
      "        786       0.00      0.00      0.00         4\n",
      "        787       0.00      0.00      0.00         1\n",
      "        788       0.80      0.62      0.70        13\n",
      "        789       0.00      0.00      0.00         4\n",
      "        792       0.71      1.00      0.83        10\n",
      "        795       0.00      0.00      0.00         1\n",
      "        799       0.00      0.00      0.00         2\n",
      "        800       0.00      0.00      0.00         3\n",
      "        801       0.00      0.00      0.00         3\n",
      "        803       0.00      0.00      0.00         1\n",
      "        804       0.00      0.00      0.00         4\n",
      "        806       0.00      0.00      0.00         1\n",
      "        808       0.00      0.00      0.00         2\n",
      "        810       0.00      0.00      0.00         4\n",
      "        811       0.00      0.00      0.00         1\n",
      "        814       0.00      0.00      0.00         2\n",
      "        815       0.00      0.00      0.00         1\n",
      "        816       0.00      0.00      0.00         1\n",
      "        817       0.00      0.00      0.00         4\n",
      "        823       0.00      0.00      0.00         6\n",
      "        828       0.80      0.50      0.62         8\n",
      "        829       0.00      0.00      0.00         3\n",
      "        830       0.00      0.00      0.00         4\n",
      "        831       0.00      0.00      0.00         1\n",
      "        832       0.00      0.00      0.00         1\n",
      "        833       0.57      0.44      0.50         9\n",
      "        835       0.00      0.00      0.00         2\n",
      "        836       1.00      0.29      0.44         7\n",
      "        837       0.00      0.00      0.00         3\n",
      "        839       0.00      0.00      0.00         1\n",
      "        842       1.00      0.83      0.91         6\n",
      "        843       1.00      0.17      0.29         6\n",
      "        844       0.00      0.00      0.00         2\n",
      "        846       0.00      0.00      0.00         1\n",
      "        847       0.00      0.00      0.00         5\n",
      "        848       0.00      0.00      0.00         1\n",
      "        849       0.00      0.00      0.00         3\n",
      "        852       0.00      0.00      0.00         4\n",
      "        853       0.00      0.00      0.00         1\n",
      "        855       0.00      0.00      0.00         1\n",
      "        856       0.00      0.00      0.00         1\n",
      "        857       0.00      0.00      0.00         1\n",
      "        858       1.00      0.20      0.33         5\n",
      "        861       0.00      0.00      0.00         1\n",
      "        862       0.00      0.00      0.00         6\n",
      "        863       0.00      0.00      0.00         1\n",
      "        865       0.00      0.00      0.00         1\n",
      "        867       0.00      0.00      0.00         1\n",
      "        868       0.00      0.00      0.00         2\n",
      "        871       0.00      0.00      0.00         1\n",
      "        872       1.00      0.60      0.75         5\n",
      "        874       0.00      0.00      0.00         4\n",
      "        875       0.67      0.33      0.44         6\n",
      "        878       0.00      0.00      0.00         2\n",
      "        879       0.00      0.00      0.00         1\n",
      "        882       0.00      0.00      0.00         4\n",
      "        883       0.00      0.00      0.00         1\n",
      "        885       1.00      0.20      0.33         5\n",
      "        886       0.00      0.00      0.00         4\n",
      "        888       0.00      0.00      0.00         1\n",
      "        890       0.00      0.00      0.00         2\n",
      "        892       0.00      0.00      0.00         4\n",
      "        893       0.00      0.00      0.00         3\n",
      "        895       0.00      0.00      0.00         2\n",
      "        896       0.00      0.00      0.00         2\n",
      "        897       0.00      0.00      0.00         5\n",
      "        901       0.00      0.00      0.00         2\n",
      "        902       0.00      0.00      0.00         6\n",
      "        903       0.00      0.00      0.00         1\n",
      "        904       0.00      0.00      0.00         1\n",
      "        905       0.00      0.00      0.00         1\n",
      "        907       0.00      0.00      0.00         1\n",
      "        908       0.00      0.00      0.00         2\n",
      "        909       0.00      0.00      0.00         2\n",
      "        910       0.00      0.00      0.00         1\n",
      "        914       0.00      0.00      0.00         4\n",
      "        915       0.00      0.00      0.00         2\n",
      "        916       0.00      0.00      0.00         3\n",
      "        917       0.00      0.00      0.00         1\n",
      "        920       0.00      0.00      0.00         5\n",
      "        921       0.00      0.00      0.00         1\n",
      "        922       0.00      0.00      0.00         2\n",
      "        930       0.00      0.00      0.00         2\n",
      "        933       0.00      0.00      0.00         2\n",
      "        935       0.00      0.00      0.00         2\n",
      "        936       0.00      0.00      0.00         1\n",
      "        937       0.00      0.00      0.00         3\n",
      "        938       0.00      0.00      0.00         3\n",
      "        939       0.00      0.00      0.00         4\n",
      "        940       0.00      0.00      0.00         1\n",
      "        944       0.00      0.00      0.00         2\n",
      "        945       0.00      0.00      0.00         1\n",
      "        946       0.00      0.00      0.00         1\n",
      "        947       0.00      0.00      0.00         1\n",
      "        948       0.00      0.00      0.00         2\n",
      "        949       0.00      0.00      0.00         4\n",
      "        951       0.00      0.00      0.00         1\n",
      "        954       0.00      0.00      0.00         2\n",
      "        955       0.00      0.00      0.00         1\n",
      "        956       0.00      0.00      0.00         5\n",
      "        957       0.00      0.00      0.00         4\n",
      "        959       1.00      0.75      0.86         8\n",
      "        960       0.00      0.00      0.00         1\n",
      "        961       0.00      0.00      0.00         2\n",
      "        962       1.00      1.00      1.00        21\n",
      "        964       0.77      0.83      0.80        12\n",
      "        969       0.00      0.00      0.00         1\n",
      "        970       0.00      0.00      0.00         3\n",
      "        971       0.00      0.00      0.00         1\n",
      "        973       0.00      0.00      0.00         1\n",
      "        974       0.00      0.00      0.00         1\n",
      "        976       0.00      0.00      0.00         1\n",
      "        977       0.00      0.00      0.00         2\n",
      "        978       0.00      0.00      0.00         1\n",
      "        979       0.00      0.00      0.00         1\n",
      "        980       0.00      0.00      0.00         2\n",
      "        982       0.00      0.00      0.00         1\n",
      "        983       0.00      0.00      0.00         1\n",
      "        984       0.00      0.00      0.00         2\n",
      "        985       0.00      0.00      0.00         1\n",
      "        987       0.00      0.00      0.00         2\n",
      "        988       0.00      0.00      0.00         4\n",
      "        989       0.00      0.00      0.00         1\n",
      "        990       0.00      0.00      0.00         1\n",
      "        991       0.00      0.00      0.00         6\n",
      "        992       0.00      0.00      0.00         3\n",
      "        993       0.00      0.00      0.00         1\n",
      "        994       0.00      0.00      0.00         3\n",
      "        995       0.00      0.00      0.00         1\n",
      "        997       0.00      0.00      0.00         1\n",
      "        998       0.00      0.00      0.00         1\n",
      "        999       0.00      0.00      0.00         1\n",
      "       1000       0.25      0.11      0.15         9\n",
      "       1004       0.00      0.00      0.00         1\n",
      "       1007       0.00      0.00      0.00         6\n",
      "       1010       0.00      0.00      0.00         3\n",
      "       1011       0.00      0.00      0.00         1\n",
      "       1012       0.00      0.00      0.00         1\n",
      "       1013       0.86      1.00      0.92         6\n",
      "       1014       0.00      0.00      0.00         1\n",
      "       1015       0.00      0.00      0.00         5\n",
      "       1016       0.00      0.00      0.00         1\n",
      "       1017       0.00      0.00      0.00         1\n",
      "       1019       0.00      0.00      0.00         1\n",
      "       1020       0.00      0.00      0.00         2\n",
      "       1021       0.00      0.00      0.00         1\n",
      "       1023       0.82      0.82      0.82        11\n",
      "       1024       0.00      0.00      0.00         1\n",
      "       1025       0.00      0.00      0.00         1\n",
      "       1027       0.00      0.00      0.00         1\n",
      "       1028       0.00      0.00      0.00         1\n",
      "       1031       0.00      0.00      0.00         1\n",
      "       1032       0.00      0.00      0.00         1\n",
      "       1033       0.00      0.00      0.00         1\n",
      "       1034       0.00      0.00      0.00         2\n",
      "       1035       0.00      0.00      0.00         1\n",
      "       1038       0.00      0.00      0.00         3\n",
      "       1039       0.00      0.00      0.00         2\n",
      "       1040       0.00      0.00      0.00         1\n",
      "       1044       0.00      0.00      0.00         2\n",
      "       1045       0.00      0.00      0.00         1\n",
      "       1046       0.00      0.00      0.00         1\n",
      "       1047       0.00      0.00      0.00         3\n",
      "       1048       0.00      0.00      0.00         1\n",
      "       1049       0.00      0.00      0.00         1\n",
      "       1050       0.00      0.00      0.00         2\n",
      "       1051       0.83      0.71      0.77         7\n",
      "       1053       0.00      0.00      0.00         1\n",
      "       1055       0.00      0.00      0.00         3\n",
      "       1057       0.00      0.00      0.00         3\n",
      "       1058       0.00      0.00      0.00         1\n",
      "       1059       0.00      0.00      0.00         1\n",
      "       1061       0.00      0.00      0.00         1\n",
      "       1062       0.00      0.00      0.00         1\n",
      "       1063       0.00      0.00      0.00         1\n",
      "       1064       0.00      0.00      0.00         2\n",
      "       1065       0.00      0.00      0.00         2\n",
      "       1066       0.00      0.00      0.00         1\n",
      "       1068       0.00      0.00      0.00         1\n",
      "       1069       0.00      0.00      0.00         1\n",
      "       1070       1.00      0.20      0.33         5\n",
      "       1071       0.00      0.00      0.00         1\n",
      "       1073       0.00      0.00      0.00         1\n",
      "       1075       0.00      0.00      0.00         2\n",
      "       1077       0.00      0.00      0.00         2\n",
      "       1078       0.00      0.00      0.00         3\n",
      "       1079       0.00      0.00      0.00         2\n",
      "       1080       0.00      0.00      0.00         3\n",
      "       1081       0.00      0.00      0.00         2\n",
      "       1082       1.00      1.00      1.00         7\n",
      "       1083       0.00      0.00      0.00         1\n",
      "       1084       0.00      0.00      0.00         1\n",
      "       1087       0.00      0.00      0.00         1\n",
      "       1089       0.00      0.00      0.00         1\n",
      "       1093       0.00      0.00      0.00         3\n",
      "       1096       0.00      0.00      0.00         2\n",
      "       1097       0.00      0.00      0.00         1\n",
      "       1099       0.00      0.00      0.00         1\n",
      "       1100       0.00      0.00      0.00         2\n",
      "       1101       0.00      0.00      0.00         4\n",
      "       1102       0.00      0.00      0.00         1\n",
      "       1103       0.00      0.00      0.00         4\n",
      "       1104       0.00      0.00      0.00         1\n",
      "       1106       0.00      0.00      0.00         2\n",
      "       1107       0.00      0.00      0.00         4\n",
      "       1108       0.00      0.00      0.00         1\n",
      "       1110       0.00      0.00      0.00         2\n",
      "       1111       0.00      0.00      0.00         1\n",
      "       1113       0.00      0.00      0.00         3\n",
      "       1114       0.00      0.00      0.00         1\n",
      "       1115       0.00      0.00      0.00         1\n",
      "       1116       0.00      0.00      0.00         2\n",
      "       1120       0.00      0.00      0.00         1\n",
      "       1121       0.00      0.00      0.00         2\n",
      "       1124       0.00      0.00      0.00         1\n",
      "       1126       0.00      0.00      0.00         2\n",
      "       1127       0.00      0.00      0.00         5\n",
      "       1128       0.00      0.00      0.00         1\n",
      "       1130       0.00      0.00      0.00         2\n",
      "       1131       0.00      0.00      0.00         2\n",
      "       1132       0.00      0.00      0.00         2\n",
      "       1136       0.00      0.00      0.00         1\n",
      "       1139       0.00      0.00      0.00         3\n",
      "       1141       0.00      0.00      0.00         1\n",
      "       1142       0.00      0.00      0.00         1\n",
      "       1143       0.00      0.00      0.00         2\n",
      "       1146       0.00      0.00      0.00         3\n",
      "       1149       0.00      0.00      0.00         1\n",
      "       1152       0.00      0.00      0.00         2\n",
      "       1153       0.00      0.00      0.00         6\n",
      "       1154       0.00      0.00      0.00         4\n",
      "       1155       0.00      0.00      0.00         1\n",
      "       1157       0.00      0.00      0.00         1\n",
      "       1158       0.00      0.00      0.00         1\n",
      "       1159       0.00      0.00      0.00         4\n",
      "       1161       0.00      0.00      0.00         1\n",
      "       1162       0.00      0.00      0.00         2\n",
      "       1165       0.00      0.00      0.00         1\n",
      "       1166       0.00      0.00      0.00         1\n",
      "       1167       0.00      0.00      0.00         3\n",
      "       1168       0.00      0.00      0.00         2\n",
      "       1170       0.00      0.00      0.00         1\n",
      "       1171       0.00      0.00      0.00         1\n",
      "       1174       0.00      0.00      0.00         1\n",
      "       1175       0.00      0.00      0.00         3\n",
      "       1178       0.00      0.00      0.00         1\n",
      "       1179       0.00      0.00      0.00         1\n",
      "       1180       0.00      0.00      0.00         1\n",
      "       1183       0.00      0.00      0.00         6\n",
      "       1186       0.00      0.00      0.00         1\n",
      "       1188       0.00      0.00      0.00         3\n",
      "       1190       0.00      0.00      0.00         1\n",
      "       1193       0.00      0.00      0.00         1\n",
      "       1195       0.00      0.00      0.00         1\n",
      "       1196       0.00      0.00      0.00         2\n",
      "       1197       0.00      0.00      0.00         4\n",
      "       1198       0.00      0.00      0.00         1\n",
      "       1199       0.00      0.00      0.00         1\n",
      "       1201       0.00      0.00      0.00         1\n",
      "       1202       0.00      0.00      0.00         1\n",
      "       1206       0.00      0.00      0.00         1\n",
      "       1209       0.00      0.00      0.00         1\n",
      "       1210       0.00      0.00      0.00         4\n",
      "       1211       0.00      0.00      0.00         1\n",
      "       1213       0.00      0.00      0.00         2\n",
      "       1215       0.00      0.00      0.00         1\n",
      "       1219       0.00      0.00      0.00         1\n",
      "       1220       0.00      0.00      0.00         2\n",
      "       1221       0.00      0.00      0.00         3\n",
      "       1223       0.00      0.00      0.00         4\n",
      "       1224       0.00      0.00      0.00         1\n",
      "       1226       0.00      0.00      0.00         1\n",
      "       1229       0.00      0.00      0.00         2\n",
      "       1230       0.00      0.00      0.00         1\n",
      "       1234       0.00      0.00      0.00         2\n",
      "       1236       0.00      0.00      0.00         1\n",
      "       1238       0.00      0.00      0.00         1\n",
      "       1239       0.00      0.00      0.00         1\n",
      "       1240       0.00      0.00      0.00         1\n",
      "       1244       0.00      0.00      0.00         1\n",
      "       1245       0.00      0.00      0.00         1\n",
      "       1247       0.00      0.00      0.00         1\n",
      "       1251       0.00      0.00      0.00         2\n",
      "       1255       0.00      0.00      0.00         1\n",
      "       1258       0.00      0.00      0.00         2\n",
      "       1260       0.00      0.00      0.00         1\n",
      "       1263       0.00      0.00      0.00         1\n",
      "       1264       0.00      0.00      0.00         2\n",
      "       1266       0.00      0.00      0.00         1\n",
      "       1268       0.00      0.00      0.00         2\n",
      "       1269       0.00      0.00      0.00         2\n",
      "       1270       0.00      0.00      0.00         1\n",
      "       1271       0.00      0.00      0.00         3\n",
      "       1275       0.00      0.00      0.00         1\n",
      "       1277       0.00      0.00      0.00         1\n",
      "       1278       0.00      0.00      0.00         2\n",
      "       1279       0.00      0.00      0.00         2\n",
      "       1280       0.00      0.00      0.00         2\n",
      "       1281       0.00      0.00      0.00         1\n",
      "       1283       0.00      0.00      0.00         2\n",
      "       1286       0.00      0.00      0.00         2\n",
      "       1289       0.00      0.00      0.00         1\n",
      "       1290       0.00      0.00      0.00         1\n",
      "       1291       0.00      0.00      0.00         2\n",
      "       1292       0.00      0.00      0.00         1\n",
      "       1293       0.00      0.00      0.00         1\n",
      "       1294       0.00      0.00      0.00         1\n",
      "       1295       0.00      0.00      0.00         1\n",
      "       1298       0.00      0.00      0.00         5\n",
      "       1299       0.00      0.00      0.00         3\n",
      "       1300       0.00      0.00      0.00         4\n",
      "       1304       0.00      0.00      0.00         3\n",
      "       1305       0.00      0.00      0.00         3\n",
      "       1306       0.00      0.00      0.00         2\n",
      "       1307       0.00      0.00      0.00         1\n",
      "       1308       0.00      0.00      0.00         1\n",
      "       1309       0.00      0.00      0.00         1\n",
      "       1310       0.00      0.00      0.00         4\n",
      "       1311       0.00      0.00      0.00         2\n",
      "       1313       0.00      0.00      0.00         2\n",
      "       1316       0.00      0.00      0.00         1\n",
      "       1317       0.00      0.00      0.00         1\n",
      "       1318       0.00      0.00      0.00         3\n",
      "       1321       0.00      0.00      0.00         1\n",
      "       1328       0.00      0.00      0.00         1\n",
      "       1330       0.00      0.00      0.00         1\n",
      "       1331       0.00      0.00      0.00         2\n",
      "       1332       0.00      0.00      0.00         1\n",
      "       1333       0.00      0.00      0.00         1\n",
      "       1336       0.00      0.00      0.00         2\n",
      "       1338       0.00      0.00      0.00         1\n",
      "       1339       0.00      0.00      0.00         1\n",
      "       1341       0.00      0.00      0.00         1\n",
      "       1342       0.00      0.00      0.00         2\n",
      "       1343       0.00      0.00      0.00         4\n",
      "       1345       0.00      0.00      0.00         1\n",
      "       1347       0.00      0.00      0.00         1\n",
      "       1351       0.00      0.00      0.00         1\n",
      "       1352       0.00      0.00      0.00         1\n",
      "       1353       0.00      0.00      0.00         1\n",
      "       1355       0.00      0.00      0.00         2\n",
      "       1359       0.00      0.00      0.00         1\n",
      "       1360       0.00      0.00      0.00         1\n",
      "       1365       0.00      0.00      0.00         3\n",
      "       1366       0.00      0.00      0.00         1\n",
      "       1367       0.00      0.00      0.00         1\n",
      "       1371       0.00      0.00      0.00         2\n",
      "       1373       0.00      0.00      0.00         1\n",
      "       1376       0.00      0.00      0.00         3\n",
      "       1377       0.00      0.00      0.00         4\n",
      "       1382       0.00      0.00      0.00         3\n",
      "       1383       0.00      0.00      0.00         1\n",
      "       1386       0.00      0.00      0.00         1\n",
      "       1389       0.00      0.00      0.00         1\n",
      "       1393       0.00      0.00      0.00         1\n",
      "       1394       0.00      0.00      0.00         1\n",
      "       1398       0.00      0.00      0.00         1\n",
      "       1402       0.00      0.00      0.00         1\n",
      "       1403       0.00      0.00      0.00         2\n",
      "       1404       0.00      0.00      0.00         2\n",
      "       1410       0.00      0.00      0.00         1\n",
      "       1414       0.00      0.00      0.00         1\n",
      "       1416       0.00      0.00      0.00         1\n",
      "       1417       0.00      0.00      0.00         3\n",
      "       1418       0.00      0.00      0.00         1\n",
      "       1419       0.00      0.00      0.00         2\n",
      "       1420       0.00      0.00      0.00         1\n",
      "       1422       0.00      0.00      0.00         2\n",
      "       1423       0.00      0.00      0.00         2\n",
      "       1424       0.00      0.00      0.00         1\n",
      "       1425       0.00      0.00      0.00         1\n",
      "       1428       0.00      0.00      0.00         1\n",
      "       1429       0.00      0.00      0.00         2\n",
      "       1431       0.00      0.00      0.00         1\n",
      "       1434       0.00      0.00      0.00         4\n",
      "       1435       0.00      0.00      0.00         1\n",
      "       1439       0.00      0.00      0.00         1\n",
      "       1440       0.00      0.00      0.00         1\n",
      "       1442       0.00      0.00      0.00         1\n",
      "       1443       0.00      0.00      0.00         1\n",
      "       1444       0.00      0.00      0.00         1\n",
      "       1445       0.00      0.00      0.00         1\n",
      "       1447       0.00      0.00      0.00         1\n",
      "       1448       0.00      0.00      0.00         1\n",
      "       1451       0.00      0.00      0.00         1\n",
      "       1455       0.00      0.00      0.00         1\n",
      "       1456       0.00      0.00      0.00         1\n",
      "       1457       0.00      0.00      0.00         4\n",
      "       1458       0.00      0.00      0.00         1\n",
      "       1460       0.00      0.00      0.00         1\n",
      "       1461       0.00      0.00      0.00         3\n",
      "       1462       0.00      0.00      0.00         1\n",
      "       1464       0.00      0.00      0.00         3\n",
      "       1465       0.00      0.00      0.00         2\n",
      "       1467       0.00      0.00      0.00         1\n",
      "       1468       0.00      0.00      0.00         1\n",
      "       1469       0.00      0.00      0.00         3\n",
      "       1470       0.00      0.00      0.00         2\n",
      "       1474       0.00      0.00      0.00         1\n",
      "       1478       0.00      0.00      0.00         4\n",
      "       1483       0.00      0.00      0.00         1\n",
      "       1491       0.00      0.00      0.00         1\n",
      "       1493       0.00      0.00      0.00         1\n",
      "       1494       0.00      0.00      0.00         1\n",
      "       1496       0.00      0.00      0.00         1\n",
      "       1498       0.00      0.00      0.00         3\n",
      "       1499       0.00      0.00      0.00         1\n",
      "       1501       0.00      0.00      0.00         1\n",
      "       1503       0.00      0.00      0.00         1\n",
      "       1504       0.00      0.00      0.00         1\n",
      "       1505       0.00      0.00      0.00         1\n",
      "       1511       0.00      0.00      0.00         3\n",
      "       1512       0.00      0.00      0.00         1\n",
      "       1513       0.00      0.00      0.00         2\n",
      "       1515       0.00      0.00      0.00         1\n",
      "       1521       0.00      0.00      0.00         1\n",
      "       1522       0.00      0.00      0.00         2\n",
      "       1524       0.00      0.00      0.00         1\n",
      "       1525       0.00      0.00      0.00         2\n",
      "       1526       0.00      0.00      0.00         1\n",
      "       1528       0.00      0.00      0.00         1\n",
      "       1532       0.00      0.00      0.00         1\n",
      "       1541       0.00      0.00      0.00         1\n",
      "       1548       0.00      0.00      0.00         1\n",
      "       1549       0.00      0.00      0.00         1\n",
      "       1555       0.00      0.00      0.00         1\n",
      "       1558       0.00      0.00      0.00         1\n",
      "       1563       0.00      0.00      0.00         3\n",
      "       1567       0.00      0.00      0.00         1\n",
      "       1569       0.00      0.00      0.00         1\n",
      "       1571       0.00      0.00      0.00         1\n",
      "       1572       0.00      0.00      0.00         1\n",
      "       1575       0.00      0.00      0.00         1\n",
      "       1576       0.00      0.00      0.00         1\n",
      "       1578       0.00      0.00      0.00         1\n",
      "       1579       0.00      0.00      0.00         1\n",
      "       1580       0.00      0.00      0.00         1\n",
      "       1581       0.00      0.00      0.00         1\n",
      "       1587       0.00      0.00      0.00         1\n",
      "       1588       0.00      0.00      0.00         1\n",
      "       1590       0.00      0.00      0.00         1\n",
      "       1594       0.00      0.00      0.00         1\n",
      "       1595       0.00      0.00      0.00         1\n",
      "       1597       0.00      0.00      0.00         1\n",
      "       1598       0.00      0.00      0.00         1\n",
      "       1600       0.00      0.00      0.00         1\n",
      "       1603       0.00      0.00      0.00         2\n",
      "       1604       0.00      0.00      0.00         1\n",
      "       1607       0.00      0.00      0.00         1\n",
      "       1609       0.00      0.00      0.00         1\n",
      "       1610       0.00      0.00      0.00         1\n",
      "       1611       0.00      0.00      0.00         2\n",
      "       1615       0.00      0.00      0.00         1\n",
      "       1621       0.00      0.00      0.00         1\n",
      "       1625       0.00      0.00      0.00         1\n",
      "       1626       0.00      0.00      0.00         1\n",
      "       1629       0.00      0.00      0.00         1\n",
      "       1630       0.00      0.00      0.00         2\n",
      "       1632       0.00      0.00      0.00         3\n",
      "       1633       0.00      0.00      0.00         1\n",
      "       1637       0.00      0.00      0.00         1\n",
      "       1639       0.00      0.00      0.00         2\n",
      "       1643       0.00      0.00      0.00         1\n",
      "       1646       0.00      0.00      0.00         1\n",
      "       1647       0.00      0.00      0.00         2\n",
      "       1648       0.00      0.00      0.00         1\n",
      "       1649       0.00      0.00      0.00         2\n",
      "       1650       0.00      0.00      0.00         1\n",
      "       1653       0.00      0.00      0.00         2\n",
      "       1657       0.00      0.00      0.00         1\n",
      "       1661       0.00      0.00      0.00         2\n",
      "       1662       0.00      0.00      0.00         1\n",
      "       1667       0.00      0.00      0.00         1\n",
      "       1668       0.00      0.00      0.00         1\n",
      "       1669       0.00      0.00      0.00         1\n",
      "       1670       0.00      0.00      0.00         1\n",
      "       1672       0.00      0.00      0.00         2\n",
      "       1673       0.00      0.00      0.00         1\n",
      "       1678       0.00      0.00      0.00         1\n",
      "       1682       0.00      0.00      0.00         1\n",
      "       1686       0.00      0.00      0.00         2\n",
      "       1691       0.00      0.00      0.00         1\n",
      "       1695       0.00      0.00      0.00         1\n",
      "       1697       0.00      0.00      0.00         1\n",
      "       1706       0.00      0.00      0.00         2\n",
      "       1707       0.00      0.00      0.00         1\n",
      "       1710       0.00      0.00      0.00         1\n",
      "       1711       0.00      0.00      0.00         1\n",
      "       1713       0.00      0.00      0.00         1\n",
      "       1714       0.00      0.00      0.00         1\n",
      "       1715       0.00      0.00      0.00         1\n",
      "       1717       0.00      0.00      0.00         1\n",
      "       1718       0.00      0.00      0.00         1\n",
      "       1720       0.00      0.00      0.00         1\n",
      "       1722       0.00      0.00      0.00         1\n",
      "       1723       0.00      0.00      0.00         1\n",
      "       1727       0.00      0.00      0.00         1\n",
      "       1730       0.00      0.00      0.00         1\n",
      "       1731       0.00      0.00      0.00         1\n",
      "       1739       0.00      0.00      0.00         1\n",
      "       1740       0.00      0.00      0.00         2\n",
      "       1748       0.00      0.00      0.00         1\n",
      "       1751       0.00      0.00      0.00         1\n",
      "       1756       0.00      0.00      0.00         1\n",
      "       1758       0.00      0.00      0.00         2\n",
      "       1759       0.00      0.00      0.00         1\n",
      "       1767       0.00      0.00      0.00         1\n",
      "       1768       0.00      0.00      0.00         1\n",
      "       1772       0.00      0.00      0.00         1\n",
      "       1773       0.00      0.00      0.00         1\n",
      "       1775       0.00      0.00      0.00         1\n",
      "       1776       0.00      0.00      0.00         1\n",
      "       1777       0.00      0.00      0.00         1\n",
      "       1778       0.00      0.00      0.00         1\n",
      "       1781       0.00      0.00      0.00         1\n",
      "       1783       0.00      0.00      0.00         2\n",
      "       1785       0.00      0.00      0.00         1\n",
      "       1791       0.00      0.00      0.00         2\n",
      "       1793       0.00      0.00      0.00         1\n",
      "       1795       0.00      0.00      0.00         2\n",
      "       1796       0.00      0.00      0.00         1\n",
      "       1798       0.00      0.00      0.00         1\n",
      "       1799       0.00      0.00      0.00         1\n",
      "       1800       0.00      0.00      0.00         1\n",
      "       1802       0.00      0.00      0.00         1\n",
      "       1803       0.00      0.00      0.00         1\n",
      "       1808       0.00      0.00      0.00         1\n",
      "       1810       0.00      0.00      0.00         1\n",
      "       1812       0.00      0.00      0.00         1\n",
      "       1814       0.00      0.00      0.00         2\n",
      "       1820       0.00      0.00      0.00         1\n",
      "       1823       0.00      0.00      0.00         1\n",
      "       1828       0.00      0.00      0.00         1\n",
      "       1829       0.00      0.00      0.00         1\n",
      "       1831       0.00      0.00      0.00         1\n",
      "       1833       0.00      0.00      0.00         1\n",
      "       1837       0.00      0.00      0.00         1\n",
      "       1845       0.00      0.00      0.00         1\n",
      "       1846       0.00      0.00      0.00         1\n",
      "       1848       0.00      0.00      0.00         1\n",
      "       1849       0.00      0.00      0.00         1\n",
      "       1850       0.00      0.00      0.00         1\n",
      "       1852       0.00      0.00      0.00         1\n",
      "       1853       0.00      0.00      0.00         1\n",
      "       1855       0.00      0.00      0.00         1\n",
      "       1859       0.00      0.00      0.00         1\n",
      "       1864       0.00      0.00      0.00         1\n",
      "       1867       0.00      0.00      0.00         1\n",
      "       1870       0.00      0.00      0.00         2\n",
      "       1871       0.00      0.00      0.00         1\n",
      "       1872       0.00      0.00      0.00         1\n",
      "       1874       0.00      0.00      0.00         1\n",
      "       1875       0.00      0.00      0.00         1\n",
      "       1876       0.00      0.00      0.00         1\n",
      "       1880       0.00      0.00      0.00         1\n",
      "       1884       0.00      0.00      0.00         1\n",
      "       1885       0.00      0.00      0.00         1\n",
      "       1886       0.00      0.00      0.00         1\n",
      "       1889       0.00      0.00      0.00         1\n",
      "       1890       0.00      0.00      0.00         1\n",
      "       1891       0.00      0.00      0.00         1\n",
      "       1893       0.00      0.00      0.00         1\n",
      "       1895       0.00      0.00      0.00         1\n",
      "       1899       0.00      0.00      0.00         1\n",
      "       1903       0.00      0.00      0.00         1\n",
      "       1907       0.00      0.00      0.00         1\n",
      "       1909       0.00      0.00      0.00         1\n",
      "       1911       0.00      0.00      0.00         1\n",
      "       1913       0.00      0.00      0.00         1\n",
      "       1915       0.00      0.00      0.00         1\n",
      "       1918       0.00      0.00      0.00         1\n",
      "       1920       0.00      0.00      0.00         1\n",
      "       1923       0.00      0.00      0.00         1\n",
      "       1930       0.00      0.00      0.00         1\n",
      "       1933       0.00      0.00      0.00         1\n",
      "       1934       0.00      0.00      0.00         1\n",
      "       1935       0.00      0.00      0.00         1\n",
      "       1942       0.00      0.00      0.00         1\n",
      "       1944       0.00      0.00      0.00         1\n",
      "       1948       0.00      0.00      0.00         1\n",
      "       1952       0.00      0.00      0.00         1\n",
      "       1957       0.00      0.00      0.00         1\n",
      "       1958       0.00      0.00      0.00         1\n",
      "       1965       0.00      0.00      0.00         1\n",
      "       1966       0.00      0.00      0.00         1\n",
      "       1968       0.00      0.00      0.00         1\n",
      "       1972       0.00      0.00      0.00         1\n",
      "       1975       0.00      0.00      0.00         1\n",
      "       1976       0.00      0.00      0.00         1\n",
      "       1977       0.00      0.00      0.00         1\n",
      "       1978       0.00      0.00      0.00         1\n",
      "       1981       0.00      0.00      0.00         1\n",
      "       1982       0.00      0.00      0.00         1\n",
      "       1989       0.00      0.00      0.00         1\n",
      "       1990       0.00      0.00      0.00         1\n",
      "       1996       0.00      0.00      0.00         1\n",
      "       1998       0.00      0.00      0.00         1\n",
      "       2001       0.00      0.00      0.00         1\n",
      "       2005       0.00      0.00      0.00         1\n",
      "       2011       0.00      0.00      0.00         1\n",
      "       2012       0.00      0.00      0.00         1\n",
      "       2015       0.00      0.00      0.00         1\n",
      "       2021       0.00      0.00      0.00         1\n",
      "       2022       0.00      0.00      0.00         1\n",
      "       2027       0.00      0.00      0.00         1\n",
      "       2032       0.00      0.00      0.00         1\n",
      "       2033       0.00      0.00      0.00         1\n",
      "       2036       0.00      0.00      0.00         1\n",
      "       2038       0.00      0.00      0.00         1\n",
      "       2039       0.00      0.00      0.00         1\n",
      "       2041       0.00      0.00      0.00         1\n",
      "       2043       0.00      0.00      0.00         1\n",
      "       2044       0.00      0.00      0.00         1\n",
      "       2045       0.00      0.00      0.00         1\n",
      "       2046       0.00      0.00      0.00         1\n",
      "       2047       0.00      0.00      0.00         1\n",
      "       2048       0.00      0.00      0.00         1\n",
      "       2050       0.00      0.00      0.00         1\n",
      "       2053       0.00      0.00      0.00         1\n",
      "       2054       0.00      0.00      0.00         1\n",
      "\n",
      "avg / total       0.87      0.89      0.88     54911\n",
      "\n",
      "score = 0.892\n"
     ]
    },
    {
     "ename": "IOError",
     "evalue": "[Errno 2] No such file or directory: 'data/data/confusion-matrix-lightgbm-vs251.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIOError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-f01c81777161>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"score = {:.3f}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maccuracy_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpred1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mcm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconfusion_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpred1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[0mwrite_confusion_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'data/confusion-matrix-lightgbm-vs251.txt'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-2-b459d1f49a18>\u001b[0m in \u001b[0;36mwrite_confusion_matrix\u001b[1;34m(cm, out_file_name)\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mwrite_confusion_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout_file_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m     \u001b[1;31m#fop = open('data/confusion-matrix-apt.txt','w')\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 38\u001b[1;33m     \u001b[0mfop\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'data/'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mout_file_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'w'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     39\u001b[0m     \u001b[1;31m# this is rubbish ->  cm.tofile(fop, \",\")\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0ma_idx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIOError\u001b[0m: [Errno 2] No such file or directory: 'data/data/confusion-matrix-lightgbm-vs251.txt'"
     ]
    }
   ],
   "source": [
    "# sklearn type API.\n",
    "clf1 = lgb.LGBMClassifier(n_estimators=100, objective='multiclass')\n",
    "pred1 = run_vs_cv(X,y,clf1,10)\n",
    "print(\" \")\n",
    "print(\"Classification report:\")\n",
    "print(classification_report(y, pred1))\n",
    "#print(\"logloss = {:.3f}\".format(log_loss(y, prob1)))\n",
    "print(\"score = {:.3f}\".format(accuracy_score(y, pred1)))\n",
    "cm = confusion_matrix(y, pred1)\n",
    "write_confusion_matrix(cm, 'data/confusion-matrix-lightgbm-vs251.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "write_confusion_matrix(cm, 'confusion-matrix-lightgbm-vs251.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import cPickle\n",
    "fop = open('data/classifier-model-vs251-xtrees-1000.pkl', 'wb')\n",
    "cPickle.dump()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 VirusShare 252 Feature Set Model Selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_name</th>\n",
       "      <th>edx</th>\n",
       "      <th>esi</th>\n",
       "      <th>es</th>\n",
       "      <th>fs</th>\n",
       "      <th>ds</th>\n",
       "      <th>ss</th>\n",
       "      <th>gs</th>\n",
       "      <th>cs</th>\n",
       "      <th>ah</th>\n",
       "      <th>...</th>\n",
       "      <th>vertex_count</th>\n",
       "      <th>edge_count</th>\n",
       "      <th>delta_max</th>\n",
       "      <th>density</th>\n",
       "      <th>entropy</th>\n",
       "      <th>file_size</th>\n",
       "      <th>file_id</th>\n",
       "      <th>percentage</th>\n",
       "      <th>trid_id</th>\n",
       "      <th>packer_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000118d12cbf9ad6103e8b914a6e1ac3</td>\n",
       "      <td>2151</td>\n",
       "      <td>2782</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>37</td>\n",
       "      <td>...</td>\n",
       "      <td>2213</td>\n",
       "      <td>3300</td>\n",
       "      <td>101</td>\n",
       "      <td>0.002277</td>\n",
       "      <td>0.834382</td>\n",
       "      <td>201600</td>\n",
       "      <td>1</td>\n",
       "      <td>2.2</td>\n",
       "      <td>1</td>\n",
       "      <td>1101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00093d5fa5cb7ce77f6eaf39962daa12</td>\n",
       "      <td>4207</td>\n",
       "      <td>11425</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>53</td>\n",
       "      <td>...</td>\n",
       "      <td>4640</td>\n",
       "      <td>12158</td>\n",
       "      <td>204</td>\n",
       "      <td>0.005596</td>\n",
       "      <td>0.803481</td>\n",
       "      <td>742064</td>\n",
       "      <td>1</td>\n",
       "      <td>8.1</td>\n",
       "      <td>17</td>\n",
       "      <td>1101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00099926d51b44c6f8c93a48c2567891</td>\n",
       "      <td>270</td>\n",
       "      <td>1033</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>522</td>\n",
       "      <td>868</td>\n",
       "      <td>284</td>\n",
       "      <td>0.267901</td>\n",
       "      <td>0.997032</td>\n",
       "      <td>725288</td>\n",
       "      <td>4</td>\n",
       "      <td>4.8</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>000a2db4762dc06628a086c9e117f884</td>\n",
       "      <td>43</td>\n",
       "      <td>1281</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>871</td>\n",
       "      <td>959</td>\n",
       "      <td>70</td>\n",
       "      <td>0.035789</td>\n",
       "      <td>0.535436</td>\n",
       "      <td>61551</td>\n",
       "      <td>5</td>\n",
       "      <td>3.5</td>\n",
       "      <td>13</td>\n",
       "      <td>153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>000ae2c63ba69fc93dfc395b40bfe03a</td>\n",
       "      <td>4334</td>\n",
       "      <td>12557</td>\n",
       "      <td>11</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>11</td>\n",
       "      <td>41</td>\n",
       "      <td>...</td>\n",
       "      <td>3939</td>\n",
       "      <td>7218</td>\n",
       "      <td>82</td>\n",
       "      <td>0.004668</td>\n",
       "      <td>0.899481</td>\n",
       "      <td>487386</td>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 130 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                          file_name   edx    esi  es  fs  ds  ss  gs  cs  ah  \\\n",
       "0  000118d12cbf9ad6103e8b914a6e1ac3  2151   2782   2   2   2   2   2   2  37   \n",
       "1  00093d5fa5cb7ce77f6eaf39962daa12  4207  11425   4   2   6   6   2   3  53   \n",
       "2  00099926d51b44c6f8c93a48c2567891   270   1033   0   0   0   0   0   0   0   \n",
       "3  000a2db4762dc06628a086c9e117f884    43   1281   0   0   0   0   0   0   0   \n",
       "4  000ae2c63ba69fc93dfc395b40bfe03a  4334  12557  11   3   8   7   3  11  41   \n",
       "\n",
       "     ...      vertex_count  edge_count  delta_max   density   entropy  \\\n",
       "0    ...              2213        3300        101  0.002277  0.834382   \n",
       "1    ...              4640       12158        204  0.005596  0.803481   \n",
       "2    ...               522         868        284  0.267901  0.997032   \n",
       "3    ...               871         959         70  0.035789  0.535436   \n",
       "4    ...              3939        7218         82  0.004668  0.899481   \n",
       "\n",
       "   file_size  file_id  percentage  trid_id  packer_id  \n",
       "0     201600        1         2.2        1       1101  \n",
       "1     742064        1         8.1       17       1101  \n",
       "2     725288        4         4.8        3          0  \n",
       "3      61551        5         3.5       13        153  \n",
       "4     487386        1         4.0       17          0  \n",
       "\n",
       "[5 rows x 130 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_train_labels = pd.read_csv('data/sorted-pe-coff-train-labels-vs252.csv')\n",
    "combined_features = pd.read_csv('data/combined-pe-features-vs252.csv')\n",
    "combined_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label\n",
       "0      2\n",
       "1     11\n",
       "2     12\n",
       "3     14\n",
       "4      1"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_train_labels.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 VirusShare 263 PE/COFF Model Selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sorted_train_labels = pd.read_csv('data/sorted-pe-coff-train-labels-vs263.csv')\n",
    "combined_features = pd.read_csv('data/combined-pe-features-vs263.csv')\n",
    "combined_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sorted_train_labels.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 VirusShare 264 PE/COFF Model Selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_name</th>\n",
       "      <th>edx</th>\n",
       "      <th>esi</th>\n",
       "      <th>es</th>\n",
       "      <th>fs</th>\n",
       "      <th>ds</th>\n",
       "      <th>ss</th>\n",
       "      <th>gs</th>\n",
       "      <th>cs</th>\n",
       "      <th>ah</th>\n",
       "      <th>...</th>\n",
       "      <th>vertex_count</th>\n",
       "      <th>edge_count</th>\n",
       "      <th>delta_max</th>\n",
       "      <th>density</th>\n",
       "      <th>entropy</th>\n",
       "      <th>file_size</th>\n",
       "      <th>file_id</th>\n",
       "      <th>percentage</th>\n",
       "      <th>trid_id</th>\n",
       "      <th>packer_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0006bac4260d377e3bf2b9cfea530eb9</td>\n",
       "      <td>8714</td>\n",
       "      <td>16348</td>\n",
       "      <td>39</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>12</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>73</td>\n",
       "      <td>...</td>\n",
       "      <td>4689</td>\n",
       "      <td>8093</td>\n",
       "      <td>377</td>\n",
       "      <td>0.005650</td>\n",
       "      <td>0.820715</td>\n",
       "      <td>2015064.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2.6</td>\n",
       "      <td>7</td>\n",
       "      <td>1101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0006e6b536f6622bf308e80f837ceaeb</td>\n",
       "      <td>4374</td>\n",
       "      <td>3295</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>34</td>\n",
       "      <td>...</td>\n",
       "      <td>1593</td>\n",
       "      <td>3325</td>\n",
       "      <td>56</td>\n",
       "      <td>0.006710</td>\n",
       "      <td>0.804215</td>\n",
       "      <td>138620.0</td>\n",
       "      <td>1</td>\n",
       "      <td>5.4</td>\n",
       "      <td>4</td>\n",
       "      <td>317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0007106c237e8689cc68b5111db1a174</td>\n",
       "      <td>3549</td>\n",
       "      <td>13709</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>76</td>\n",
       "      <td>...</td>\n",
       "      <td>5753</td>\n",
       "      <td>11775</td>\n",
       "      <td>168</td>\n",
       "      <td>0.013797</td>\n",
       "      <td>0.826531</td>\n",
       "      <td>524877.0</td>\n",
       "      <td>1</td>\n",
       "      <td>8.1</td>\n",
       "      <td>17</td>\n",
       "      <td>1101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00094c34bd1a0f622b49c1b1b9274ae6</td>\n",
       "      <td>1167</td>\n",
       "      <td>4780</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2737</td>\n",
       "      <td>3884</td>\n",
       "      <td>125</td>\n",
       "      <td>0.011075</td>\n",
       "      <td>0.833980</td>\n",
       "      <td>325328.0</td>\n",
       "      <td>1</td>\n",
       "      <td>7.4</td>\n",
       "      <td>1</td>\n",
       "      <td>1101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>000b01ac8e485e5a143ff577be88b853</td>\n",
       "      <td>1703</td>\n",
       "      <td>1181</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "      <td>...</td>\n",
       "      <td>575</td>\n",
       "      <td>1052</td>\n",
       "      <td>59</td>\n",
       "      <td>0.018804</td>\n",
       "      <td>0.983293</td>\n",
       "      <td>304458.0</td>\n",
       "      <td>1</td>\n",
       "      <td>7.7</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 130 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                          file_name   edx    esi  es  fs  ds  ss  gs  cs  ah  \\\n",
       "0  0006bac4260d377e3bf2b9cfea530eb9  8714  16348  39   8   9  12   8   8  73   \n",
       "1  0006e6b536f6622bf308e80f837ceaeb  4374   3295   1   0   0   0   0   0  34   \n",
       "2  0007106c237e8689cc68b5111db1a174  3549  13709   8   0   0   0   0   0  76   \n",
       "3  00094c34bd1a0f622b49c1b1b9274ae6  1167   4780   0   0   0   0   0   0   0   \n",
       "4  000b01ac8e485e5a143ff577be88b853  1703   1181   0   0   0   0   0   0  32   \n",
       "\n",
       "     ...      vertex_count  edge_count  delta_max   density   entropy  \\\n",
       "0    ...              4689        8093        377  0.005650  0.820715   \n",
       "1    ...              1593        3325         56  0.006710  0.804215   \n",
       "2    ...              5753       11775        168  0.013797  0.826531   \n",
       "3    ...              2737        3884        125  0.011075  0.833980   \n",
       "4    ...               575        1052         59  0.018804  0.983293   \n",
       "\n",
       "   file_size  file_id  percentage  trid_id  packer_id  \n",
       "0  2015064.0        1         2.6        7       1101  \n",
       "1   138620.0        1         5.4        4        317  \n",
       "2   524877.0        1         8.1       17       1101  \n",
       "3   325328.0        1         7.4        1       1101  \n",
       "4   304458.0        1         7.7       12          0  \n",
       "\n",
       "[5 rows x 130 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_train_labels = pd.read_csv('data/sorted-pe-coff-train-labels-vs264.csv')\n",
    "combined_features = pd.read_csv('data/combined-pe-features-vs264.csv')\n",
    "combined_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label\n",
       "0      0\n",
       "1     29\n",
       "2     78\n",
       "3     76\n",
       "4      0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_train_labels.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.series.Series"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = combined_features.iloc[:,1:]\n",
    "y = sorted_train_labels['label']\n",
    "type(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. ExtraTreesClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Memory exhaustion problems, NEED MORE MEMBERBERRIES.\n",
    "clf1 = ExtraTreesClassifier(n_estimators=1000, max_features=None, min_samples_leaf=1, min_samples_split=9, n_jobs=4, criterion='gini')\n",
    "p1, pred1 = run_cv(X,y,clf1)\n",
    "print(\"logloss = {:.3f}\".format(log_loss(y, p1)))\n",
    "print(\"score = {:.3f}\".format(accuracy_score(y, pred1)))\n",
    "cm = confusion_matrix(y, pred1)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clfextra = ExtraTreesClassifier(n_jobs=4)\n",
    "\n",
    "# use a full grid over all parameters, most important parameters are n_estimators (larger is better) and\n",
    "# max_features (for classification best value is square root of the number of features)\n",
    "# Reference: http://scikit-learn.org/stable/modules/ensemble.html\n",
    "param_grid = {\"n_estimators\": [1000, 2000],\n",
    "              \"max_depth\": [3, None],\n",
    "              \"max_features\": [1, 2],\n",
    "              \"min_samples_split\": [1, 3, 10],\n",
    "              \"min_samples_leaf\": [1, 3, 10],\n",
    "              \"bootstrap\": [True, False],\n",
    "              \"criterion\": [\"gini\", \"entropy\"]}\n",
    "\n",
    "# run grid search\n",
    "grid_search = GridSearchCV(clfextra, param_grid=param_grid)\n",
    "start = time()\n",
    "grid_search.fit(X, y)\n",
    "\n",
    "print(\"ExtraTreesClassifier - GridSearchCV:\")\n",
    "print(\" \")\n",
    "print(\"Best parameters set found on training set:\")\n",
    "print(\" \")\n",
    "print(grid_search.best_params_)\n",
    "print(\" \")\n",
    "#print(\"Grid scores on training set:\")\n",
    "#print(\" \")\n",
    "#report(grid_search.grid_scores_)\n",
    "#print(\" \")\n",
    "print(\"Classification report:\")\n",
    "print(\"GridSearchCV took {:.2f} seconds.\".format((time() - start)))\n",
    "print(\" \")\n",
    "y_pred = grid_search.predict(X)\n",
    "print(classification_report(y, y_pred))\n",
    "print(\" \")\n",
    "y_prob = grid_search.predict_proba(X)\n",
    "print(\"logloss = {:.3f}\".format(log_loss(y, y_prob)))\n",
    "print(\"score = {:.3f}\".format(accuracy_score(y, y_pred)))\n",
    "cm = confusion_matrix(y, y_pred)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load up all combined data: vs251"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xgclf = xgb.XGBClassifier(n_estimators=1000, max_depth=10, learning_rate=0.1,objective=\"multi:softprob\", nthread=4)\n",
    "prob1, pred1 = run_cv(X,y,xgclf)\n",
    "print(\"logloss = {:.3f}\".format(log_loss(y, prob1)))\n",
    "print(\"score = {:.3f}\".format(accuracy_score(y, pred1)))\n",
    "cm = confusion_matrix(y, pred1)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xgclf = xgb.XGBClassifier(objective=\"multi:softprob\", nthread=4)\n",
    "\n",
    "params     = {\"n_estimators\": [1000, 2000],\n",
    "              \"max_depth\": [5, 10],\n",
    "              \"learning_rate\": [0.1, 0.09]}\n",
    "\n",
    "# run grid search\n",
    "grid_search = GridSearchCV(xgclf, param_grid=params)\n",
    "start = time()\n",
    "grid_search.fit(X, y)\n",
    "\n",
    "print(\"XGBoost Classifier - GridSearchCV:\")\n",
    "print(\" \")\n",
    "print(\"Best parameters set found on training set:\")\n",
    "print(\" \")\n",
    "print(grid_search.best_params_)\n",
    "print(\" \")\n",
    "#print(\"Grid scores on training set:\")\n",
    "#print(\" \")\n",
    "#report(grid_search.grid_scores_)\n",
    "#print(\" \")\n",
    "print(\"Classification report:\")\n",
    "print(\"GridSearchCV took {:.2f} seconds.\".format((time() - start)))\n",
    "print(\" \")\n",
    "y_pred = grid_search.predict(X)\n",
    "print(classification_report(y, y_pred))\n",
    "print(\" \")\n",
    "y_prob = grid_search.predict_proba(X)\n",
    "print(\"logloss = {:.3f}\".format(log_loss(y, y_prob)))\n",
    "print(\"score = {:.3f}\".format(accuracy_score(y, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## X. Try  Some Less Memory Intensive Models.\n",
    "    - Logistic Regression.\n",
    "    - Support Vector machines.\n",
    "    - Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "#import sklearn as skl\n",
    "import matplotlib.pyplot as plt\n",
    "from time import time\n",
    "from scipy.stats import randint as sp_randint\n",
    "from sklearn.feature_selection import SelectKBest, SelectPercentile\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.metrics import log_loss, confusion_matrix, accuracy_score, classification_report\n",
    "from sklearn.cross_validation import cross_val_score, KFold, train_test_split\n",
    "from sklearn.linear_model import RidgeClassifierCV\n",
    "from sklearn.svm import SVC\n",
    "import seaborn as sns\n",
    "import xgboost as xgb\n",
    "%pylab inline\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## X. Test/Debug Code Only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "help(pd.DataFrame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_name</th>\n",
       "      <th>edx</th>\n",
       "      <th>esi</th>\n",
       "      <th>es</th>\n",
       "      <th>fs</th>\n",
       "      <th>ds</th>\n",
       "      <th>ss</th>\n",
       "      <th>gs</th>\n",
       "      <th>cs</th>\n",
       "      <th>ah</th>\n",
       "      <th>...</th>\n",
       "      <th>sidt</th>\n",
       "      <th>stc</th>\n",
       "      <th>std</th>\n",
       "      <th>sti</th>\n",
       "      <th>stos</th>\n",
       "      <th>sub</th>\n",
       "      <th>test</th>\n",
       "      <th>wait</th>\n",
       "      <th>xchg</th>\n",
       "      <th>xor</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>001dd76872d80801692ff942308c64e6</td>\n",
       "      <td>556</td>\n",
       "      <td>859</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>65</td>\n",
       "      <td>224</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>002325a0a67fded0381b5648d7fe9b8e</td>\n",
       "      <td>556</td>\n",
       "      <td>860</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>65</td>\n",
       "      <td>224</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00dbb9e1c09dbdafb360f3163ba5a3de</td>\n",
       "      <td>983</td>\n",
       "      <td>2246</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>276</td>\n",
       "      <td>262</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0149b7bd7218aab4e257d28469fddb0d</td>\n",
       "      <td>76</td>\n",
       "      <td>376</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>24</td>\n",
       "      <td>65</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>01e0dc079d4e33d8edd050c4900818da</td>\n",
       "      <td>1107</td>\n",
       "      <td>2517</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>339</td>\n",
       "      <td>342</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>297</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 120 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                          file_name   edx   esi  es  fs  ds  ss  gs  cs  ah  \\\n",
       "0  001dd76872d80801692ff942308c64e6   556   859   0   0   0   0   0   0   0   \n",
       "1  002325a0a67fded0381b5648d7fe9b8e   556   860   0   0   0   0   0   0   0   \n",
       "2  00dbb9e1c09dbdafb360f3163ba5a3de   983  2246   0   0   0   0   0   0   1   \n",
       "3  0149b7bd7218aab4e257d28469fddb0d    76   376   0   0   0   0   0   0   0   \n",
       "4  01e0dc079d4e33d8edd050c4900818da  1107  2517   0   0   0   0   0   0   1   \n",
       "\n",
       "  ...   sidt  stc  std  sti  stos  sub  test  wait  xchg  xor  \n",
       "0 ...      0    0    0    0     0   65   224     0     0  268  \n",
       "1 ...      0    0    0    0     0   65   224     0     0  268  \n",
       "2 ...      0    0    0    0     0  276   262     0     0  279  \n",
       "3 ...      0    0    0    0     0   24    65     0     0   49  \n",
       "4 ...      0    0    8    0     0  339   342     0     0  297  \n",
       "\n",
       "[5 rows x 120 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# VirusShare APT Feature sets:\n",
    "# 1. PE ASM features.\n",
    "# 2. Entropy features.\n",
    "# 3. File ID features.\n",
    "# 4. Packer ID features.\n",
    "# 5. PE Call Graph features.\n",
    "# 6. Trid ID features.\n",
    "# 7. PE Header features.\n",
    "# 8. TODO: function count features.\n",
    "# 9. TODO: Binary or ASM images (RNN/CNN models).\n",
    "#\n",
    "# Final Combination: combine all PE/COFF features into feature set ->\n",
    "#     all-combined-pe-features-10perc-vsapt.csv\n",
    "\n",
    "sorted_train_labels = pd.read_csv('data/sorted-train-labels-apt.csv')\n",
    "sorted_asm_features = pd.read_csv('data/sorted-pe-asm-features-apt.csv')\n",
    "sorted_entropy_features = pd.read_csv('data/sorted-entropy-features-apt.csv')\n",
    "sorted_file_id_features = pd.read_csv('data/sorted-file-id-features-apt.csv')\n",
    "sorted_packer_id_features = pd.read_csv('data/sorted-packer-id-features-apt.csv')\n",
    "sorted_call_graph_features = pd.read_csv('data/sorted-pe-call-graph-features-apt.csv')\n",
    "sorted_trid_id_features = pd.read_csv('data/sorted-trid-id-features-apt.csv') # Select only scalar columns.\n",
    "sorted_header_features = pd.read_csv('data/sorted-pe-header-features-apt.csv')\n",
    "\n",
    "# BROKEN: rerun function counts.\n",
    "#sorted_function_count_features = pd.read_csv('data/sorted-pe-function-counts-10percent-apt.csv')\n",
    "\n",
    "sorted_asm_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending 1 -> 001dd76872d80801692ff942308c64e6\n",
      "Appending 101 -> 55886d571c2a57984ea9659b57e1c63a\n",
      "Appending 201 -> b8277cce81e0a372bc35d33a0c9483c2\n",
      "Length of y: 271\n",
      "Shape of X: 271 119\n"
     ]
    }
   ],
   "source": [
    "X,y = get_training_data(sorted_asm_features, sorted_train_labels, 'data/sorted-pe-coff-train-labels-apt.csv')\n",
    "print(\"Length of y: {:d}\".format(len(y)))\n",
    "print(\"Shape of X: {:d} {:d}\".format(X.shape[0], X.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_name</th>\n",
       "      <th>file_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>001dd76872d80801692ff942308c64e6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>002325a0a67fded0381b5648d7fe9b8e</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00dbb9e1c09dbdafb360f3163ba5a3de</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0149b7bd7218aab4e257d28469fddb0d</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>01e0dc079d4e33d8edd050c4900818da</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          file_name  file_id\n",
       "0  001dd76872d80801692ff942308c64e6        1\n",
       "1  002325a0a67fded0381b5648d7fe9b8e        1\n",
       "2  00dbb9e1c09dbdafb360f3163ba5a3de        1\n",
       "3  0149b7bd7218aab4e257d28469fddb0d        1\n",
       "4  01e0dc079d4e33d8edd050c4900818da        1"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fileidfeatures = pd.DataFrame(sorted_file_id_features.iloc[:,[0,2]])\n",
    "fileidfeatures.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_name</th>\n",
       "      <th>packer_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>001dd76872d80801692ff942308c64e6</td>\n",
       "      <td>1111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>002325a0a67fded0381b5648d7fe9b8e</td>\n",
       "      <td>1111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00dbb9e1c09dbdafb360f3163ba5a3de</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0149b7bd7218aab4e257d28469fddb0d</td>\n",
       "      <td>1111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>01e0dc079d4e33d8edd050c4900818da</td>\n",
       "      <td>1111</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          file_name  packer_id\n",
       "0  001dd76872d80801692ff942308c64e6       1111\n",
       "1  002325a0a67fded0381b5648d7fe9b8e       1111\n",
       "2  00dbb9e1c09dbdafb360f3163ba5a3de          0\n",
       "3  0149b7bd7218aab4e257d28469fddb0d       1111\n",
       "4  01e0dc079d4e33d8edd050c4900818da       1111"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "packeridfeatures = pd.DataFrame(sorted_packer_id_features.iloc[:,[0,2]])\n",
    "packeridfeatures.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_name</th>\n",
       "      <th>percentage</th>\n",
       "      <th>trid_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>001dd76872d80801692ff942308c64e6</td>\n",
       "      <td>4.6</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>002325a0a67fded0381b5648d7fe9b8e</td>\n",
       "      <td>4.6</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00dbb9e1c09dbdafb360f3163ba5a3de</td>\n",
       "      <td>6.4</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0149b7bd7218aab4e257d28469fddb0d</td>\n",
       "      <td>4.6</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>01e0dc079d4e33d8edd050c4900818da</td>\n",
       "      <td>7.4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          file_name  percentage  trid_id\n",
       "0  001dd76872d80801692ff942308c64e6         4.6       21\n",
       "1  002325a0a67fded0381b5648d7fe9b8e         4.6       21\n",
       "2  00dbb9e1c09dbdafb360f3163ba5a3de         6.4       24\n",
       "3  0149b7bd7218aab4e257d28469fddb0d         4.6       21\n",
       "4  01e0dc079d4e33d8edd050c4900818da         7.4        1"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trididfeatures = pd.DataFrame(sorted_trid_id_features.iloc[:,[0,2,3]])\n",
    "trididfeatures.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_name</th>\n",
       "      <th>edx</th>\n",
       "      <th>esi</th>\n",
       "      <th>es</th>\n",
       "      <th>fs</th>\n",
       "      <th>ds</th>\n",
       "      <th>ss</th>\n",
       "      <th>gs</th>\n",
       "      <th>cs</th>\n",
       "      <th>ah</th>\n",
       "      <th>...</th>\n",
       "      <th>vertex_count</th>\n",
       "      <th>edge_count</th>\n",
       "      <th>delta_max</th>\n",
       "      <th>density</th>\n",
       "      <th>entropy</th>\n",
       "      <th>file_size</th>\n",
       "      <th>file_id</th>\n",
       "      <th>percentage</th>\n",
       "      <th>trid_id</th>\n",
       "      <th>packer_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>001dd76872d80801692ff942308c64e6</td>\n",
       "      <td>556</td>\n",
       "      <td>859</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>354</td>\n",
       "      <td>455</td>\n",
       "      <td>34</td>\n",
       "      <td>0.232975</td>\n",
       "      <td>0.804603</td>\n",
       "      <td>32768</td>\n",
       "      <td>1</td>\n",
       "      <td>4.6</td>\n",
       "      <td>21</td>\n",
       "      <td>1111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>002325a0a67fded0381b5648d7fe9b8e</td>\n",
       "      <td>556</td>\n",
       "      <td>860</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>356</td>\n",
       "      <td>457</td>\n",
       "      <td>34</td>\n",
       "      <td>0.233999</td>\n",
       "      <td>0.798349</td>\n",
       "      <td>33792</td>\n",
       "      <td>1</td>\n",
       "      <td>4.6</td>\n",
       "      <td>21</td>\n",
       "      <td>1111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00dbb9e1c09dbdafb360f3163ba5a3de</td>\n",
       "      <td>983</td>\n",
       "      <td>2246</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>376</td>\n",
       "      <td>1506</td>\n",
       "      <td>121</td>\n",
       "      <td>0.144253</td>\n",
       "      <td>0.789731</td>\n",
       "      <td>65024</td>\n",
       "      <td>1</td>\n",
       "      <td>6.4</td>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0149b7bd7218aab4e257d28469fddb0d</td>\n",
       "      <td>76</td>\n",
       "      <td>376</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>344</td>\n",
       "      <td>381</td>\n",
       "      <td>44</td>\n",
       "      <td>0.721591</td>\n",
       "      <td>0.725844</td>\n",
       "      <td>17408</td>\n",
       "      <td>1</td>\n",
       "      <td>4.6</td>\n",
       "      <td>21</td>\n",
       "      <td>1111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>01e0dc079d4e33d8edd050c4900818da</td>\n",
       "      <td>1107</td>\n",
       "      <td>2517</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1164</td>\n",
       "      <td>1589</td>\n",
       "      <td>121</td>\n",
       "      <td>0.019519</td>\n",
       "      <td>0.735541</td>\n",
       "      <td>131072</td>\n",
       "      <td>1</td>\n",
       "      <td>7.4</td>\n",
       "      <td>1</td>\n",
       "      <td>1111</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1266 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                          file_name   edx   esi  es  fs  ds  ss  gs  cs  ah  \\\n",
       "0  001dd76872d80801692ff942308c64e6   556   859   0   0   0   0   0   0   0   \n",
       "1  002325a0a67fded0381b5648d7fe9b8e   556   860   0   0   0   0   0   0   0   \n",
       "2  00dbb9e1c09dbdafb360f3163ba5a3de   983  2246   0   0   0   0   0   0   1   \n",
       "3  0149b7bd7218aab4e257d28469fddb0d    76   376   0   0   0   0   0   0   0   \n",
       "4  01e0dc079d4e33d8edd050c4900818da  1107  2517   0   0   0   0   0   0   1   \n",
       "\n",
       "     ...      vertex_count  edge_count  delta_max   density   entropy  \\\n",
       "0    ...               354         455         34  0.232975  0.804603   \n",
       "1    ...               356         457         34  0.233999  0.798349   \n",
       "2    ...               376        1506        121  0.144253  0.789731   \n",
       "3    ...               344         381         44  0.721591  0.725844   \n",
       "4    ...              1164        1589        121  0.019519  0.735541   \n",
       "\n",
       "   file_size  file_id  percentage  trid_id  packer_id  \n",
       "0      32768        1         4.6       21       1111  \n",
       "1      33792        1         4.6       21       1111  \n",
       "2      65024        1         6.4       24          0  \n",
       "3      17408        1         4.6       21       1111  \n",
       "4     131072        1         7.4        1       1111  \n",
       "\n",
       "[5 rows x 1266 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_train_features = sorted_asm_features.merge(sorted_header_features, on='file_name', how='inner', suffixes=('_asm','_hd'))\n",
    "combined_train_features = combined_train_features.merge(sorted_call_graph_features, on='file_name', how='inner', suffixes=('_asm','_cg'))\n",
    "combined_train_features = combined_train_features.merge(sorted_entropy_features, on='file_name', how='inner', suffixes=('_asm','_ent'))\n",
    "combined_train_features = combined_train_features.merge(fileidfeatures, on='file_name', how='inner', suffixes=('_asm','_fid'))\n",
    "combined_train_features = combined_train_features.merge(trididfeatures, on='file_name', how='inner', suffixes=('_asm','_tid'))\n",
    "combined_train_features = combined_train_features.merge(packeridfeatures, on='file_name', how='inner', suffixes=('_asm','_pid'))\n",
    "\n",
    "combined_train_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "combined_train_features.to_csv('data/combined-pe-features-apt.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_name</th>\n",
       "      <th>edx</th>\n",
       "      <th>esi</th>\n",
       "      <th>es</th>\n",
       "      <th>fs</th>\n",
       "      <th>ds</th>\n",
       "      <th>ss</th>\n",
       "      <th>gs</th>\n",
       "      <th>cs</th>\n",
       "      <th>ah</th>\n",
       "      <th>...</th>\n",
       "      <th>vertex_count</th>\n",
       "      <th>edge_count</th>\n",
       "      <th>delta_max</th>\n",
       "      <th>density</th>\n",
       "      <th>entropy</th>\n",
       "      <th>file_size</th>\n",
       "      <th>file_id</th>\n",
       "      <th>percentage</th>\n",
       "      <th>trid_id</th>\n",
       "      <th>packer_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>001dd76872d80801692ff942308c64e6</td>\n",
       "      <td>556</td>\n",
       "      <td>859</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>354</td>\n",
       "      <td>455</td>\n",
       "      <td>34</td>\n",
       "      <td>0.232975</td>\n",
       "      <td>0.804603</td>\n",
       "      <td>32768</td>\n",
       "      <td>1</td>\n",
       "      <td>4.6</td>\n",
       "      <td>21</td>\n",
       "      <td>1111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>002325a0a67fded0381b5648d7fe9b8e</td>\n",
       "      <td>556</td>\n",
       "      <td>860</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>356</td>\n",
       "      <td>457</td>\n",
       "      <td>34</td>\n",
       "      <td>0.233999</td>\n",
       "      <td>0.798349</td>\n",
       "      <td>33792</td>\n",
       "      <td>1</td>\n",
       "      <td>4.6</td>\n",
       "      <td>21</td>\n",
       "      <td>1111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00dbb9e1c09dbdafb360f3163ba5a3de</td>\n",
       "      <td>983</td>\n",
       "      <td>2246</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>376</td>\n",
       "      <td>1506</td>\n",
       "      <td>121</td>\n",
       "      <td>0.144253</td>\n",
       "      <td>0.789731</td>\n",
       "      <td>65024</td>\n",
       "      <td>1</td>\n",
       "      <td>6.4</td>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0149b7bd7218aab4e257d28469fddb0d</td>\n",
       "      <td>76</td>\n",
       "      <td>376</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>344</td>\n",
       "      <td>381</td>\n",
       "      <td>44</td>\n",
       "      <td>0.721591</td>\n",
       "      <td>0.725844</td>\n",
       "      <td>17408</td>\n",
       "      <td>1</td>\n",
       "      <td>4.6</td>\n",
       "      <td>21</td>\n",
       "      <td>1111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>01e0dc079d4e33d8edd050c4900818da</td>\n",
       "      <td>1107</td>\n",
       "      <td>2517</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1164</td>\n",
       "      <td>1589</td>\n",
       "      <td>121</td>\n",
       "      <td>0.019519</td>\n",
       "      <td>0.735541</td>\n",
       "      <td>131072</td>\n",
       "      <td>1</td>\n",
       "      <td>7.4</td>\n",
       "      <td>1</td>\n",
       "      <td>1111</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1266 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                          file_name   edx   esi  es  fs  ds  ss  gs  cs  ah  \\\n",
       "0  001dd76872d80801692ff942308c64e6   556   859   0   0   0   0   0   0   0   \n",
       "1  002325a0a67fded0381b5648d7fe9b8e   556   860   0   0   0   0   0   0   0   \n",
       "2  00dbb9e1c09dbdafb360f3163ba5a3de   983  2246   0   0   0   0   0   0   1   \n",
       "3  0149b7bd7218aab4e257d28469fddb0d    76   376   0   0   0   0   0   0   0   \n",
       "4  01e0dc079d4e33d8edd050c4900818da  1107  2517   0   0   0   0   0   0   1   \n",
       "\n",
       "     ...      vertex_count  edge_count  delta_max   density   entropy  \\\n",
       "0    ...               354         455         34  0.232975  0.804603   \n",
       "1    ...               356         457         34  0.233999  0.798349   \n",
       "2    ...               376        1506        121  0.144253  0.789731   \n",
       "3    ...               344         381         44  0.721591  0.725844   \n",
       "4    ...              1164        1589        121  0.019519  0.735541   \n",
       "\n",
       "   file_size  file_id  percentage  trid_id  packer_id  \n",
       "0      32768        1         4.6       21       1111  \n",
       "1      33792        1         4.6       21       1111  \n",
       "2      65024        1         6.4       24          0  \n",
       "3      17408        1         4.6       21       1111  \n",
       "4     131072        1         7.4        1       1111  \n",
       "\n",
       "[5 rows x 1266 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now reduce the feature set to 10% best features.\n",
    "sorted_train_labels = pd.read_csv('data/sorted-pe-coff-train-labels-apt.csv')\n",
    "combined_features = pd.read_csv('data/combined-pe-features-apt.csv')\n",
    "combined_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2064</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label\n",
       "0   2057\n",
       "1   2057\n",
       "2   2064\n",
       "3   2057\n",
       "4   2064"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_train_labels.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.series.Series"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = combined_features.iloc[:,1:]\n",
    "y = sorted_train_labels['label']\n",
    "type(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def run_prediction_cv(X, y, clf, train_folds):\n",
    "\n",
    "    # Construct a kfolds object\n",
    "    len_y = len(y)\n",
    "    len_l = y.nunique()\n",
    "    counter = 0\n",
    "    \n",
    "    kf = KFold(len_y, n_folds=train_folds, shuffle=True)\n",
    "    y_prob = np.zeros((len_y, len_l)) # 2D array (sample size x number of labels)\n",
    "    y_pred = np.zeros(len_y)\n",
    "    \n",
    "    print(\"y_prob -> {:d} x {:d}\".format(len_y, len_l))\n",
    "    \n",
    "    # Iterate through folds\n",
    "    for train_index, test_index in kf:\n",
    "        #print(test_index, train_index)\n",
    "        print(\"Fold -> {:d}\".format(counter))\n",
    "        X_train = X.loc[train_index,:]\n",
    "        X_test = X.loc[test_index,:]\n",
    "        y_train = y[train_index]\n",
    "\n",
    "        clf.fit(X_train, y_train) # use flatten to get rid of data conversion warnings\n",
    "        \n",
    "        #y_prob[test_index] = clf.predict_proba(X_test)\n",
    "        y_pred[test_index] = clf.predict(X_test)\n",
    "        #print(clf.get_params())\n",
    "        counter += 1\n",
    "    \n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def reduce_feature_set(feature_set_df, X, y, out_filename):\n",
    "    # Find the top 10 percent variance features.\n",
    "    # print(\"Sorted feature subset: {:d}\".format(idx))\n",
    "    print(\"Subset shape: {:d} {:d}\".format(X.shape[0], X.shape[1]))\n",
    "    print(\"Length of y: {:d}\".format(len(y)))\n",
    "    #sorted_feature_subset.head()\n",
    "\n",
    "    # Now select the 10% best features for this feature subset.\n",
    "    fsp = SelectPercentile(chi2, 10)\n",
    "    X_new_10 = fsp.fit_transform(X,y)\n",
    "    selected_names = fsp.get_support(indices=True)\n",
    "    selected_names = selected_names + 1 # the column name indices start at 0 so add 1 to all.\n",
    "\n",
    "    data_trimmed = feature_set_df.iloc[:,selected_names]\n",
    "    data_fnames = pd.DataFrame(feature_set_df['file_name'])\n",
    "    data_reduced = data_fnames.join(data_trimmed)\n",
    "\n",
    "    # Write to file as we do not have enough memory.\n",
    "    # filename = \"data/\" + out_filename\n",
    "    data_reduced.to_csv(out_filename, index=False)\n",
    "    #sorted_feature_subset['file_name'].to_csv(filename, index=False)\n",
    "    print(\"Writing file: {:s}\".format(out_filename))\n",
    "                                      \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subset shape: 271 1265\n",
      "Length of y: 271\n",
      "Writing file: data/combined-pe-features-apt-reduced.csv\n"
     ]
    }
   ],
   "source": [
    "reduce_feature_set(combined_features, X, y, 'data/combined-pe-features-apt-reduced.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## X.X START HERE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_name</th>\n",
       "      <th>edx</th>\n",
       "      <th>esi</th>\n",
       "      <th>es</th>\n",
       "      <th>fs</th>\n",
       "      <th>ds</th>\n",
       "      <th>ss</th>\n",
       "      <th>gs</th>\n",
       "      <th>cs</th>\n",
       "      <th>ah</th>\n",
       "      <th>...</th>\n",
       "      <th>IsWindow</th>\n",
       "      <th>WinHttpOpen</th>\n",
       "      <th>vertex_count</th>\n",
       "      <th>edge_count</th>\n",
       "      <th>delta_max</th>\n",
       "      <th>density</th>\n",
       "      <th>file_size</th>\n",
       "      <th>file_id</th>\n",
       "      <th>trid_id</th>\n",
       "      <th>packer_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>001dd76872d80801692ff942308c64e6</td>\n",
       "      <td>556</td>\n",
       "      <td>859</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>354</td>\n",
       "      <td>455</td>\n",
       "      <td>34</td>\n",
       "      <td>0.232975</td>\n",
       "      <td>32768</td>\n",
       "      <td>1</td>\n",
       "      <td>21</td>\n",
       "      <td>1111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>002325a0a67fded0381b5648d7fe9b8e</td>\n",
       "      <td>556</td>\n",
       "      <td>860</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>356</td>\n",
       "      <td>457</td>\n",
       "      <td>34</td>\n",
       "      <td>0.233999</td>\n",
       "      <td>33792</td>\n",
       "      <td>1</td>\n",
       "      <td>21</td>\n",
       "      <td>1111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00dbb9e1c09dbdafb360f3163ba5a3de</td>\n",
       "      <td>983</td>\n",
       "      <td>2246</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>376</td>\n",
       "      <td>1506</td>\n",
       "      <td>121</td>\n",
       "      <td>0.144253</td>\n",
       "      <td>65024</td>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0149b7bd7218aab4e257d28469fddb0d</td>\n",
       "      <td>76</td>\n",
       "      <td>376</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>344</td>\n",
       "      <td>381</td>\n",
       "      <td>44</td>\n",
       "      <td>0.721591</td>\n",
       "      <td>17408</td>\n",
       "      <td>1</td>\n",
       "      <td>21</td>\n",
       "      <td>1111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>01e0dc079d4e33d8edd050c4900818da</td>\n",
       "      <td>1107</td>\n",
       "      <td>2517</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1164</td>\n",
       "      <td>1589</td>\n",
       "      <td>121</td>\n",
       "      <td>0.019519</td>\n",
       "      <td>131072</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1111</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 128 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                          file_name   edx   esi  es  fs  ds  ss  gs  cs  ah  \\\n",
       "0  001dd76872d80801692ff942308c64e6   556   859   0   0   0   0   0   0   0   \n",
       "1  002325a0a67fded0381b5648d7fe9b8e   556   860   0   0   0   0   0   0   0   \n",
       "2  00dbb9e1c09dbdafb360f3163ba5a3de   983  2246   0   0   0   0   0   0   1   \n",
       "3  0149b7bd7218aab4e257d28469fddb0d    76   376   0   0   0   0   0   0   0   \n",
       "4  01e0dc079d4e33d8edd050c4900818da  1107  2517   0   0   0   0   0   0   1   \n",
       "\n",
       "     ...      IsWindow  WinHttpOpen  vertex_count  edge_count  delta_max  \\\n",
       "0    ...             0            0           354         455         34   \n",
       "1    ...             0            0           356         457         34   \n",
       "2    ...             0            0           376        1506        121   \n",
       "3    ...             0            0           344         381         44   \n",
       "4    ...             0            0          1164        1589        121   \n",
       "\n",
       "    density  file_size  file_id  trid_id  packer_id  \n",
       "0  0.232975      32768        1       21       1111  \n",
       "1  0.233999      33792        1       21       1111  \n",
       "2  0.144253      65024        1       24          0  \n",
       "3  0.721591      17408        1       21       1111  \n",
       "4  0.019519     131072        1        1       1111  \n",
       "\n",
       "[5 rows x 128 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_train_features = pd.read_csv('data/combined-pe-features-apt-reduced.csv')\n",
    "train_labels = pd.read_csv('data/sorted-pe-coff-train-labels-apt.csv')\n",
    "combined_train_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>edx</th>\n",
       "      <th>esi</th>\n",
       "      <th>es</th>\n",
       "      <th>fs</th>\n",
       "      <th>ds</th>\n",
       "      <th>ss</th>\n",
       "      <th>gs</th>\n",
       "      <th>cs</th>\n",
       "      <th>ah</th>\n",
       "      <th>al</th>\n",
       "      <th>...</th>\n",
       "      <th>IsWindow</th>\n",
       "      <th>WinHttpOpen</th>\n",
       "      <th>vertex_count</th>\n",
       "      <th>edge_count</th>\n",
       "      <th>delta_max</th>\n",
       "      <th>density</th>\n",
       "      <th>file_size</th>\n",
       "      <th>file_id</th>\n",
       "      <th>trid_id</th>\n",
       "      <th>packer_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>556</td>\n",
       "      <td>859</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>123</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>354</td>\n",
       "      <td>455</td>\n",
       "      <td>34</td>\n",
       "      <td>0.232975</td>\n",
       "      <td>32768</td>\n",
       "      <td>1</td>\n",
       "      <td>21</td>\n",
       "      <td>1111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>556</td>\n",
       "      <td>860</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>123</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>356</td>\n",
       "      <td>457</td>\n",
       "      <td>34</td>\n",
       "      <td>0.233999</td>\n",
       "      <td>33792</td>\n",
       "      <td>1</td>\n",
       "      <td>21</td>\n",
       "      <td>1111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>983</td>\n",
       "      <td>2246</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1289</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>376</td>\n",
       "      <td>1506</td>\n",
       "      <td>121</td>\n",
       "      <td>0.144253</td>\n",
       "      <td>65024</td>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>76</td>\n",
       "      <td>376</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>344</td>\n",
       "      <td>381</td>\n",
       "      <td>44</td>\n",
       "      <td>0.721591</td>\n",
       "      <td>17408</td>\n",
       "      <td>1</td>\n",
       "      <td>21</td>\n",
       "      <td>1111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1107</td>\n",
       "      <td>2517</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1244</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1164</td>\n",
       "      <td>1589</td>\n",
       "      <td>121</td>\n",
       "      <td>0.019519</td>\n",
       "      <td>131072</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1111</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 127 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    edx   esi  es  fs  ds  ss  gs  cs  ah    al    ...      IsWindow  \\\n",
       "0   556   859   0   0   0   0   0   0   0   123    ...             0   \n",
       "1   556   860   0   0   0   0   0   0   0   123    ...             0   \n",
       "2   983  2246   0   0   0   0   0   0   1  1289    ...             0   \n",
       "3    76   376   0   0   0   0   0   0   0    62    ...             0   \n",
       "4  1107  2517   0   0   0   0   0   0   1  1244    ...             0   \n",
       "\n",
       "   WinHttpOpen  vertex_count  edge_count  delta_max   density  file_size  \\\n",
       "0            0           354         455         34  0.232975      32768   \n",
       "1            0           356         457         34  0.233999      33792   \n",
       "2            0           376        1506        121  0.144253      65024   \n",
       "3            0           344         381         44  0.721591      17408   \n",
       "4            0          1164        1589        121  0.019519     131072   \n",
       "\n",
       "   file_id  trid_id  packer_id  \n",
       "0        1       21       1111  \n",
       "1        1       21       1111  \n",
       "2        1       24          0  \n",
       "3        1       21       1111  \n",
       "4        1        1       1111  \n",
       "\n",
       "[5 rows x 127 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = combined_train_features.iloc[:,1:]\n",
    "y = train_labels['label']\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_prob -> 271 x 58\n",
      "Run -> 0\n",
      "Run -> 1\n",
      "Run -> 2\n",
      "Run -> 3\n",
      "Run -> 4\n",
      "Run -> 5\n",
      "Run -> 6\n",
      "Run -> 7\n",
      "Run -> 8\n",
      "Run -> 9\n",
      "score = 0.738\n",
      "[[0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " ..., \n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "# Now get the reduced combined feature set and try ExtraTreesClassifier and XGBoost.\n",
    "clf1 = ExtraTreesClassifier(n_estimators=1000, max_features=None, min_samples_leaf=1, min_samples_split=9, n_jobs=4, criterion='gini')\n",
    "pred1 = run_prediction_cv(X,y,clf1,10)\n",
    "#print(\"logloss = {:.3f}\".format(log_loss(y, prob1)))\n",
    "print(\"score = {:.3f}\".format(accuracy_score(y, pred1)))\n",
    "cm = confusion_matrix(y, pred1)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(58, 58)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def write_confusion_matrix(cm, out_file_name):\n",
    "    #fop = open('data/confusion-matrix-apt.txt','w')\n",
    "    fop = open('data/' + out_file_name, 'w')\n",
    "    # this is rubbish ->  cm.tofile(fop, \",\")\n",
    "    for a_idx in range(0,cm.shape[0]):\n",
    "        #line = \",\".join(cm[a_idx])\n",
    "        line = ','.join(str(x) for x in cm[a_idx])\n",
    "        print(\"{:d} -> {:s}\".format(a_idx, line))\n",
    "        fop.write(line + \"\\n\")\n",
    "        #for b_idx in cm.shape[1]:\n",
    "        #    line = line + \",\".acm[a_idx,b_idx]\n",
    "\n",
    "    fop.close()\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_name</th>\n",
       "      <th>edx</th>\n",
       "      <th>esi</th>\n",
       "      <th>es</th>\n",
       "      <th>fs</th>\n",
       "      <th>ds</th>\n",
       "      <th>ss</th>\n",
       "      <th>gs</th>\n",
       "      <th>cs</th>\n",
       "      <th>ah</th>\n",
       "      <th>...</th>\n",
       "      <th>IsWindow</th>\n",
       "      <th>WinHttpOpen</th>\n",
       "      <th>vertex_count</th>\n",
       "      <th>edge_count</th>\n",
       "      <th>delta_max</th>\n",
       "      <th>density</th>\n",
       "      <th>file_size</th>\n",
       "      <th>file_id</th>\n",
       "      <th>trid_id</th>\n",
       "      <th>packer_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>001dd76872d80801692ff942308c64e6</td>\n",
       "      <td>556</td>\n",
       "      <td>859</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>354</td>\n",
       "      <td>455</td>\n",
       "      <td>34</td>\n",
       "      <td>0.232975</td>\n",
       "      <td>32768</td>\n",
       "      <td>1</td>\n",
       "      <td>21</td>\n",
       "      <td>1111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>002325a0a67fded0381b5648d7fe9b8e</td>\n",
       "      <td>556</td>\n",
       "      <td>860</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>356</td>\n",
       "      <td>457</td>\n",
       "      <td>34</td>\n",
       "      <td>0.233999</td>\n",
       "      <td>33792</td>\n",
       "      <td>1</td>\n",
       "      <td>21</td>\n",
       "      <td>1111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00dbb9e1c09dbdafb360f3163ba5a3de</td>\n",
       "      <td>983</td>\n",
       "      <td>2246</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>376</td>\n",
       "      <td>1506</td>\n",
       "      <td>121</td>\n",
       "      <td>0.144253</td>\n",
       "      <td>65024</td>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0149b7bd7218aab4e257d28469fddb0d</td>\n",
       "      <td>76</td>\n",
       "      <td>376</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>344</td>\n",
       "      <td>381</td>\n",
       "      <td>44</td>\n",
       "      <td>0.721591</td>\n",
       "      <td>17408</td>\n",
       "      <td>1</td>\n",
       "      <td>21</td>\n",
       "      <td>1111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>01e0dc079d4e33d8edd050c4900818da</td>\n",
       "      <td>1107</td>\n",
       "      <td>2517</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1164</td>\n",
       "      <td>1589</td>\n",
       "      <td>121</td>\n",
       "      <td>0.019519</td>\n",
       "      <td>131072</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1111</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 128 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                          file_name   edx   esi  es  fs  ds  ss  gs  cs  ah  \\\n",
       "0  001dd76872d80801692ff942308c64e6   556   859   0   0   0   0   0   0   0   \n",
       "1  002325a0a67fded0381b5648d7fe9b8e   556   860   0   0   0   0   0   0   0   \n",
       "2  00dbb9e1c09dbdafb360f3163ba5a3de   983  2246   0   0   0   0   0   0   1   \n",
       "3  0149b7bd7218aab4e257d28469fddb0d    76   376   0   0   0   0   0   0   0   \n",
       "4  01e0dc079d4e33d8edd050c4900818da  1107  2517   0   0   0   0   0   0   1   \n",
       "\n",
       "     ...      IsWindow  WinHttpOpen  vertex_count  edge_count  delta_max  \\\n",
       "0    ...             0            0           354         455         34   \n",
       "1    ...             0            0           356         457         34   \n",
       "2    ...             0            0           376        1506        121   \n",
       "3    ...             0            0           344         381         44   \n",
       "4    ...             0            0          1164        1589        121   \n",
       "\n",
       "    density  file_size  file_id  trid_id  packer_id  \n",
       "0  0.232975      32768        1       21       1111  \n",
       "1  0.233999      33792        1       21       1111  \n",
       "2  0.144253      65024        1       24          0  \n",
       "3  0.721591      17408        1       21       1111  \n",
       "4  0.019519     131072        1        1       1111  \n",
       "\n",
       "[5 rows x 128 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# XGBoost does not like feature names containing [,<] characters!!!!\n",
    "combined_train_features = pd.read_csv('data/combined-pe-features-apt-reduced-xgboost.csv')\n",
    "train_labels = pd.read_csv('data/sorted-pe-coff-train-labels-apt.csv')\n",
    "combined_train_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>edx</th>\n",
       "      <th>esi</th>\n",
       "      <th>es</th>\n",
       "      <th>fs</th>\n",
       "      <th>ds</th>\n",
       "      <th>ss</th>\n",
       "      <th>gs</th>\n",
       "      <th>cs</th>\n",
       "      <th>ah</th>\n",
       "      <th>al</th>\n",
       "      <th>...</th>\n",
       "      <th>IsWindow</th>\n",
       "      <th>WinHttpOpen</th>\n",
       "      <th>vertex_count</th>\n",
       "      <th>edge_count</th>\n",
       "      <th>delta_max</th>\n",
       "      <th>density</th>\n",
       "      <th>file_size</th>\n",
       "      <th>file_id</th>\n",
       "      <th>trid_id</th>\n",
       "      <th>packer_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>556</td>\n",
       "      <td>859</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>123</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>354</td>\n",
       "      <td>455</td>\n",
       "      <td>34</td>\n",
       "      <td>0.232975</td>\n",
       "      <td>32768</td>\n",
       "      <td>1</td>\n",
       "      <td>21</td>\n",
       "      <td>1111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>556</td>\n",
       "      <td>860</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>123</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>356</td>\n",
       "      <td>457</td>\n",
       "      <td>34</td>\n",
       "      <td>0.233999</td>\n",
       "      <td>33792</td>\n",
       "      <td>1</td>\n",
       "      <td>21</td>\n",
       "      <td>1111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>983</td>\n",
       "      <td>2246</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1289</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>376</td>\n",
       "      <td>1506</td>\n",
       "      <td>121</td>\n",
       "      <td>0.144253</td>\n",
       "      <td>65024</td>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>76</td>\n",
       "      <td>376</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>344</td>\n",
       "      <td>381</td>\n",
       "      <td>44</td>\n",
       "      <td>0.721591</td>\n",
       "      <td>17408</td>\n",
       "      <td>1</td>\n",
       "      <td>21</td>\n",
       "      <td>1111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1107</td>\n",
       "      <td>2517</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1244</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1164</td>\n",
       "      <td>1589</td>\n",
       "      <td>121</td>\n",
       "      <td>0.019519</td>\n",
       "      <td>131072</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1111</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 127 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    edx   esi  es  fs  ds  ss  gs  cs  ah    al    ...      IsWindow  \\\n",
       "0   556   859   0   0   0   0   0   0   0   123    ...             0   \n",
       "1   556   860   0   0   0   0   0   0   0   123    ...             0   \n",
       "2   983  2246   0   0   0   0   0   0   1  1289    ...             0   \n",
       "3    76   376   0   0   0   0   0   0   0    62    ...             0   \n",
       "4  1107  2517   0   0   0   0   0   0   1  1244    ...             0   \n",
       "\n",
       "   WinHttpOpen  vertex_count  edge_count  delta_max   density  file_size  \\\n",
       "0            0           354         455         34  0.232975      32768   \n",
       "1            0           356         457         34  0.233999      33792   \n",
       "2            0           376        1506        121  0.144253      65024   \n",
       "3            0           344         381         44  0.721591      17408   \n",
       "4            0          1164        1589        121  0.019519     131072   \n",
       "\n",
       "   file_id  trid_id  packer_id  \n",
       "0        1       21       1111  \n",
       "1        1       21       1111  \n",
       "2        1       24          0  \n",
       "3        1       21       1111  \n",
       "4        1        1       1111  \n",
       "\n",
       "[5 rows x 127 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = combined_train_features.iloc[:,1:]\n",
    "y = train_labels['label']\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_prob -> 271 x 58\n",
      "Run -> 0\n",
      "Run -> 1\n",
      "Run -> 2\n",
      "Run -> 3\n",
      "Run -> 4\n",
      "Run -> 5\n",
      "Run -> 6\n",
      "Run -> 7\n",
      "Run -> 8\n",
      "Run -> 9\n",
      "score = 0.697\n",
      "0 -> 0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0\n",
      "1 -> 0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,2,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,2,0,0,0,0,0,0,0\n",
      "2 -> 0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0\n",
      "3 -> 0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0\n",
      "4 -> 0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0\n",
      "5 -> 0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0\n",
      "6 -> 0,0,0,0,0,0,2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0\n",
      "7 -> 0,0,0,0,0,0,0,2,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0\n",
      "8 -> 0,0,0,0,0,0,0,0,2,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0\n",
      "9 -> 0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0\n",
      "10 -> 0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0\n",
      "11 -> 0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0\n",
      "12 -> 0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0\n",
      "13 -> 0,0,0,2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0\n",
      "14 -> 0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0\n",
      "15 -> 0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,19,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0\n",
      "16 -> 0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,10,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0\n",
      "17 -> 0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0\n",
      "18 -> 0,3,0,0,0,0,1,0,2,1,0,0,0,0,0,1,0,0,24,0,1,0,0,0,0,0,1,0,0,1,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,1,0,0\n",
      "19 -> 0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,7,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0\n",
      "20 -> 0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,6,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0\n",
      "21 -> 0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0\n",
      "22 -> 0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0\n",
      "23 -> 0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,7,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0\n",
      "24 -> 0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,5,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0\n",
      "25 -> 0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0\n",
      "26 -> 0,0,0,0,0,0,2,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0\n",
      "27 -> 0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,4,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0\n",
      "28 -> 0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0\n",
      "29 -> 0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0\n",
      "30 -> 0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0\n",
      "31 -> 0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0\n",
      "32 -> 0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,32,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0\n",
      "33 -> 0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,4,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0\n",
      "34 -> 0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,4,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0\n",
      "35 -> 0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,21,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0\n",
      "36 -> 0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,3,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0\n",
      "37 -> 0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0\n",
      "38 -> 0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,7,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0\n",
      "39 -> 0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0\n",
      "40 -> 0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,5,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0\n",
      "41 -> 0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,3,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0\n",
      "42 -> 0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0\n",
      "43 -> 0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0\n",
      "44 -> 0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0\n",
      "45 -> 0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,3,0,0,0,0,0,0,0,0,0,0,0,0\n",
      "46 -> 0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0\n",
      "47 -> 0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0\n",
      "48 -> 0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0\n",
      "49 -> 0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,0,0,0,0,0,0,0,0\n",
      "50 -> 0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,8,0,0,0,0,0,0,0\n",
      "51 -> 0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0\n",
      "52 -> 0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0\n",
      "53 -> 0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0\n",
      "54 -> 0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0\n",
      "55 -> 0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0\n",
      "56 -> 0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0\n",
      "57 -> 0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0\n"
     ]
    }
   ],
   "source": [
    "#xgclf = xgb.XGBClassifier(n_estimators=1000, max_depth=10, learning_rate=0.1,objective=\"multi:softprob\")\n",
    "xgclf = xgb.XGBClassifier(n_estimators=1000, max_depth=10, learning_rate=0.1, objective=\"multi:softmax\")\n",
    "pred1 = run_prediction_cv(X, y, xgclf, 10)\n",
    "#print(\"logloss = {:.3f}\".format(log_loss(y, prob1)))\n",
    "print(\"score = {:.3f}\".format(accuracy_score(y, pred1)))\n",
    "cm = confusion_matrix(y, pred1)\n",
    "write_confusion_matrix(cm, 'confusion-matrix-xgboost-apt.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### X.X Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RBF SVM:\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWwAAAELCAYAAAD9brxbAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nNGcAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fc67036c050>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import svm\n",
    "\n",
    "combined_train_features = pd.read_csv('data/combined-pe-features-apt-reduced.csv')\n",
    "train_labels = pd.read_csv('data/sorted-pe-coff-train-labels-apt.csv')\n",
    "\n",
    "#X = combined_train_features.iloc[:,1:]\n",
    "y = train_labels['label']\n",
    "h = 10.0  # step size in the mesh\n",
    "\n",
    "# we create an instance of SVM and fit out data. We do not scale our\n",
    "# data since we want to plot the support vectors\n",
    "C = 1.0  # SVM regularization parameter\n",
    "\n",
    "print(\"RBF SVM:\")\n",
    "X = combined_train_features.iloc[:,1:3]\n",
    "#X.head()\n",
    "\n",
    "rbf_svc = svm.SVC(kernel='rbf', gamma=0.7, C=C).fit(X, y)\n",
    "\n",
    "# create a mesh to plot in\n",
    "x_min, x_max = X.iloc[:,0].min() - 1, X.iloc[:,0].max() + 1\n",
    "y_min, y_max = X.iloc[:,1].min() - 1, X.iloc[:,1].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "\n",
    "Z = rbf_svc.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "\n",
    "# Put the result into a color plot\n",
    "Z = Z.reshape(xx.shape)\n",
    "plt.contourf(xx, yy, Z, cmap=plt.cm.coolwarm, alpha=0.8)\n",
    "\n",
    "# Plot also the training points\n",
    "plt.scatter(X.iloc[:, 0], X.iloc[:, 1], c=y, cmap=plt.cm.coolwarm)\n",
    "plt.xlabel('EDI')\n",
    "plt.ylabel('ESI')\n",
    "plt.xlim(xx.min(), xx.max())\n",
    "plt.ylim(yy.min(), yy.max())\n",
    "plt.xticks(())\n",
    "plt.yticks(())\n",
    "plt.title('SVM - RBF Kernel')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA2IAAANxCAYAAACG7QqnAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fc63ff68490>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(15,15))\n",
    "plt.contourf(xx, yy, Z, cmap=plt.cm.coolwarm, alpha=0.8)\n",
    "\n",
    "# Plot also the training points\n",
    "plt.scatter(X.iloc[:, 0], X.iloc[:, 1], c=y, cmap=plt.cm.coolwarm)\n",
    "plt.xlabel('EDI')\n",
    "plt.ylabel('ESI')\n",
    "plt.xlim(xx.min(), xx.max())\n",
    "plt.ylim(yy.min(), yy.max())\n",
    "plt.xticks(())\n",
    "plt.yticks(())\n",
    "plt.title('SVM - RBF Kernel')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear SVM\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import svm\n",
    "\n",
    "combined_train_features = pd.read_csv('data/combined-pe-features-apt-reduced.csv')\n",
    "train_labels = pd.read_csv('data/sorted-pe-coff-train-labels-apt.csv')\n",
    "\n",
    "#X = combined_train_features.iloc[:,1:]\n",
    "y = train_labels['label']\n",
    "h = .02  # step size in the mesh\n",
    "\n",
    "# we create an instance of SVM and fit out data. We do not scale our\n",
    "# data since we want to plot the support vectors\n",
    "C = 1.0  # SVM regularization parameter\n",
    "\n",
    "print(\"Linear SVM\")\n",
    "X = combined_train_features.iloc[:,1:3]\n",
    "svc = svm.SVC(kernel='linear', C=C).fit(X, y)\n",
    "\n",
    "print(\"RBF SVM\")\n",
    "X = combined_train_features.iloc[:,1:3]\n",
    "rbf_svc = svm.SVC(kernel='rbf', gamma=0.7, C=C).fit(X, y)\n",
    "\n",
    "print(\"Poly SVM\")\n",
    "X = combined_train_features.iloc[:,1:3]\n",
    "poly_svc = svm.SVC(kernel='poly', degree=3, C=C).fit(X, y)\n",
    "\n",
    "print(\"Linear SVM 2\")\n",
    "X = combined_train_features.iloc[:,1:3]\n",
    "lin_svc = svm.LinearSVC(C=C).fit(X, y)\n",
    "\n",
    "# create a mesh to plot in\n",
    "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "\n",
    "# title for the plots\n",
    "titles = ['SVC with linear kernel',\n",
    "          'LinearSVC (linear kernel)',\n",
    "          'SVC with RBF kernel',\n",
    "          'SVC with polynomial (degree 3) kernel']\n",
    "\n",
    "\n",
    "for i, clf in enumerate((svc, lin_svc, rbf_svc, poly_svc)):\n",
    "    # Plot the decision boundary. For that, we will assign a color to each\n",
    "    # point in the mesh [x_min, x_max]x[y_min, y_max].\n",
    "    plt.subplot(2, 2, i + 1)\n",
    "    plt.subplots_adjust(wspace=0.4, hspace=0.4)\n",
    "\n",
    "    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "\n",
    "    # Put the result into a color plot\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    plt.contourf(xx, yy, Z, cmap=plt.cm.coolwarm, alpha=0.8)\n",
    "\n",
    "    # Plot also the training points\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.coolwarm)\n",
    "    plt.xlabel('Sepal length')\n",
    "    plt.ylabel('Sepal width')\n",
    "    plt.xlim(xx.min(), xx.max())\n",
    "    plt.ylim(yy.min(), yy.max())\n",
    "    plt.xticks(())\n",
    "    plt.yticks(())\n",
    "    plt.title(titles[i])\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "help(plt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Lets do some plots and have look.\n",
    "plt.figure(figsize=(15,15))\n",
    "plt.xlabel(\"Vertex Count\")\n",
    "plt.ylabel(\"Edge Count\")\n",
    "xa = np.array(X['vertex_count'])\n",
    "xb = np.array(X['edge_count'])\n",
    "ya = np.array(y)\n",
    "plt.scatter(xa,xb,c=ya,cmap='brg')\n",
    "\n",
    "# Lets do some plots and have look.\n",
    "plt.figure(figsize=(15,15))\n",
    "plt.xlabel(\"EDX Register\")\n",
    "plt.ylabel(\"Malware Class\")\n",
    "xa = np.array(X['edx'])\n",
    "xb = np.array(X['esi'])\n",
    "ya = np.array(y)\n",
    "plt.scatter(xa,ya,c=ya,cmap='brg')\n",
    "\n",
    "plt.figure(figsize=(15,15))\n",
    "plt.xlabel(\"EDX Register\")\n",
    "plt.ylabel(\"ESI Register\")\n",
    "plt.scatter(xa,xb,c=ya,cmap='brg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Fix ASM feature file_name values.\n",
    "\n",
    "in_file = open('data/sorted-pe-asm-features-vs251.csv', 'r')\n",
    "in_lines = in_file.readlines()\n",
    "in_file.close()\n",
    "\n",
    "for line in in_lines:\n",
    "    idx = line.find('.pe')\n",
    "    if idx > 0:\n",
    "        # TODO:"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
