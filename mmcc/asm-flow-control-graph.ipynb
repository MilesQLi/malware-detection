{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 1. Generate Flow Control Graphs From Intel x86/AMD 64 Assembly\n",
    "       Parse a directory of .asm files and construct IDA Pro style flow control graphs using code\n",
    "       blocks as vertices and jmp/call instructions as directed edges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import graph as gra # http://www.python-course.eu/graphs_python.php\n",
    "import os\n",
    "from csv import writer\n",
    "from multiprocessing import Pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "opcodes = ['call','int','ja','jb','jc','je','jg','jge','jl','jle','jmp','jna','jnb','jnl','jno','jnp','jns','jnz','jo','jp','jz']\n",
    "blocks = ['sub_', 'loc_', 'locret_']\n",
    "#blocks = ['loc_','locret_']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TRAINING\n",
    "# Now divide the train files into four groups for multiprocessing\n",
    "ext_drive = '/opt/kaggle/train/'\n",
    "tfiles = os.listdir(ext_drive)\n",
    "quart = len(tfiles)/4\n",
    "train1 = tfiles[:quart]\n",
    "train2 = tfiles[quart:(2*quart)]\n",
    "train3 = tfiles[(2*quart):(3*quart)]\n",
    "train4 = tfiles[(3*quart):]\n",
    "print(len(tfiles), quart, (len(train1)+len(train2)+len(train3)+len(train4)))\n",
    "trains = [train1, train2, train3, train4]\n",
    "p = Pool(4)\n",
    "p.map(extract_flow_control_graphs, trains)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TESTING\n",
    "# Now divide the test files into four groups for multiprocessing\n",
    "ext_drive = '/opt/kaggle/test/'\n",
    "tfiles = os.listdir(ext_drive)\n",
    "quart = len(tfiles)/4\n",
    "test1 = tfiles[:quart]\n",
    "test2 = tfiles[quart:(2*quart)]\n",
    "test3 = tfiles[(2*quart):(3*quart)]\n",
    "test4 = tfiles[(3*quart):]\n",
    "print(len(tfiles), quart, (len(test1)+len(test2)+len(test3)+len(test4)))\n",
    "tests = [test1, test2, test3, test4]\n",
    "p = Pool(4)\n",
    "p.map(extract_flow_control_graphs, tests)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def construct_flow_control_graph(lines):\n",
    "    vertex = '.program_entry_point' # this is the root node, corresponds to the program entry point not C main().\n",
    "    vertex_count = 1\n",
    "    edge_count = 0\n",
    "    cfgraph = gra.Graph()\n",
    "    cfgraph.add_vertex(vertex)\n",
    "    \n",
    "    for row in lines:\n",
    "      row = row.rstrip('\\r\\n')  # get rid of newlines they are annoying.\n",
    "      if ';' in row:\n",
    "        row = row.split(';')[0] # get rid of comments they are annoying.\n",
    "        #print(row)\n",
    "      \n",
    "      # get rid of all these things they are annoying.\n",
    "      row = row.replace('short',' ')\n",
    "      row = row.replace('ds:',' ')\n",
    "      row = row.replace('dword',' ')\n",
    "      row = row.replace('ptr',' ').replace(':',' ').replace(',',' ') #.replace('??',' ')\n",
    "      row = row.replace('@','').replace('?','')\n",
    "      parts = row.split() # tokenize code line\n",
    "        \n",
    "      parts_len = len(parts)\n",
    "    \n",
    "      if (parts_len < 3): # this is just a comment line, do NOT change this!!!\n",
    "        continue\n",
    "        \n",
    "      if (parts_len > 3 ):\n",
    "        if (parts[3] == 'endp'): # skip procedure end labels\n",
    "            continue\n",
    "        if (parts[3] == 'proc'): # check for procedures not labelled sub_????\n",
    "            vertex = parts[2]\n",
    "            cfgraph.add_vertex(vertex)\n",
    "            vertex_count += 1\n",
    "            continue\n",
    "            \n",
    "      # check for subroutines and block labels\n",
    "      # block and subroutine labels are always after the .text HHHHHHHH relative address\n",
    "      for block in blocks:\n",
    "          token = parts[2]  \n",
    "          idx = token.find(block)\n",
    "          if (idx == 0): # add new vertex to the graph, we are now in a new subroutine or code block\n",
    "              vertex = token\n",
    "              cfgraph.add_vertex(vertex)\n",
    "              # print(\"Vertex: \" + vertex)\n",
    "              vertex_count += 1\n",
    "              break\n",
    "                              \n",
    "\n",
    "      # now check for edge opcode    \n",
    "      for opcode in opcodes: # check the line for a new edge\n",
    "          if opcode in parts:\n",
    "              # Extract desination address/function name/interrupt number as the directed edge.\n",
    "              idx = parts.index(opcode)\n",
    "              if ((idx + 1) < parts_len):\n",
    "                  next_vertex = parts[idx + 1]\n",
    "              else:\n",
    "                  next_vertex = \"none\"\n",
    "              cfgraph.add_edge(vertex, next_vertex)\n",
    "              # print(\"Edge: \" + vertex + \" \" + parts[idx] + \" \" + edge)\n",
    "              break\n",
    "\n",
    "    print(\"Vertex Count: {:d}\".format(vertex_count))\n",
    "    \n",
    "    return cfgraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "help(str.find)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "help(str.split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_flow_control_graphs(tfiles):\n",
    "    asm_files = [i for i in tfiles if '.asm' in i]\n",
    "    ftot = len(asm_files)\n",
    "    \n",
    "    pid = os.getpid()\n",
    "    print('Process id:', pid)\n",
    "    feature_file = 'data/' + str(pid) + '-malware-flow-control-graph.csv'  \n",
    "    print('Flow Control Graph file:', feature_file)\n",
    "    \n",
    "    graph_lines = []\n",
    "    with open(feature_file, 'w') as f:\n",
    "        # write the column names for the csv file\n",
    "        fw = writer(f)\n",
    "        # colnames = ['filename','entropy','filesize']\n",
    "        # fw.writerow(colnames)\n",
    "        \n",
    "        # Now iterate through the file list and extract the graph from each file.\n",
    "        for idx, fname in enumerate(asm_files):\n",
    "            fasm = open(ext_drive + fname, 'r', errors='ignore')\n",
    "            #filesize = os.path.getsize(ext_drive + fname)\n",
    "            lines = fasm.readlines()\n",
    "            \n",
    "            flow_control_graph = construct_flow_control_graph(lines)\n",
    "\n",
    "            graph_lines.append([fname[:fname.find('.asm')]] + [str(flow_control_graph.to_str_multi_line_sorted())])   \n",
    "            \n",
    "            del(flow_control_graph)\n",
    "            \n",
    "            # Print progress\n",
    "            if (idx+1) % 10 == 0:\n",
    "              print(pid, idx + 1, 'of', ftot, 'files processed.')\n",
    "              fw.writerows(graph_lines)\n",
    "              feature_counts = []\n",
    "                \n",
    "        # Write remaining files\n",
    "        if len(graph_lines) > 0:\n",
    "            fw.writerows(graph_lines)\n",
    "            graph_lines = []\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Convert Flow Control Graphs To Feature Vectors\n",
    "       Since determining graph isomorphisms is NP-Complete, convert the graphs to feature vectors and\n",
    "       use chi-squared tests to reduce the number features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 3. Generate Call Graphs From Intel x86/AMD 64 Assembly\n",
    "       Parse a directory of .asm files and construct call graphs using subroutines\n",
    "       as vertices and call instructions as directed edges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "call_opcodes = ['call','int']\n",
    "call_blocks = ['sub_']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def construct_call_graph(lines):\n",
    "    vertex = '.program_entry_point' # this is the root node, corresponds to the program entry point not C main().\n",
    "    vertex_count = 1\n",
    "    edge_count = 0\n",
    "    cfgraph = gra.Graph()\n",
    "    cfgraph.add_vertex(vertex)\n",
    "    \n",
    "    for row in lines:\n",
    "      row = row.rstrip('\\r\\n')  # get rid of newlines they are annoying.\n",
    "      if ';' in row:\n",
    "        row = row.split(';')[0] # get rid of comments they are annoying.\n",
    "        #print(row)\n",
    "      \n",
    "      # get rid of all these things they are annoying.\n",
    "      row = row.replace('short','').replace('ds:',' ')\n",
    "      row = row.replace('dword','').replace('near','')\n",
    "      row = row.replace('ptr','').replace(':',' ').replace(',',' ') #.replace('??',' ')\n",
    "      row = row.replace('@','').replace('?','')\n",
    "      parts = row.split() # tokenize code line\n",
    "        \n",
    "      if (len(parts) < 4): # this is just a comment line\n",
    "        continue\n",
    "        \n",
    "      if (parts[3] == 'endp'): # ignore subroutine end labels\n",
    "        continue\n",
    "        \n",
    "      # check for subroutines and block labels\n",
    "      # block and subroutine labels are always after the .text HHHHHHHH relative address\n",
    "      for block in call_blocks:\n",
    "          token = parts[2]  \n",
    "          idx = token.find(block)\n",
    "          if ((idx == 0) or (parts[3] == 'proc')):\n",
    "              # add new vertex to the graph, we are now in a new subroutine\n",
    "              vertex = token\n",
    "              cfgraph.add_vertex(vertex)\n",
    "              # print(\"Vertex: \" + vertex)\n",
    "              vertex_count += 1\n",
    "              break\n",
    "\n",
    "      # now check for edge opcode    \n",
    "      for opcode in call_opcodes: # check the line for a new edge\n",
    "          if opcode in parts:\n",
    "              # Extract desination address/function name/interrupt number as the directed edge.\n",
    "              idx = parts.index(opcode)\n",
    "              edge_count += 1\n",
    "              if ((idx + 1) < len(parts)): # in a few ASM files there is no operand, disassembly error?\n",
    "                  next_vertex = parts[idx + 1]\n",
    "              else:\n",
    "                  next_vertex = \"none\"\n",
    "              cfgraph.add_edge(vertex, next_vertex)\n",
    "              # print(\"Edge: \" + vertex + \" \" + parts[idx] + \" \" + edge)\n",
    "              break\n",
    "\n",
    "    # print(\"Vertex Count: {:d}\".format(vertex_count))\n",
    "    \n",
    "    return cfgraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_call_graphs(tfiles):\n",
    "    asm_files = [i for i in tfiles if '.asm' in i]\n",
    "    ftot = len(asm_files)\n",
    "    \n",
    "    pid = os.getpid()\n",
    "    print('Process id:', pid)\n",
    "    feature_file = 'data/' + str(pid) + '-malware-call-graph-features.csv'  \n",
    "    print('Graph Feature file:', feature_file)\n",
    "    \n",
    "    graph_lines = []\n",
    "    graph_features = []\n",
    "    graph_file = open('data/' + str(pid) + '-malware-call-graphs.gv', 'w') # write as a graphviz DOT format file\n",
    "    with open(feature_file, 'w') as f:\n",
    "        # write the column names for the csv file\n",
    "        fw = writer(f)\n",
    "        #colnames = ['filename','vertex_count','edge_count','delta_max','density','diameter']\n",
    "        colnames = ['filename','vertex_count','edge_count','delta_max','density']\n",
    "        fw.writerow(colnames)\n",
    "        \n",
    "        # Now iterate through the file list and extract the call graph from each file.\n",
    "        for idx, fname in enumerate(asm_files):\n",
    "            fasm = open(ext_drive + fname, 'r', errors='ignore')\n",
    "            \n",
    "            lines = fasm.readlines()\n",
    "            \n",
    "            call_graph = construct_call_graph(lines)\n",
    "            cgvc = call_graph.n_vertices()\n",
    "            cgec = call_graph.n_edges()\n",
    "            cgdm = call_graph.delta_max()\n",
    "            cgde = call_graph.density()\n",
    "            # cdia = call_graph.diameter() this is constantly problematic !!!\n",
    "            graph_features.append([fname[:fname.find('.asm')]] + [cgvc, cgec, cgdm, cgde])\n",
    "            call_graph.set_graph_name(fname[:fname.find('.asm')])\n",
    "            # graph_lines.append(call_graph.to_str('multinoleaf')) \n",
    "            graph_lines.append(call_graph.to_str('singlenoleaf'))\n",
    "            \n",
    "            del(call_graph) # for some reason new graphs get appended to the previous graphs if not deleted???\n",
    "            \n",
    "            # Print progress\n",
    "            if (idx + 1) % 10 == 0:\n",
    "                print(pid, idx + 1, 'of', ftot, 'files processed.')\n",
    "                fw.writerows(graph_features)\n",
    "                graph_file.writelines(graph_lines)\n",
    "                graph_features = []\n",
    "                graph_lines = []\n",
    "                \n",
    "        # Write remaining files\n",
    "        if len(graph_lines) > 0:\n",
    "            fw.writerows(graph_features)\n",
    "            graph_file.writelines(graph_lines)\n",
    "            graph_features = []\n",
    "            graph_lines = []\n",
    "\n",
    "    graph_file.close()\n",
    "    \n",
    "    print('Process id: {:d} finished.'.format(pid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 4. Generate Function Counts From Call Graphs.\n",
    "\n",
    "      Construct a dictionary of functions names and counts for every ASM file call graph then\n",
    "      write the function counts out to a csv feature file, it will be a sparse matrix.\n",
    "      feature columns will be like (filename, function names in sorted order.....)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Generate column names for the function count feature set\n",
    "#call_graph_files = ['../3815-malware-call-graphs.gv', '../3816-malware-call-graphs.gv', '../3817-malware-call-graphs.gv', '../3818-malware-call-graphs.gv']\n",
    "#call_graph_files = ['data/2278-malware-call-graphs.gv']\n",
    "\n",
    "def generate_column_names(call_graph_file):\n",
    "    counter = 0\n",
    "    column_names = ['filename']\n",
    "    graph_names = []\n",
    "    graph_name = \"none\"\n",
    "    graph_functions = {}\n",
    "\n",
    "    fapi = open(\"data/APIs.txt\")\n",
    "    defined_apis = fapi.readlines()\n",
    "    defined_apis = defined_apis[0].split(',')\n",
    "    fapi.close()\n",
    "    \n",
    "    pid = os.getpid()\n",
    "    print('Process id:', pid)\n",
    "    column_names_file = 'data/' + str(pid) + '-reduced-column-names.csv'  \n",
    "    print('Column names file: {:s}'.format(column_names_file))\n",
    "    graph_names_file = 'data/' + str(pid) + '-graph-names.csv'  \n",
    "    print('Graph names file: {:s}'.format(graph_names_file))    \n",
    "\n",
    "    with open(call_graph_file, 'r', errors='ignore') as cfg:\n",
    "        print(\"Starting graph file: {:s}\".format(call_graph_file))\n",
    "        for line in cfg:\n",
    "            line = line.rstrip('\\r\\n')  # get rid of newlines they are annoying.\n",
    "            # get rid of all these things they are annoying.\n",
    "            line = line.replace(',',' ').replace('[',' ').replace(']',' ').replace('->',' ').replace(\"\\'\", ' ')\n",
    "            parts = line.split() # tokenize call graph line\n",
    "            graph_name = parts[0]\n",
    "            parts = parts[1:]\n",
    "            graph_names.append(graph_name)\n",
    "            graph_functions = {}\n",
    "            \n",
    "            for func in parts:\n",
    "                if func not in defined_apis: # ignore these API functions, they have already been counted.\n",
    "                    if func.startswith('sub') or func.startswith('loc') or func.startswith('unk'):\n",
    "                        func = func[:5] # lets try to reduce the vast number of functions.\n",
    "                    elif func.startswith('eax+') or func.startswith('ebx+') or func.startswith('ecx+') or func.startswith('edx+'):\n",
    "                        func = func[:5]\n",
    "                    elif func.startswith('edi+') or func.startswith('esi+'):\n",
    "                        func = func[:5]\n",
    "                    elif func.startswith('byte_') or func.startswith('word_'): # or func.startswith('nullsub')\n",
    "                        func = func[:6]\n",
    "                    else: # reduce the feature set some more so my pissy pants PC can handle it.\n",
    "                        func = func[:8]\n",
    "                    if func not in column_names: # NOTE: or in Defined APIs, these have already been counted.    \n",
    "                        column_names.append(func)\n",
    "\n",
    " \n",
    "            counter += 1\n",
    "            # Print progress\n",
    "            if ((counter + 1) % 1000) == 0:\n",
    "                print(\"Processed number {:d} Graph_name {:s} Total column names {:d}\".format(counter,graph_name,len(column_names)))       \n",
    "\n",
    "                \n",
    "    with open(column_names_file, 'w') as cols:\n",
    "        fw = writer(cols)\n",
    "        fw.writerow(column_names)\n",
    "    \n",
    "    print(\"Completed writing {:d} column names.\".format(len(column_names)))\n",
    "\n",
    "    with open(graph_names_file, 'w') as gras:\n",
    "        fw = writer(gras)\n",
    "        fw.writerow(graph_names)\n",
    "    \n",
    "    print(\"Completed writing {:d} graph names.\".format(len(graph_names)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Generate the merged column names file single line.\n",
    "counter = 0\n",
    "column_names = []\n",
    "column_name_files = ['data/3346-reduced-column-names.csv', 'data/3347-reduced-column-names.csv', 'data/3348-reduced-column-names.csv', 'data/3349-reduced-column-names.csv']\n",
    "for cnamefile in column_name_files:\n",
    "    with open(cnamefile, 'r') as cras:\n",
    "        print(\"Starting file: {:s}\".format(cnamefile))\n",
    "        colstr = cras.readline()\n",
    "        colnames = colstr.split(',')\n",
    "        for cname in colnames:\n",
    "            if cname not in column_names:\n",
    "                column_names.append(cname)\n",
    "                \n",
    "            counter += 1\n",
    "            # Print progress\n",
    "            if ((counter + 1) % 1000) == 0:\n",
    "                print(\"Processed column names {:d}\".format(counter))       \n",
    "\n",
    "with open('data/all-reduced-function-column-names.csv', 'w') as cols:\n",
    "    fw = writer(cols)\n",
    "    fw.writerow(column_names)\n",
    "    \n",
    "print(\"Completed writing column names total = {:d}\".format(len(column_names)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Generate the merged column names file multiline.\n",
    "counter = 0\n",
    "column_names = []\n",
    "column_name_files = ['data/3346-reduced-column-names.csv', 'data/3347-reduced-column-names.csv', 'data/3348-reduced-column-names.csv', 'data/3349-reduced-column-names.csv']\n",
    "for cnamefile in column_name_files:\n",
    "    with open(cnamefile, 'r') as cras:\n",
    "        print(\"Starting file: {:s}\".format(cnamefile))\n",
    "        colstr = cras.readline()\n",
    "        colnames = colstr.split(',')\n",
    "        for cname in colnames:\n",
    "            if cname not in column_names:    \n",
    "                column_names.append(cname)\n",
    "                \n",
    "            counter += 1\n",
    "            # Print progress\n",
    "            if ((counter + 1) % 1000) == 0:\n",
    "                print(\"Processed column names {:d}\".format(counter))       \n",
    "\n",
    "with open('data/all-reduced-function-column-names-multiline.csv', 'w') as cols:\n",
    "    for cname in column_names:\n",
    "        outline = cname + \"\\n\"\n",
    "        cols.write(outline)\n",
    "    \n",
    "print(\"Completed writing column names total = {:d}\".format(len(column_names)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "help(cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed writing column names total = 154810\n"
     ]
    }
   ],
   "source": [
    "print(\"Completed writing column names total = {:d}\".format(len(column_names)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "call_graph_files = ['/opt/kaggle/3662-malware-call-graphs-sline.gv', '/opt/kaggle/3663-malware-call-graphs-sline.gv', '/opt/kaggle/3664-malware-call-graphs-sline.gv', '/opt/kaggle/3665-malware-call-graphs-sline.gv']\n",
    "p = Pool(4)\n",
    "p.map(generate_column_names, call_graph_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "792\n"
     ]
    }
   ],
   "source": [
    "print(len(defined_apis))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Generate function counts from graph files of the ASM malware samples.\n",
    "# call_graph_files = ['../3815-malware-call-graphs.gv', '../3816-malware-call-graphs.gv', '../3817-malware-call-graphs.gv', '../3818-malware-call-graphs.gv']\n",
    "\n",
    "def generate_function_counts(call_graph_file):\n",
    "    counter = 0\n",
    "    error_count = 0\n",
    "    \n",
    "    fapi = open(\"data/APIs.txt\")\n",
    "    defined_apis = fapi.readlines()\n",
    "    defined_apis = defined_apis[0].split(',')\n",
    "    fapi.close()\n",
    "    \n",
    "    colf = open('data/all-reduced-function-column-names.csv', 'r')\n",
    "    all_column_names = []\n",
    "    column_lines = colf.readlines()\n",
    "    for line in column_lines:\n",
    "        all_column_names += line.split(',')\n",
    "    col_names_len = len(all_column_names)\n",
    "    colf.close()\n",
    "    print(\"Column Names: {:d}\".format(col_names_len))\n",
    "    \n",
    "    pid = os.getpid()\n",
    "    print('Process id:', pid)\n",
    "    feature_file_name = 'data/' + str(pid) + '-call-graph-reduced-function_counts.csv'  \n",
    "    print('Call graph function counts file: {:s}'.format(feature_file_name))\n",
    "    feature_file = open(feature_file_name, 'w')\n",
    "    fw = writer(feature_file)\n",
    "    \n",
    "    call_graph_function_features = []\n",
    "    \n",
    "    with open(call_graph_file, 'r', errors='ignore') as cfg:\n",
    "        for line in cfg:\n",
    "            line.rstrip('\\r\\n')  # get rid of newlines they are annoying.\n",
    "            # get rid of all these things they are annoying.\n",
    "            line = line.replace(',',' ').replace('[',' ').replace(']',' ').replace('->',' ').replace(\"\\'\", ' ')\n",
    "            parts = line.split() # tokenize graph line\n",
    "            \n",
    "            graph_name = parts[0]\n",
    "            parts = parts[1:]\n",
    "            function_dict = {}\n",
    "            \n",
    "            # now generate the function counts for this call graph\n",
    "            \n",
    "            for func in parts:\n",
    "                if func not in defined_apis: # ignore these API functions, they have already been counted.\n",
    "                    if func.startswith('sub') or func.startswith('loc') or func.startswith('unk'):\n",
    "                        func = func[:5] # lets try to reduce the vast number of functions.\n",
    "                    elif func.startswith('eax+') or func.startswith('ebx+') or func.startswith('ecx+') or func.startswith('edx+'):\n",
    "                        func = func[:5]\n",
    "                    elif func.startswith('edi+') or func.startswith('esi+'):\n",
    "                        func = func[:5]\n",
    "                    elif func.startswith('byte_') or func.startswith('word_'): # or func.startswith('nullsub')\n",
    "                        func = func[:6]\n",
    "                    else: # reduce the feature set some more so my pissy pants PC can handle it.\n",
    "                        func = func[:8]\n",
    "                        \n",
    "                    if (func in function_dict):\n",
    "                        function_dict[func] += 1\n",
    "                    else:\n",
    "                        function_dict[func] = 1\n",
    "            \n",
    "            # now generate the output row for this call graph\n",
    "\n",
    "            function_counts = [0] * col_names_len # zero everything because this is a sparse matrix\n",
    "            for func in function_dict:\n",
    "                for idx, cname in enumerate(all_column_names):\n",
    "                    if func == cname:\n",
    "                        function_counts[idx] = function_dict[func]\n",
    "                        break\n",
    "                \n",
    "            call_graph_function_features.append([graph_name] + function_counts)\n",
    "            \n",
    "            # Print progress and write out rows\n",
    "            counter += 1\n",
    "            if ((counter + 1) % 100) == 0:\n",
    "                print(\"{:d} Graph: {:s} Count: {:d}\".format(pid, graph_name, counter))\n",
    "                fw.writerows(call_graph_function_features)\n",
    "                call_graph_function_features = []\n",
    "                \n",
    "        # Write remaining files\n",
    "        if len(call_graph_function_features) > 0:\n",
    "            fw.writerows(call_graph_function_features)\n",
    "            call_graph_function_features = []  \n",
    "    \n",
    "    feature_file.close()\n",
    "    \n",
    "    print(\"Completed processing {:d} graphs.\".format(counter))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "call_graph_files = ['/opt/kaggle/3662-malware-call-graphs-sline.gv', '/opt/kaggle/3663-malware-call-graphs-sline.gv', '/opt/kaggle/3664-malware-call-graphs-sline.gv', '/opt/kaggle/3665-malware-call-graphs-sline.gv']\n",
    "p = Pool(4)\n",
    "p.map(generate_function_counts, call_graph_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Ok, so we still have 71000+ features even after severely reducing the function name lengths.\n",
    "# This is a problem. Having to process such a huge sparse matrix requires a lot of memory.\n",
    "# Solution 1: rent an AWS server with plenty-o-ram.\n",
    "# Solution 2: buy more RAM for my linux box.\n",
    "# Solution 3: break the sparse matrix into smaller chunks and process individually.\n",
    "# Solution 4: try the pandas sparse matrix data structure.\n",
    "# Goto: feature-reduction-call-graphs.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TRAINING\n",
    "# Now divide the train files into four groups for multiprocessing\n",
    "ext_drive = '/opt/kaggle/train/'\n",
    "tfiles = os.listdir(ext_drive)\n",
    "quart = int(len(tfiles)/4)\n",
    "train1 = tfiles[:quart]\n",
    "train2 = tfiles[quart:(2*quart)]\n",
    "train3 = tfiles[(2*quart):(3*quart)]\n",
    "train4 = tfiles[(3*quart):]\n",
    "print(len(tfiles), quart, (len(train1)+len(train2)+len(train3)+len(train4)))\n",
    "trains = [train1, train2, train3, train4]\n",
    "p = Pool(4)\n",
    "p.map(extract_call_graphs, trains)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TESTING\n",
    "# Now divide the test files into four groups for multiprocessing\n",
    "ext_drive = '/opt/kaggle/test/'\n",
    "tfiles = os.listdir(ext_drive)\n",
    "quart = int(len(tfiles)/4)\n",
    "test1 = tfiles[:quart]\n",
    "test2 = tfiles[quart:(2*quart)]\n",
    "test3 = tfiles[(2*quart):(3*quart)]\n",
    "test4 = tfiles[(3*quart):]\n",
    "print(len(tfiles), quart, (len(test1)+len(test2)+len(test3)+len(test4)))\n",
    "tests = [test1, test2, test3, test4]\n",
    "p = Pool(4)\n",
    "p.map(extract_call_graphs, tests)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 5. Test Code Only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process id: 4845\n",
      "Graph file: data/4845-malware-graph.csv\n",
      "Vertex Count: 1103\n",
      "Vertex Count: 6418\n"
     ]
    }
   ],
   "source": [
    "# Test graph generation\n",
    "ext_drive = '/opt/kaggle/train/'\n",
    "tfiles = ['0A32eTdBKayjCWhZqDOQ.asm', '1aAwe4J9VHrsq8uEoZhf.asm']\n",
    "extract_graph(tfiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process id: 17216\n",
      "Graph file: data/17216-malware-graph.csv\n",
      "Vertex Count: 1103\n",
      "Vertex Count: 180\n"
     ]
    }
   ],
   "source": [
    "# Test graph generation\n",
    "ext_drive = '/opt/kaggle/train/'\n",
    "tfiles = ['0A32eTdBKayjCWhZqDOQ.asm', '0ACDbR5M3ZhBJajygTuf.asm']\n",
    "extract_graph(tfiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process id: 21155\n",
      "Flow Control Graph file: data/21155-malware-flow-control-graph.csv\n",
      "Vertex Count: 179\n"
     ]
    }
   ],
   "source": [
    "# Test graph generation\n",
    "ext_drive = '/opt/kaggle/train/'\n",
    "tfiles = ['0ACDbR5M3ZhBJajygTuf.asm']\n",
    "extract_flow_control_graphs(tfiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process id: 17385\n",
      "Graph file: data/17385-malware-call-graph.csv\n",
      "Vertex Count: 179\n",
      "Vertex Count: 5\n"
     ]
    }
   ],
   "source": [
    "# Test call graph generation\n",
    "ext_drive = '/opt/kaggle/train/'\n",
    "tfiles = ['0A32eTdBKayjCWhZqDOQ.asm', '0ACDbR5M3ZhBJajygTuf.asm']\n",
    "extract_call_graphs(tfiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process id: 2278\n",
      "Graph Feature file: data/2278-malware-call-graph-features.csv\n",
      "Process id: 2278 finished.\n"
     ]
    }
   ],
   "source": [
    "# Test call graph generation\n",
    "ext_drive = '/opt/kaggle/train/'\n",
    "tfiles = ['0A32eTdBKayjCWhZqDOQ.asm', '0ACDbR5M3ZhBJajygTuf.asm']\n",
    "extract_call_graphs(tfiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "g = { \"a\" : [\"c\"],\n",
    "      \"b\" : [\"c\",\"e\",\"f\"],\n",
    "      \"c\" : [\"a\",\"b\",\"d\",\"e\"],\n",
    "      \"d\" : [\"c\"],\n",
    "      \"e\" : [\"b\",\"c\",\"f\"],\n",
    "      \"f\" : [\"b\",\"e\"]\n",
    "}\n",
    "\n",
    "\n",
    "graph = gra.Graph(g)\n",
    "\n",
    "diameter = graph.diameter()\n",
    "\n",
    "print(diameter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process id: 20756\n",
      "Graph file: data/20756-malware-call-graph.csv\n",
      "Vertex Count: 90\n",
      "Vertex Count: 4\n"
     ]
    }
   ],
   "source": [
    "# Test call graph generation\n",
    "ext_drive = '/opt/kaggle/train/'\n",
    "tfiles = ['0A32eTdBKayjCWhZqDOQ.asm', '0ACDbR5M3ZhBJajygTuf.asm']\n",
    "extract_call_graphs(tfiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process id: 20756\n",
      "Graph file: data/20756-malware-call-graph.csv\n",
      "Vertex Count: 90\n",
      "Vertex Count: 4\n"
     ]
    }
   ],
   "source": [
    "# Test call graph generation\n",
    "ext_drive = '/opt/kaggle/train/'\n",
    "tfiles = ['0A32eTdBKayjCWhZqDOQ.asm', '0ACDbR5M3ZhBJajygTuf.asm']\n",
    "extract_call_graphs(tfiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(cfgraph.vertices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fasm = open('/opt/kaggle/train/1aAwe4J9VHrsq8uEoZhf.asm', 'r', errors='ignore')\n",
    "lines = fasm.readlines()\n",
    "parse_asm_code(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fasm = open('/opt/kaggle/train/0A32eTdBKayjCWhZqDOQ.asm', 'r', errors='ignore')\n",
    "lines = fasm.readlines()\n",
    "cfgraph = parse_asm_code(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "help(open)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bar': 2, 'foo': 1}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import OrderedDict\n",
    "a = {'foo': 1, 'bar': 2}\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('bar', 2), ('foo', 1)])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{'foo': 1, 'bar': 2}\n",
    "b = OrderedDict(sorted(a.items()))\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 5, 10]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myDic={10: 'b', 3:'a', 5:'c'}\n",
    "sorted_list=sorted(myDic.keys())\n",
    "sorted_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Deprecated, too slowwwww.\n",
    "\n",
    "def construct_function_dict(call_graph):\n",
    "    vertex_dict = call_graph.get_vertex_counts()\n",
    "    file_name = call_graph.get_graph_name()\n",
    "    function_dict[file_name] = vertex_dict\n",
    "            \n",
    "    return\n",
    "\n",
    "def generate_column_names():\n",
    "    for file_name in function_dict:\n",
    "        function_counts = function_dict[file_name]\n",
    "        for func_name in function_counts:\n",
    "            if func_name not in function_column_names:\n",
    "                function_column_names.append(func_name)\n",
    "            \n",
    "    function_column_names.sort()\n",
    "    \n",
    "    return\n",
    "\n",
    "def write_function_counts():\n",
    "    # open function count feature file\n",
    "    pid = os.getpid()\n",
    "    feature_file = 'data/' + str(pid) + '-malware-function-count-features.csv'  \n",
    "    print('Function Count Feature file:', feature_file)\n",
    "    f = open(feature_file, 'w')\n",
    "    fw = writer(f)\n",
    "    \n",
    "    colnames = ['filename'] + function_column_names\n",
    "    fw.writerow(colnames)\n",
    "    \n",
    "    counter = 0\n",
    "    function_features = []\n",
    "    function_count_list = [0] * len(function_column_names)\n",
    "    for filename in function_dict:\n",
    "        function_counts = function_dict[filename]\n",
    "        for function in function_counts:\n",
    "            idx = function_column_names.index(function)\n",
    "\n",
    "            function_count_list[idx] = function_counts[function]\n",
    "        \n",
    "        print(filename)\n",
    "        print(function_count_list)\n",
    "        function_features.append([filename] + function_count_list)\n",
    "        counter += 1\n",
    "        # Print progress\n",
    "        if (counter + 1) % 10 == 0:\n",
    "            print(pid, counter + 1, ' files processed.')\n",
    "            fw.writerows(function_features)\n",
    "            function_features = []\n",
    "                \n",
    "        \n",
    "    # Write remaining files\n",
    "    if (len(function_features) > 0):\n",
    "        fw.writerows(function_features)\n",
    "        func_features = []\n",
    "       \n",
    "        \n",
    "    f.close()\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
