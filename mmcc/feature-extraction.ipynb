{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from multiprocessing import Pool\n",
    "import os\n",
    "from csv import writer\n",
    "import numpy as np\n",
    "import math\n",
    "import scipy.misc\n",
    "import array\n",
    "import time as tm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. ASM Feature Extraction\n",
    "#### Run time for asm feature extraction: 21.5 hours\n",
    "Machine:\n",
    "- AMD 6 Core CPU\n",
    "- 8GB RAM\n",
    "- 120GB SSD\n",
    "- 2TB HDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "keywords = ['Virtual','Offset','loc','Import','Imports','var','Forwarder','UINT','LONG','BOOL','WORD','BYTES','large','short','dd','db','dw','XREF','ptr','DATA','FUNCTION','extrn','byte','word','dword','char','DWORD','stdcall','arg','locret','asc','align','WinMain','unk','cookie','off','nullsub','DllEntryPoint','System32','dll','CHUNK','BASS','HMENU','DLL','LPWSTR','void','HRESULT','HDC','LRESULT','HANDLE','HWND','LPSTR','int','HLOCAL','FARPROC','ATOM','HMODULE','WPARAM','HGLOBAL','entry','rva','COLLAPSED','config','exe','Software','CurrentVersion','__imp_','INT_PTR','UINT_PTR','---Seperator','PCCTL_CONTEXT','__IMPORT_','INTERNET_STATUS_CALLBACK','.rdata:','.data:','.text:','case','installdir','market','microsoft','policies','proc','scrollwindow','search','trap','visualc','___security_cookie','assume','callvirtualalloc','exportedentry','hardware','hkey_current_user','hkey_local_machine','sp-analysisfailed','unableto']\n",
    "known_sections = ['.text', '.data', '.bss', '.rdata', '.edata', '.idata', '.rsrc', '.tls', '.reloc']\n",
    "registers = ['edx','esi','es','fs','ds','ss','gs','cs','ah','al',\n",
    "                 'ax','bh','bl','bx','ch','cl','cx','dh','dl','dx',\n",
    "                 'eax','ebp','ebx','ecx','edi','esp']\n",
    "\n",
    "opcodes = ['add','al','bt','call','cdq','cld','cli','cmc','cmp','const','cwd','daa','db'\n",
    "                ,'dd','dec','dw','endp','ends','faddp','fchs','fdiv','fdivp','fdivr','fild'\n",
    "                ,'fistp','fld','fstcw','fstcwimul','fstp','fword','fxch','imul','in','inc'\n",
    "                ,'ins','int','jb','je','jg','jge','jl','jmp','jnb','jno','jnz','jo','jz'\n",
    "                ,'lea','loope','mov','movzx','mul','near','neg','not','or','out','outs'\n",
    "                ,'pop','popf','proc','push','pushf','rcl','rcr','rdtsc','rep','ret','retn'\n",
    "                ,'rol','ror','sal','sar','sbb','scas','setb','setle','setnle','setnz'\n",
    "                ,'setz','shl','shld','shr','sidt','stc','std','sti','stos','sub','test'\n",
    "                ,'wait','xchg','xor']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "paths = ['train','test']\n",
    "\n",
    "def count_1gram(byte_code):\n",
    "    OneByte = [0]*16**2\n",
    "    for row in byte_code:\n",
    "        row = row.rstrip('\\r\\n')\n",
    "        codes = row.split()[1:]\n",
    "        # Convert code to 1byte\n",
    "        OneByteCode = []\n",
    "        for i in codes:\n",
    "            if i != '??':\n",
    "                OneByteCode += [int(i,16)]\n",
    "\n",
    "        # Calculate the frequency of 1byte\n",
    "        for i in OneByteCode:\n",
    "                    OneByte[i] += 1\n",
    "    return OneByte\n",
    "\n",
    "\n",
    "def count_2gram(byte_code):\n",
    "    twoByte = [0]*16**4\n",
    "    for row in byte_code:\n",
    "        codes = row[:-2].split()[1:]\n",
    "        codes_2g = codes[:-1]\n",
    "        for i in range(len(codes_2g)):\n",
    "            codes_2g[i] += codes[i+1]\n",
    "\n",
    "        twoByteCode = []\n",
    "        for i in codes_2g:\n",
    "            if '??' not in i:\n",
    "                twoByteCode += [int(i,16)]\n",
    "\n",
    "        for i in twoByteCode:\n",
    "            twoByte[i] += 1\n",
    "    return twoByte\n",
    "\n",
    "def count_4gram(byte_code): #TODO: NOT DONE YET\n",
    "    twoByte = [0]*16**8\n",
    "    for row in byte_code:\n",
    "        codes = row[:-2].split()[1:]\n",
    "        codes_2g = codes[:-1]\n",
    "        for i in range(len(codes_2g)):\n",
    "            codes_2g[i]+= codes[i+1]\n",
    "\n",
    "        twoByteCode = []\n",
    "        for i in codes_2g:\n",
    "            if '??' not in i:\n",
    "                twoByteCode += [int(i,16)]\n",
    "\n",
    "        for i in twoByteCode:\n",
    "            twoByte[i] += 1\n",
    "    return twoByte\n",
    "\n",
    "def count_asm_symbols(asm_code):\n",
    "    symbols = [0]*7\n",
    "    for row in asm_code:\n",
    "        if '*' in row:\n",
    "            symbols[0] += 1\n",
    "        if '-' in row:\n",
    "            symbols[1] += 1\n",
    "        if '+' in row:\n",
    "            symbols[2] += 1\n",
    "        if '[' in row:\n",
    "            symbols[3] += 1\n",
    "        if ']' in row:\n",
    "            symbols[4] += 1\n",
    "        if '@' in row:\n",
    "            symbols[5] += 1\n",
    "        if '?' in row:\n",
    "            symbols[6] += 1\n",
    "\n",
    "    return symbols\n",
    "\n",
    "\n",
    "def count_asm_registers(asm_code):\n",
    "    registers_values = [0]*len(registers)\n",
    "    for row in asm_code:\n",
    "        parts = row.replace(',',' ').replace('+',' ').replace('*',' ').replace('[',' ').replace(']',' ') \\\n",
    "                    .replace('-',' ').split()\n",
    "        for register in registers:\n",
    "            registers_values[registers.index(register)] += parts.count(register)\n",
    "    return registers_values\n",
    "\n",
    "\n",
    "def count_asm_opcodes(asm_code):\n",
    "    opcodes_values = [0]*len(opcodes)\n",
    "    for row in asm_code:\n",
    "        parts = row.split()\n",
    "\n",
    "        for opcode in opcodes:\n",
    "            if opcode in parts:\n",
    "                opcodes_values[opcodes.index(opcode)] += 1\n",
    "                break\n",
    "    return opcodes_values\n",
    "\n",
    "\n",
    "def count_asm_APIs(asm_code, apis):\n",
    "    apis_values = [0]*len(apis)\n",
    "    for row in asm_code:\n",
    "        for i in range(len(apis)):\n",
    "            if apis[i] in row:\n",
    "                apis_values[i] += 1\n",
    "                break\n",
    "    return apis_values\n",
    "\n",
    "\n",
    "def count_asm_misc(asm_code):\n",
    "    keywords_values = [0]*len(keywords)\n",
    "    for row in asm_code:\n",
    "        for i in range(len(keywords)):\n",
    "            if keywords[i] in row:\n",
    "                keywords_values[i] += 1\n",
    "                break\n",
    "    return keywords_values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Extract features from test/training asm files, file list is passed in as a parameter\n",
    "\n",
    "def extract_asm_features(tfiles):\n",
    "    \n",
    "    pid = os.getpid()\n",
    "    print('Process id:', pid)\n",
    "    feature_file = 'data/' + str(pid) + '-malware-features-asm.csv' # Windows API, symbols, registers, opcodes, etc...   \n",
    "    print('feature file:', feature_file)\n",
    "\n",
    "    fapi = open(\"data/APIs.txt\")\n",
    "    defined_apis = fapi.readlines()\n",
    "    defined_apis = defined_apis[0].split(',')\n",
    "\n",
    "    asm_files = [i for i in tfiles if '.asm' in i]\n",
    "    ftot = len(asm_files)\n",
    "    \n",
    "    feature_counts = []\n",
    "    with open(feature_file, 'w') as f:\n",
    "        # write the csv header\n",
    "        fw = writer(f)\n",
    "        colnames = ['filename'] + registers + opcodes + defined_apis + keywords\n",
    "        fw.writerow(colnames)\n",
    "        \n",
    "        for idx, fname in enumerate(asm_files):\n",
    "            fasm = open(ext_drive + fname, 'r')\n",
    "            content = fasm.readlines()\n",
    "            \n",
    "            reg_vals = count_asm_registers(content)\n",
    "            opc_vals = count_asm_opcodes(content)\n",
    "            api_vals = count_asm_APIs(content, defined_apis)\n",
    "            #sec_vals = count_asm_sections(content)\n",
    "            mis_vals = count_asm_misc(content)\n",
    "            count_vals = reg_vals + opc_vals + api_vals + mis_vals\n",
    "            \n",
    "            feature_counts.append([fname[:fname.find('.asm')]] + count_vals)   \n",
    "            \n",
    "            # Writing rows after every 10 files processed\n",
    "            if (idx+1) % 10 == 0:\n",
    "              print(pid, idx + 1, 'of', ftot, 'files processed.')\n",
    "              fw.writerows(feature_counts)\n",
    "              feature_counts = []\n",
    "                \n",
    "        # Writing remaining files\n",
    "        if len(feature_counts) > 0:\n",
    "            fw.writerows(feature_counts)\n",
    "            feature_counts = []\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TRAINING\n",
    "# Now divide the train files into four groups for multiprocessing\n",
    "ext_drive = '/opt/kaggle/train/'\n",
    "tfiles = os.listdir(ext_drive)\n",
    "quart = len(tfiles)/4\n",
    "train1 = tfiles[:quart]\n",
    "train2 = tfiles[quart:(2*quart)]\n",
    "train3 = tfiles[(2*quart):(3*quart)]\n",
    "train4 = tfiles[(3*quart):]\n",
    "print len(tfiles), quart, (len(train1)+len(train2)+len(train3)+len(train4))\n",
    "trains = [train1, train2, train3, train4]\n",
    "p = Pool(4)\n",
    "p.map(extract_asm_features, trains)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TESTING\n",
    "# Now divide the test files into four groups for multiprocessing\n",
    "ext_drive = '/opt/kaggle/test/'\n",
    "tfiles = os.listdir(ext_drive)\n",
    "quart = len(tfiles)/4\n",
    "test1 = tfiles[:quart]\n",
    "test2 = tfiles[quart:(2*quart)]\n",
    "test3 = tfiles[(2*quart):(3*quart)]\n",
    "test4 = tfiles[(3*quart):]\n",
    "print len(tfiles), quart, (len(test1)+len(test2)+len(test3)+len(test4))\n",
    "tests = [test1, test2, test3, test4]\n",
    "p = Pool(4)\n",
    "p.map(extract_asm_features, tests)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Byte Feature Extraction\n",
    "#### Run time for byte feature extraction: 35 minutes\n",
    "Machine:\n",
    "- AMD 6 Core CPU\n",
    "- 8GB RAM\n",
    "- 120GB SSD\n",
    "- 2TB HDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Calculate Shannon's Entropy, https://en.wikipedia.org/wiki/Entropy_(information_theory)\n",
    "\n",
    "def calculate_entropy(byte_counts, total):\n",
    "    \n",
    "  entropy = 0.0\n",
    "\n",
    "  for count in byte_counts:\n",
    "    # If no bytes of this value were seen in the value, it doesn't affect\n",
    "    # the entropy of the file.\n",
    "    if count == 0:\n",
    "        continue\n",
    "    # p is the probability of seeing this byte in the file, as a floating-point number\n",
    "    p = 1.0 * count / total\n",
    "    entropy -= p * math.log(p, 256)\n",
    "    \n",
    "\n",
    "  return entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def entropy_counter(byte_code):\n",
    "  byte_counts = [0] * 256\n",
    "  total = 0\n",
    "  #code_length = len(byte_code)\n",
    "  for row in byte_code:\n",
    "    nrow = row.rstrip('\\r\\n')\n",
    "    bytes = nrow.split(' ')\n",
    "    # skip first token as it is the relative memory address\n",
    "    # print(bytes) \n",
    "    for i in range(1, len(bytes)):\n",
    "      #   print(bytes[i])\n",
    "      if bytes[i] != '??':\n",
    "        byte_counts[int(bytes[i], 16)] += 1\n",
    "      else:\n",
    "        byte_counts[0] += 1\n",
    "      total += 1\n",
    "        \n",
    "  entropy = calculate_entropy(byte_counts, total)\n",
    "\n",
    "  return entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# feature extraction for the .byte files\n",
    "\n",
    "def extract_byte_features(tfiles):\n",
    "    byte_files = [i for i in tfiles if '.bytes' in i]\n",
    "    ftot = len(byte_files)\n",
    "    \n",
    "    pid = os.getpid()\n",
    "    print('Process id:', pid)\n",
    "    feature_file = 'data/' + str(pid) + '-malware-features-byte.csv' # entropy, file size, ngrams...   \n",
    "    print('feature file:', feature_file)\n",
    "    \n",
    "    feature_counts = []\n",
    "    with open(feature_file, 'w') as f:\n",
    "        # write the column names for the csv file\n",
    "        fw = writer(f)\n",
    "        colnames = ['filename'] + ['entropy'] + ['filesize']\n",
    "        fw.writerow(colnames)\n",
    "        \n",
    "        # Now iterate through the file list and extract the features from each file.\n",
    "        for idx, fname in enumerate(byte_files):\n",
    "            fasm = open(ext_drive + fname, 'r')\n",
    "            filesize = os.path.getsize(ext_drive + fname)\n",
    "            lines = fasm.readlines()\n",
    "            \n",
    "            # TODO: Do ngram extraction\n",
    "            # First do entropy calculations and filesize\n",
    "            \n",
    "            entropy = entropy_counter(lines)\n",
    "            #print(fname + ' : entropy = ' + str(entropy) + ' file size = ' + str(filesize))\n",
    "            count_vals = [entropy, filesize]\n",
    "            \n",
    "            feature_counts.append([fname[:fname.find('.byte')]] + count_vals)   \n",
    "            \n",
    "            # Print progress\n",
    "            if (idx+1) % 10 == 0:\n",
    "              print(pid, idx + 1, 'of', ftot, 'files processed.')\n",
    "              fw.writerows(feature_counts)\n",
    "              feature_counts = []\n",
    "                \n",
    "        # Write remaining files\n",
    "        if len(feature_counts) > 0:\n",
    "            fw.writerows(feature_counts)\n",
    "            feature_counts = []\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TRAINING\n",
    "# Now divide the train files into four groups for multiprocessing\n",
    "ext_drive = '/opt/kaggle/train/'\n",
    "tfiles = os.listdir(ext_drive)\n",
    "quart = len(tfiles)/4\n",
    "train1 = tfiles[:quart]\n",
    "train2 = tfiles[quart:(2*quart)]\n",
    "train3 = tfiles[(2*quart):(3*quart)]\n",
    "train4 = tfiles[(3*quart):]\n",
    "print len(tfiles), quart, (len(train1)+len(train2)+len(train3)+len(train4))\n",
    "trains = [train1, train2, train3, train4]\n",
    "p = Pool(4)\n",
    "p.map(extract_byte_features, trains)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TESTING\n",
    "# Now divide the test files into four groups for multiprocessing\n",
    "ext_drive = '/opt/kaggle/test/'\n",
    "tfiles = os.listdir(ext_drive)\n",
    "quart = len(tfiles)/4\n",
    "test1 = tfiles[:quart]\n",
    "test2 = tfiles[quart:(2*quart)]\n",
    "test3 = tfiles[(2*quart):(3*quart)]\n",
    "test4 = tfiles[(3*quart):]\n",
    "print len(tfiles), quart, (len(test1)+len(test2)+len(test3)+len(test4))\n",
    "tests = [test1, test2, test3, test4]\n",
    "p = Pool(4)\n",
    "p.map(extract_byte_features, tests)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Byte and ASM Image Extraction\n",
    "    \n",
    "#### Run time for byte feature extraction: 2.4 hours\n",
    "Machine:\n",
    "- AMD 6 Core CPU\n",
    "- 8GB RAM\n",
    "- 120GB SSD\n",
    "- 2TB HDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# From Say_No_to_Overfitting\n",
    "def entropy(p,n):\n",
    "    p_ratio = float(p)/(p+n)\n",
    "    n_ratio = float(n)/(p+n)\n",
    "    return -p_ratio*math.log(p_ratio) - n_ratio * math.log(n_ratio)\n",
    "\n",
    "def info_gain(p0,n0,p1,n1,p,n):\n",
    "    return entropy(p,n) - float(p0+n0)/(p+n)*entropy(p0,n0) - float(p1+n1)/(p+n)*entropy(p1,n1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_image(filename):\n",
    "    f = open(filename,'rb')\n",
    "    ln = os.path.getsize(filename) # length of file in bytes\n",
    "    width = 256\n",
    "    rem = ln%width\n",
    "    a = array.array(\"B\") # uint8 array\n",
    "    a.fromfile(f,ln-rem)\n",
    "    f.close()\n",
    "    g = np.reshape(a,(len(a)/width,width))\n",
    "    g = np.uint8(g)\n",
    "    g.resize((1000,))\n",
    "    return list(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Do asm image extraction\n",
    "def extract_asm_image_features(tfiles):\n",
    "    asm_files = [i for i in tfiles if '.asm' in i]\n",
    "    ftot = len(asm_files)\n",
    "    \n",
    "    pid = os.getpid()\n",
    "    print('Process id:', pid)\n",
    "    feature_file = 'data/' + str(pid) + '-test-image-features-asm.csv'  \n",
    "    print('feature file:', feature_file)\n",
    "    \n",
    "    outrows = []\n",
    "    with open(feature_file,'w') as f:\n",
    "        fw = writer(f)\n",
    "        column_names = ['filename'] + [(\"ASM_{:s}\".format(str(x))) for x in range(1000)]\n",
    "        fw.writerow(column_names)\n",
    "        for idx, fname in enumerate(asm_files):\n",
    "            file_id = fname.split('.')[0]\n",
    "            image_data = read_image(ext_drive + fname)\n",
    "            outrows.append([file_id] + image_data)\n",
    "            \n",
    "            # Print progress\n",
    "            if (idx+1) % 10 == 0:\n",
    "              print(pid, idx + 1, 'of', ftot, 'files processed.')\n",
    "              fw.writerows(outrows)\n",
    "              outrows = []\n",
    "                                       \n",
    "        # Write remaining files\n",
    "        if len(outrows) > 0:\n",
    "            fw.writerows(outrows)\n",
    "            outrows = []                                       \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Do byte image extraction\n",
    "def extract_byte_image_features(tfiles):\n",
    "    asm_files = [i for i in tfiles if '.bytes' in i]\n",
    "    ftot = len(asm_files)\n",
    "    \n",
    "    pid = os.getpid()\n",
    "    print('Process id:', pid)\n",
    "    feature_file = 'data/' + str(pid) + '-train-image-features-byte.csv'   \n",
    "    print('feature file:', feature_file)\n",
    "    \n",
    "    outrows = []\n",
    "    with open(feature_file,'w') as f:\n",
    "        fw = writer(f)\n",
    "        column_names = ['filename'] + [(\"BYTE_{:s}\".format(str(x))) for x in range(1000)]\n",
    "        fw.writerow(column_names)\n",
    "        for idx, fname in enumerate(asm_files):\n",
    "            file_id = fname.split('.')[0]\n",
    "            image_data = read_image(ext_drive + fname)\n",
    "            outrows.append([file_id] + image_data)\n",
    "            \n",
    "            # Print progress\n",
    "            if (idx+1) % 10 == 0:\n",
    "              print(pid, idx + 1, 'of', ftot, 'files processed.')\n",
    "              fw.writerows(outrows)\n",
    "              outrows = []\n",
    "                                       \n",
    "        # Write remaining files\n",
    "        if len(outrows) > 0:\n",
    "            fw.writerows(outrows)\n",
    "            outrows = []                                       \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TRAIN FILES ASM\n",
    "# Now divide the train files into four groups for multiprocessing\n",
    "start_time = tm.time()\n",
    "ext_drive = '/opt/kaggle/train/'\n",
    "tfiles = os.listdir(ext_drive)\n",
    "quart = len(tfiles)/4\n",
    "train1 = tfiles[:quart]\n",
    "train2 = tfiles[quart:(2*quart)]\n",
    "train3 = tfiles[(2*quart):(3*quart)]\n",
    "train4 = tfiles[(3*quart):]\n",
    "print len(tfiles), quart, (len(train1)+len(train2)+len(train3)+len(train4))\n",
    "trains = [train1, train2, train3, train4]\n",
    "p = Pool(4)\n",
    "p.map(extract_asm_image_features, trains)\n",
    "print(\"Elapsed time: {:.2f} hours.\".format((tm.time() - start_time)/3600.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# TRAIN FILES BYTE\n",
    "# Now divide the train files into four groups for multiprocessing\n",
    "start_time = tm.time()\n",
    "ext_drive = '/opt/kaggle/train/'\n",
    "tfiles = os.listdir(ext_drive)\n",
    "quart = len(tfiles)/4\n",
    "train1 = tfiles[:quart]\n",
    "train2 = tfiles[quart:(2*quart)]\n",
    "train3 = tfiles[(2*quart):(3*quart)]\n",
    "train4 = tfiles[(3*quart):]\n",
    "print len(tfiles), quart, (len(train1)+len(train2)+len(train3)+len(train4))\n",
    "trains = [train1, train2, train3, train4]\n",
    "p = Pool(4)\n",
    "p.map(extract_byte_image_features, trains)\n",
    "print(\"Elapsed time: {:.2f} hours.\".format((tm.time() - start_time)/3600.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TEST FILES ASM\n",
    "# Now divide the test files into four groups for multiprocessing\n",
    "start_time = tm.time()\n",
    "ext_drive = '/opt/kaggle/test/'\n",
    "tfiles = os.listdir(ext_drive)\n",
    "quart = len(tfiles)/4\n",
    "test1 = tfiles[:quart]\n",
    "test2 = tfiles[quart:(2*quart)]\n",
    "test3 = tfiles[(2*quart):(3*quart)]\n",
    "test4 = tfiles[(3*quart):]\n",
    "print len(tfiles), quart, (len(test1)+len(test2)+len(test3)+len(test4))\n",
    "tests = [test1, test2, test3, test4]\n",
    "p = Pool(4)\n",
    "p.map(extract_asm_image_features, tests)\n",
    "print(\"Elapsed time: {:.2f} hours.\".format((tm.time() - start_time)/3600.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TEST FILES BYTE\n",
    "# Now divide the test files into four groups for multiprocessing\n",
    "start_time = tm.time()\n",
    "ext_drive = '/opt/kaggle/test/'\n",
    "tfiles = os.listdir(ext_drive)\n",
    "quart = len(tfiles)/4\n",
    "test1 = tfiles[:quart]\n",
    "test2 = tfiles[quart:(2*quart)]\n",
    "test3 = tfiles[(2*quart):(3*quart)]\n",
    "test4 = tfiles[(3*quart):]\n",
    "print len(tfiles), quart, (len(test1)+len(test2)+len(test3)+len(test4))\n",
    "tests = [test1, test2, test3, test4]\n",
    "p = Pool(4)\n",
    "p.map(extract_byte_image_features, tests)\n",
    "print(\"Elapsed time: {:.2f} hours.\".format((tm.time() - start_time)/3600.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## CODE TESTING BELOW ONLY:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_entropy(tfiles):\n",
    "  byte_files = [i for i in tfiles if '.bytes' in i]\n",
    "  for fname in byte_files:\n",
    "    f = open(ext_drive + fname, 'r')\n",
    "    filesize = os.path.getsize(ext_drive + fname)\n",
    "    lines = f.readlines()\n",
    "    entropy = entropy_counter(lines)\n",
    "    print(fname + ' : entropy = ' + str(entropy) + ' file size = ' + str(filesize))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# this is just a 1gram counter that calls calculate_entropy when it is finished counting bytes\n",
    "# ONLY USE FOR XXD format files\n",
    "def entropy_counter_xxd(byte_code):\n",
    "  byte_counts = [0] * 256\n",
    "  total = 0\n",
    "  #code_length = len(byte_code)\n",
    "  for row in byte_code:\n",
    "    bytes = row.split(' ')\n",
    "    # skip first and last tokens, also xxd puts two spaces before the last column so subtract 2 to skip\n",
    "    # print(bytes) \n",
    "    # codes = row[:-2].split()[1:]\n",
    "    for i in range(1, 9):\n",
    "      #   print(bytes[i])\n",
    "      hexword = bytes[i]\n",
    "      highbyte = hexword[0:2]\n",
    "      lowbyte = hexword[2:]\n",
    "      binary_val = int(highbyte, 16)\n",
    "      byte_counts[binary_val] += 1\n",
    "      binary_val = int(lowbyte, 16)\n",
    "      byte_counts[binary_val] += 1\n",
    "      total += 2\n",
    "        \n",
    "  entropy = calculate_entropy(byte_counts, total)\n",
    "\n",
    "  return entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2  is not equal to  3\n"
     ]
    }
   ],
   "source": [
    "num1 = 2\n",
    "num2 = 3\n",
    "print(num1, \" is not equal to \", num2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0A32eTdBKayjCWhZqDOQ.bytes : entropy = 0.7799095070146072 file size = 4356052\n",
      "0ACDbR5M3ZhBJajygTuf.bytes : entropy = 0.4677855747448026 file size = 5731328\n"
     ]
    }
   ],
   "source": [
    "tfiles = os.listdir('/temp/')\n",
    "byte_files = [i for i in tfiles if '.bytes' in i]\n",
    "for fname in byte_files:\n",
    "    f = open('/temp/' + fname, 'r')\n",
    "    filesize = os.path.getsize('/temp/' + fname)\n",
    "    lines = f.readlines()\n",
    "    entropy = entropy_counter(lines)\n",
    "    print(fname + ' : entropy = ' + str(entropy) + ' file size = ' + str(filesize))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "downloader.bytes : entropy = 0.36326625172661214 file size = 4288\n",
      "fu.bytes : entropy = 0.7363563437480696 file size = 411648\n",
      "agobot.bytes : entropy = 0.9887759997811357 file size = 857600\n"
     ]
    }
   ],
   "source": [
    "tfiles = os.listdir('/temp/')\n",
    "byte_files = [i for i in tfiles if '.bytes' in i]\n",
    "for fname in byte_files:\n",
    "    f = open('/temp/' + fname, 'r')\n",
    "    filesize = os.path.getsize('/temp/' + fname)\n",
    "    lines = f.readlines()\n",
    "    entropy = entropy_counter(lines)\n",
    "    print(fname + ' : entropy = ' + str(entropy) + ' file size = ' + str(filesize))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method_descriptor:\n",
      "\n",
      "split(...)\n",
      "    S.split(sep=None, maxsplit=-1) -> list of strings\n",
      "    \n",
      "    Return a list of the words in S, using sep as the\n",
      "    delimiter string.  If maxsplit is given, at most maxsplit\n",
      "    splits are done. If sep is not specified or is None, any\n",
      "    whitespace string is a separator and empty strings are\n",
      "    removed from the result.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(str.split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pid = os.getpid()\n",
    "print 'Process id:', pid \n",
    "feature_file = 'data/' + str(pid) + '-malware-features-asm.csv' # Windows API, symbols, registers, opcodes, defines etc...   \n",
    "print 'feature file:', feature_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#alternative separation method\n",
    "ext_drive = '/opt/kaggle/test/'\n",
    "tfiles = os.listdir(ext_drive)\n",
    "quart = len(tfiles)/4\n",
    "train1 = tfiles[:quart]\n",
    "train2 = tfiles[quart:(2*quart)]\n",
    "train3 = tfiles[(2*quart):(3*quart)]\n",
    "train4 = tfiles[(3*quart):]\n",
    "print len(tfiles), quart, (len(train1)+len(train2)+len(train3)+len(train4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ext_drive = '/opt/kaggle/malware/'\n",
    "train_paths = ['train1', 'train2', 'train3', 'train4']\n",
    "feature_file = ext_drive + 'data/' + train_paths[2] + '-malware-features-asm.csv'\n",
    "print feature_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "column_names = ['filename'] + [ (\"ASM_{:s}\".format(str(x))) for x in range(1000)]\n",
    "print(column_names)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
