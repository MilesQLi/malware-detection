{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Various data validation and cleaning functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Validate PE/COFF Disassembly.\n",
    "    - feature_extraction_validation.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from multiprocessing import Pool\n",
    "import os\n",
    "from csv import writer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import scipy.misc\n",
    "import array\n",
    "import time as tm\n",
    "import re\n",
    "import subprocess as sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def validate_disassembly(asm_path, hdr_path, file_ext): \n",
    "    # Check disassembly results for the PE/COFF files in the malware set.\n",
    "\n",
    "    t1asm = os.listdir(asm_path)\n",
    "    t1hdr = os.listdir(hdr_path)\n",
    "    asm_files = []\n",
    "    hdr_files = []\n",
    "\n",
    "    for fname in t1asm:\n",
    "        if fname.endswith('.pe.asm'):\n",
    "            asm_files.append(fname)\n",
    "\n",
    "    for fname in t1hdr:\n",
    "        if fname.endswith('.pe.txt'):\n",
    "            hdr_files.append(fname)\n",
    "\n",
    "    print(\"asm dir: {:d} asm files {:d} hdr dir {:d} hdr files {:d}\".format(len(t1asm),len(asm_files),len(t1hdr),len(hdr_files)))\n",
    "    \n",
    "    counter = 0\n",
    "    missing_hdr_list = []\n",
    "\n",
    "    for fname in asm_files:\n",
    "        hdr_name = fname.replace('.asm', '.txt')\n",
    "        if hdr_name not in hdr_files:\n",
    "            print(\"{:s} not in header file list.\".format(hdr_name))\n",
    "            counter += 1\n",
    "            missing_hdr_list.append(hdr_name)\n",
    "\n",
    "    print(\"{:d} missing header files.\".format(counter))\n",
    " \n",
    "    counter = 0\n",
    "    missing_asm_list = []\n",
    "\n",
    "    for fname in hdr_files:\n",
    "        asm_name = fname.replace('.txt','.asm')\n",
    "        if asm_name not in asm_files:\n",
    "            print(\"{:s} not in asm file list.\".format(asm_name))\n",
    "            counter += 1\n",
    "            missing_asm_list.append(asm_name)\n",
    "\n",
    "    print(\"{:d} missing assembly files.\".format(counter))\n",
    "\n",
    "    if len(missing_asm_list) > 0:\n",
    "        counter = 0\n",
    "        fop = open('data/temp-disass-missing-asm-files' + file_ext + '.txt', 'w')\n",
    "        for fname in missing_asm_list:\n",
    "            fop.write(fname + \"\\n\")\n",
    "            counter += 1\n",
    "\n",
    "        fop.close()\n",
    "        print(\"Wrote {:d} missing asm file names.\".format(counter))\n",
    "\n",
    "    if len(missing_hdr_list) > 0:\n",
    "        counter = 0\n",
    "        fop = open('data/temp-disass-missing-hdr-files' + file_ext + '.txt', 'w')\n",
    "        for fname in missing_hdr_list:\n",
    "            fop.write(fname + \"\\n\")\n",
    "            counter += 1\n",
    "\n",
    "        fop.close()\n",
    "        print(\"Wrote {:d} missing hdr file names.\".format(counter))\n",
    "        \n",
    "    counter = 0\n",
    "    bad_asm_list = []\n",
    "\n",
    "    for fname in asm_files:\n",
    "        fsize = os.path.getsize(asm_path + fname)\n",
    "        if fsize < 1000:\n",
    "            print(\"{:s} bad output, filesize = {:d}.\".format(fname, fsize))\n",
    "            counter += 1\n",
    "            bad_asm_list.append(fname)\n",
    "\n",
    "    print(\"{:d} bad asm files.\".format(counter))\n",
    "\n",
    "    counter = 0\n",
    "    bad_hdr_list = []\n",
    "\n",
    "    for fname in hdr_files:\n",
    "        fsize = os.path.getsize(hdr_path + fname)\n",
    "        if fsize < 1000:\n",
    "            print(\"{:s} bad output, filesize = {:d}.\".format(fname, fsize))\n",
    "            counter += 1\n",
    "            bad_hdr_list.append(fname)\n",
    "\n",
    "    print(\"{:d} bad header files.\".format(counter))\n",
    "\n",
    "    if len(bad_hdr_list) > 0:\n",
    "        counter = 0\n",
    "        fop = open('data/temp-disass-bad-hdr-files' + file_ext + '.txt', 'w')\n",
    "        for fname in bad_hdr_list:\n",
    "            fop.write(fname + \"\\n\")\n",
    "            counter += 1\n",
    "\n",
    "        fop.close()\n",
    "        \n",
    "    print(\"Wrote {:d} bad hdr file names.\".format(counter))\n",
    "\n",
    "    if len(bad_asm_list) > 0:\n",
    "        counter = 0\n",
    "        fop = open('data/temp-disass-bad-asm-files' + file_ext + '.txt', 'w')\n",
    "        for fname in bad_asm_list:\n",
    "            fop.write(fname + \"\\n\")\n",
    "            counter += 1\n",
    "\n",
    "        fop.close()\n",
    "        \n",
    "    print(\"Wrote {:d} bad asm file names.\".format(counter))\n",
    "    \n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "validate_disassembly('/opt/vs/train1asm/', '/opt/vs/train1hdr/', '-vs251')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "validate_disassembly('/opt/vs/train2asm/', '/opt/vs/train2hdr/', '-vs252')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "validate_disassembly('/opt/vs/train3asm/', '/opt/vs/train3hdr/', '-vs263')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "validate_disassembly('/opt/vs/train4asm/', '/opt/vs/train4hdr/', '-vs264')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "asm dir: 271 asm files 271 hdr dir 275 hdr files 275\n",
      "0 missing header files.\n",
      "VirusShare_d8b7b276710127d233abcdb7313aac36.pe.asm not in asm file list.\n",
      "VirusShare_d4ba6430996fb4021241efc97c607504.pe.asm not in asm file list.\n",
      "VirusShare_af719814507fdca4b96184f33b6b92ea.pe.asm not in asm file list.\n",
      "VirusShare_6a4fbcfb44717eae2145c761c1c99b6a.pe.asm not in asm file list.\n",
      "4 missing assembly files.\n",
      "Wrote 4 missing asm file names.\n",
      "0 bad asm files.\n",
      "0 bad header files.\n",
      "Wrote 0 bad hdr file names.\n",
      "Wrote 0 bad asm file names.\n"
     ]
    }
   ],
   "source": [
    "validate_disassembly('/opt/vs/aptasm/', '/opt/vs/apthdr/', '-vsapt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 1. Run the validate_disassembly function to identify missing and incomplete ASM and Header files.\n",
    "# 2. List the missing and incomplete files and retry disassembly.\n",
    "# 3. If retry stills fails manually analyse the culprit file to determine cause of error.\n",
    "# 4. Run the feature extraction processes again on the ASM and Header files.\n",
    "# 5. Run the validate_features function to identify missing or incomplete features for each sample.\n",
    "# 6. Manually analyse files that have missing or incomplete feature sets to determine the cause of the errors.\n",
    "# 7. TODO:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyse Disassembly Errors and Attempt to Fix Them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Open the bad or missing ASM file list and copy the the binaries\n",
    "# to another directory for manual analysis.\n",
    "\n",
    "def copy_bad_pe(in_file, ext_dir, err_dir):\n",
    "    \n",
    "    if os.path.isfile(in_file):\n",
    "        \n",
    "        fip = open(in_file, 'r')\n",
    "        in_lines = fip.readlines()\n",
    "        fip.close()\n",
    "\n",
    "        print(\"Got {:d} file names.\".format(len(in_lines)))\n",
    "        counter = 0\n",
    "\n",
    "        if len(in_lines) > 0:\n",
    "            for line in in_lines:\n",
    "                line = line.rstrip()\n",
    "                if line.endswith('.pe.asm'):\n",
    "                    fname = line[0:line.find('.pe.asm')]\n",
    "                    print(\"Copying file: {:s}\".format(fname))\n",
    "                    sub.call([\"cp\", ext_dir + fname, err_dir + fname])\n",
    "                    counter += 1\n",
    "\n",
    "        print(\"Completed copyinging {:d} files.\".format(counter))\n",
    "    \n",
    "    else:\n",
    "        print(\"{:s} input file not found.\".format(in_file))\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "copy_bad_pe('data/temp-disass-missing-asm-files-vs251.txt', '/opt/vs/train1/', '/opt/vs/train1err/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "copy_bad_pe('data/temp-disass-missing-asm-files-vs252.txt', '/opt/vs/train2/', '/opt/vs/train2err/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "copy_bad_pe('data/temp-disass-missing-asm-files-vs263.txt', '/opt/vs/train3/', '/opt/vs/train3err/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "copy_bad_pe('data/temp-disass-missing-asm-files-vs264.txt', '/opt/vs/train4/', '/opt/vs/train4err/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "copy_bad_pe('data/temp-disass-bad-asm-files-vs251.txt', '/opt/vs/train1/', '/opt/vs/train1err/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "copy_bad_pe('data/temp-disass-bad-asm-files-vs252.txt', '/opt/vs/train2/', '/opt/vs/train2err/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "copy_bad_pe('data/temp-disass-bad-asm-files-vs263.txt', '/opt/vs/train3/', '/opt/vs/train3err/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "copy_bad_pe('data/temp-disass-bad-asm-files-vs264.txt', '/opt/vs/train4/', '/opt/vs/train4err/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def copy_bad_pe_header(in_file, ext_dir, err_dir):\n",
    "\n",
    "    if os.path.isfile(in_file):\n",
    "        \n",
    "        fip = open(in_file, 'r')\n",
    "        in_lines = fip.readlines()\n",
    "        fip.close()\n",
    "\n",
    "        print(\"Got {:d} file names.\".format(len(in_lines)))\n",
    "        counter = 0\n",
    "\n",
    "        if len(in_lines) > 0:\n",
    "            for line in in_lines:\n",
    "                hdr_file_name = line.rstrip()\n",
    "                if hdr_file_name.endswith('.pe.txt'):\n",
    "                    fname = line[0:line.find('.pe.txt')]\n",
    "                    print(\"Copying PE file: {:s}\".format(fname))\n",
    "                    sub.call([\"cp\", ext_dir + fname, err_dir + fname])\n",
    "                    counter += 1\n",
    "\n",
    "        print(\"Completed copying {:d} PE files.\".format(counter))\n",
    "    \n",
    "    else:\n",
    "        print(\"{:s} input file not found.\".format(in_file))\n",
    "        \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 9 file names.\n",
      "Copying PE file: VirusShare_7dc02b8661b9cd6311943701c90aef4e\n",
      "Copying PE file: VirusShare_adbcb556aabf27758191f3be3f466c36\n",
      "Copying PE file: VirusShare_787d3645c5b5393984e7557daa389249\n",
      "Copying PE file: VirusShare_36cb828738111e0580a28607c713fcc7\n",
      "Copying PE file: VirusShare_592d7ac775519110d58e9ce1975c1b5b\n",
      "Copying PE file: VirusShare_4a0c79f6ad27b0a674b08005d102e16d\n",
      "Copying PE file: VirusShare_7e681c6b0488c8533389660c86a70982\n",
      "Copying PE file: VirusShare_d5eff38b212286c46db007aa7159ffd8\n",
      "Copying PE file: VirusShare_c80d9b2dbf9b7953a3b6e9b51a39a0c2\n",
      "Completed copying 9 PE files.\n"
     ]
    }
   ],
   "source": [
    "copy_bad_pe_header('data/temp-disass-bad-hdr-files-vs251.txt', '/opt/vs/train1/', '/opt/vs/train1err/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 12 file names.\n",
      "Copying PE file: VirusShare_e8c51dae6396d78e1c42c735f99c24e6\n",
      "Copying PE file: VirusShare_f7e2f4f676454287ac2a7ec1fa941f00\n",
      "Copying PE file: VirusShare_76cad0f51839af82a2c55270b3e27981\n",
      "Copying PE file: VirusShare_c34c0c753fdca67a21674dfc7820fa71\n",
      "Copying PE file: VirusShare_8c0ea62a8a791d81a7441e835b3320a4\n",
      "Copying PE file: VirusShare_5d6417a3dc81e53127c50dfad1572252\n",
      "Copying PE file: VirusShare_3eae26d3da9c58ee9519e23ef6ae5371\n",
      "Copying PE file: VirusShare_e18a2dbf74eb09df1518ec79aba01073\n",
      "Copying PE file: VirusShare_6c19c8e181dbaf6b50fe26322389459c\n",
      "Copying PE file: VirusShare_d5bf45ef758c093f3f15ed243882b105\n",
      "Copying PE file: VirusShare_3c5b41a6660c1f6e65bbeb136a91ecd3\n",
      "Copying PE file: VirusShare_857738eff74ce15405aac235a5e25577\n",
      "Completed copying 12 PE files.\n"
     ]
    }
   ],
   "source": [
    "copy_bad_pe_header('data/temp-disass-bad-hdr-files-vs252.txt', '/opt/vs/train2/', '/opt/vs/train2err/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "copy_bad_pe_header('data/temp-disass-bad-hdr-files-vs263.txt', '/opt/vs/train3/', '/opt/vs/train3err/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "copy_bad_pe_header('data/temp-disass-bad-hdr-files-vs264.txt', '/opt/vs/train4/', '/opt/vs/train4err/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/temp-disass-missing-hdr-files-vs251.txt input file not found.\n"
     ]
    }
   ],
   "source": [
    "copy_bad_pe_header('data/temp-disass-missing-hdr-files-vs251.txt', '/opt/vs/train1/', '/opt/vs/train1err/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/temp-disass-missing-hdr-files-vs252.txt input file not found.\n"
     ]
    }
   ],
   "source": [
    "copy_bad_pe_header('data/temp-disass-missing-hdr-files-vs252.txt', '/opt/vs/train2/', '/opt/vs/train2err/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "copy_bad_pe_header('data/temp-disass-missing-hdr-files-vs263.txt', '/opt/vs/train3/', '/opt/vs/train3err/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "copy_bad_pe_header('data/temp-disass-missing-hdr-files-vs264.txt', '/opt/vs/train4/', '/opt/vs/train4err/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Functions to fix ASM file rename stuff up for train3.\n",
    "\n",
    "def fix_file_names(ext_drive, file_list):\n",
    "    asm_list_file = open('data/temp-train3-asm-files.txt','w')\n",
    "    counter = 0\n",
    "    \n",
    "    for idx, file_name in enumerate(file_list):\n",
    "        if not file_name.endswith('.asm'):\n",
    "            continue\n",
    "            \n",
    "        file_path = ext_drive + file_name\n",
    "        \n",
    "        signat = sub.check_output([\"file\", file_path])\n",
    "        signat = signat.replace(',','').rstrip() # get rid of newlines and commas they are annoying\n",
    "        #print(\"File type: {:s}\".format(signat))\n",
    "            \n",
    "        if 'data' in signat or 'text' in signat:\n",
    "            fip = open(file_path, 'r')\n",
    "            in_lines = fip.readlines()\n",
    "            fip.close()\n",
    "            \n",
    "            for idx, line in enumerate(in_lines):\n",
    "                if 'Hex-Rays' in line:\n",
    "                    asm_list_file.write(file_path + \"\\n\")\n",
    "                    counter += 1\n",
    "                    if counter % 1000 == 0:\n",
    "                        print(\"{:d} IDA Pro ASM File: {:s}\".format(counter, line))\n",
    "                        \n",
    "                    break\n",
    "                    \n",
    "                if idx > 10:\n",
    "                    break\n",
    "                    \n",
    "    asm_list_file.close()\n",
    "    \n",
    "    print(\"Found {:d} IDA Pro ASM files.\".format(counter))\n",
    "    \n",
    "    return\n",
    "\n",
    "\n",
    "def move_asm_files(in_file):\n",
    "    asm_file_list = open(in_file,'r')\n",
    "    file_list = asm_file_list.readlines()\n",
    "    asm_file_list.close()\n",
    "    \n",
    "    counter = 0\n",
    "    \n",
    "    print(\"Found {:d} files.\".format(len(file_list)))\n",
    "    \n",
    "    for line in file_list:\n",
    "        file_name = line.rstrip()\n",
    "        signat = sub.check_output([\"mv\", file_name, file_name + \".bak\"])\n",
    "        counter += 1\n",
    "        \n",
    "        if counter % 1000 == 0:\n",
    "            print(\"{:d} Moved file: {:s}\".format(counter, file_name))\n",
    "\n",
    "\n",
    "    \n",
    "    print(\"Moved {:d} files.\".format(counter))\n",
    "    \n",
    "    return\n",
    "\n",
    "\n",
    "def revert_asm_files(ext_dir, file_list):\n",
    "    \n",
    "    counter = 0\n",
    "    \n",
    "    print(\"Found {:d} files.\".format(len(file_list)))\n",
    "    \n",
    "    for fname in file_list:\n",
    "        if fname.endswith('.bak'):\n",
    "            new_file_path = ext_dir + fname[0:fname.find('.bak')]\n",
    "            file_path = ext_dir + fname\n",
    "        \n",
    "            signat = sub.check_output([\"mv\", file_path, new_file_path])\n",
    "            \n",
    "            counter += 1\n",
    "            \n",
    "            if counter % 1000 == 0:\n",
    "                print(\"{:d} Moved file: {:s}\".format(counter, new_file_path))\n",
    "        \n",
    "    \n",
    "    print(\"Reverted {:d} files.\".format(counter))\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ext_drive = '/opt/vs/train3asm/'\n",
    "tfiles = os.listdir(ext_drive)\n",
    "revert_asm_files(ext_drive, tfiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "move_asm_files('data/temp-train3-asm-files.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#ext_drive = '/opt/vs/apt/'\n",
    "ext_drive = '/opt/vs/train3/'\n",
    "tfiles = os.listdir(ext_drive)\n",
    "fix_file_names(ext_drive, tfiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#ext_drive = '/opt/vs/apt/'\n",
    "ext_drive = '/opt/vs/asm/'\n",
    "tfiles = os.listdir(ext_drive)\n",
    "fix_file_names(tfiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ext_drive = '/opt/vs/hdr/'\n",
    "tfiles = os.listdir(ext_drive)\n",
    "fix_file_names(tfiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rename_asm_files_fix(ext_dir):\n",
    "    # Rename all the PE headers files so it is easier to process them.\n",
    "    \n",
    "    file_list = os.listdir(ext_dir)\n",
    "    pe_counter = 0\n",
    "    unpe_counter = 0\n",
    "    \n",
    "    print(\"Got total files: {:d}\".format(len(file_list)))\n",
    "    \n",
    "    for fname in file_list:\n",
    "        if fname.endswith('.pe.asm'):\n",
    "            pe_counter += 1\n",
    "        elif fname.endswith('.asm'):\n",
    "            file_path = ext_dir + fname\n",
    "            trunc_name = fname[0:fname.find('.asm')]\n",
    "            new_path = ext_dir + trunc_name + '.pe.asm'\n",
    "            result = sub.check_call(['mv', file_path, new_path])\n",
    "            unpe_counter += 1\n",
    "\n",
    "            if (unpe_counter % 1000) == 0:\n",
    "                print('Renamed {:d} ASM files.'.format(unpe_counter))\n",
    "\n",
    "    print('Completed move of {:d} ASM files with {:d} files already renamed.'.format(unpe_counter, pe_counter))\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got total files: 14366\n",
      "Renamed 1000 ASM files.\n",
      "Renamed 2000 ASM files.\n",
      "Renamed 3000 ASM files.\n",
      "Renamed 4000 ASM files.\n",
      "Renamed 5000 ASM files.\n",
      "Renamed 6000 ASM files.\n",
      "Renamed 7000 ASM files.\n",
      "Renamed 8000 ASM files.\n",
      "Renamed 9000 ASM files.\n",
      "Renamed 10000 ASM files.\n",
      "Renamed 11000 ASM files.\n",
      "Renamed 12000 ASM files.\n",
      "Renamed 13000 ASM files.\n",
      "Renamed 14000 ASM files.\n",
      "Completed move of 14366 ASM files with 0 files already renamed.\n"
     ]
    }
   ],
   "source": [
    "rename_asm_files_fix('/opt/vs/train4asm/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got total files: 40980\n",
      "Renamed 1000 ASM files.\n",
      "Renamed 2000 ASM files.\n",
      "Renamed 3000 ASM files.\n",
      "Renamed 4000 ASM files.\n",
      "Renamed 5000 ASM files.\n",
      "Renamed 6000 ASM files.\n",
      "Renamed 7000 ASM files.\n",
      "Renamed 8000 ASM files.\n",
      "Renamed 9000 ASM files.\n",
      "Renamed 10000 ASM files.\n",
      "Renamed 11000 ASM files.\n",
      "Renamed 12000 ASM files.\n",
      "Renamed 13000 ASM files.\n",
      "Renamed 14000 ASM files.\n",
      "Renamed 15000 ASM files.\n",
      "Completed move of 15376 ASM files with 25603 files already renamed.\n"
     ]
    }
   ],
   "source": [
    "rename_asm_files_fix('/opt/vs/train3asm/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40979"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "25603 + 15376"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rename_binary_files_fix(ext_dir):\n",
    "    \n",
    "    file_list = os.listdir(ext_dir)\n",
    "    pe_counter = 0\n",
    "    \n",
    "    print(\"Got total files: {:d}\".format(len(file_list)))\n",
    "    \n",
    "    for fname in file_list:\n",
    "        if fname.endswith('.pe.asm'):\n",
    "            pe_counter += 1\n",
    "            file_path = ext_dir + fname\n",
    "            trunc_name = fname[0:fname.find('.pe.asm')]\n",
    "            new_path = ext_dir + trunc_name + '.bin'\n",
    "            result = sub.check_call(['mv', file_path, new_path])\n",
    "            \n",
    "            if (pe_counter % 1000) == 0:\n",
    "                print('Renamed {:d} binary files.'.format(pe_counter))\n",
    "\n",
    "    print('Completed rename of {:d} binary files.'.format(pe_counter))\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rename_binary_files_fix('/opt/vs/train3/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train3_binary_files_fix(ext_dir):\n",
    "    \n",
    "    file_list = os.listdir(ext_dir)\n",
    "    counter = 0\n",
    "    name_match_counter = 0\n",
    "    file_id_file = 'data/sorted-file-id-features-vs263.csv'\n",
    "    file_id_features = pd.read_csv(file_id_file)\n",
    "    file_names_list = file_id_features['file_name']\n",
    "    \n",
    "    print(\"Got total files: {:d}\".format(len(file_list)))\n",
    "    \n",
    "    for fname in file_list:\n",
    "        tokens = fname.split('_')\n",
    "        if len(tokens) < 2:\n",
    "            continue\n",
    "            \n",
    "        if fname.endswith('.bin'):\n",
    "            counter += 1\n",
    "            file_path = ext_dir + fname\n",
    "            \n",
    "            trunc_name = tokens[1]\n",
    "            trunc_name = trunc_name[0:trunc_name.find('.bin')]\n",
    "            \n",
    "            \n",
    "            # Now lookup the correct hash value in the file id database\n",
    "            # and generate the correct file name.\n",
    "            for hash_name in file_names_list:\n",
    "                short_hash = hash_name[0:-1]\n",
    "                if trunc_name == short_hash:\n",
    "                    new_path = ext_dir + 'VirusShare_' + hash_name\n",
    "                    name_match_counter += 1\n",
    "                    result = sub.call(['mv', file_path, new_path])\n",
    "            \n",
    "            if (counter % 1000) == 0:\n",
    "                print('Renamed {:d} binary files {:s}'.format(counter, new_path))\n",
    "                print('Match {:d} {:s}'.format(name_match_counter, fname))\n",
    "\n",
    "    print('Completed rename of {:d} binary files with {:d} name matches.'.format(counter, name_match_counter))\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train3_binary_files_fix('/opt/vs/train3/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "help(os.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examine IDA Pro ASM Disassembly (call/int) Formats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Test Call Graph Generation.\n",
    "\n",
    "call_opcodes = ['call','int']\n",
    "call_blocks = ['sub_']\n",
    "\n",
    "def construct_call_graph(lines):\n",
    "    vertex = '.program_entry_point' # this is the root node, corresponds to the program original entry point not C main().\n",
    "    vertex_count = 1\n",
    "    edge_count = 0\n",
    "    cfgraph = gra.Graph()\n",
    "    cfgraph.add_vertex(vertex)\n",
    "    \n",
    "    for row in lines:\n",
    "        row = row.rstrip('\\r\\n')  # get rid of newlines they are annoying.\n",
    "        if row.startswith(';'):\n",
    "            continue\n",
    "            \n",
    "        if ';' in row:\n",
    "            row = row.split(';')[0] # get rid of comments they are annoying.\n",
    "            #print(row)\n",
    "      \n",
    "        # get rid of all these things they are annoying.\n",
    "        row = row.replace('short','').replace('ds:',' ')\n",
    "        row = row.replace('dword','').replace('near','').replace('far','')\n",
    "        row = row.replace('ptr','').replace(':',' ').replace(',',' ')\n",
    "        row = row.replace('@','').replace('?','')\n",
    "        parts = row.split() # tokenize code line\n",
    "        \n",
    "        if (len(parts) < 2): # this is just a comment line\n",
    "            continue\n",
    "        \n",
    "        if (parts[3] == 'endp'): # ignore subroutine end labels\n",
    "            continue\n",
    "        \n",
    "        # check for subroutines and block labels\n",
    "        # block and subroutine labels are always after the .text HHHHHHHH relative address\n",
    "        for block in call_blocks:\n",
    "            token = parts[2]  \n",
    "            idx = token.find(block)\n",
    "            if ((idx == 0) or (parts[3] == 'proc')):\n",
    "                # add new vertex to the graph, we are now in a new subroutine\n",
    "                vertex = token\n",
    "                cfgraph.add_vertex(vertex)\n",
    "                # print(\"Vertex: \" + vertex)\n",
    "                vertex_count += 1\n",
    "                break\n",
    "\n",
    "        # now check for edge opcode    \n",
    "        for opcode in call_opcodes: # check the line for a new edge\n",
    "            if opcode in parts:\n",
    "                # Extract desination address/function name/interrupt number as the directed edge.\n",
    "                idx = parts.index(opcode)\n",
    "                edge_count += 1\n",
    "                if ((idx + 1) < len(parts)): # in a few ASM files there is no operand, disassembly error?\n",
    "                    next_vertex = parts[idx + 1]\n",
    "                else:\n",
    "                    next_vertex = \"none\"\n",
    "                cfgraph.add_edge(vertex, next_vertex)\n",
    "                # print(\"Edge: \" + vertex + \" \" + parts[idx] + \" \" + edge)\n",
    "                break\n",
    "\n",
    "    # print(\"Vertex Count: {:d}\".format(vertex_count))\n",
    "    \n",
    "    return cfgraph\n",
    "\n",
    "\n",
    "def extract_call_graphs(multi_params):\n",
    "    asm_files = multi_params.file_list\n",
    "    ftot = len(asm_files)\n",
    "    ext_drive = multi_params.ext_drive\n",
    "    \n",
    "    pid = os.getpid()\n",
    "    feature_file = 'data/' + str(pid) + \"-\" + multi_params.feature_file \n",
    "    \n",
    "    print('Process ID: {:d} Graph Feature file: {:s}'.format(pid, feature_file))\n",
    "    \n",
    "    graph_lines = []\n",
    "    graph_features = []\n",
    "    graph_file = open('data/' + str(pid) + \"-\" + multi_params.graph_file, 'w') # write as a graphviz DOT format file\n",
    "    \n",
    "    with open(feature_file, 'w') as f:\n",
    "        # write the column names for the csv file\n",
    "        fw = writer(f)\n",
    "        #colnames = ['filename','vertex_count','edge_count','delta_max','density','diameter']\n",
    "        #colnames = ['file_name','vertex_count','edge_count','delta_max','density']\n",
    "        #fw.writerow(colnames) put in combine_feature_files\n",
    "        \n",
    "        # Now iterate through the file list and extract the call graph from each file.\n",
    "        for idx, fname in enumerate(asm_files):\n",
    "            fasm = open(ext_drive + fname, 'r', errors='ignore')\n",
    "            lines = fasm.readlines()\n",
    "            fasm.close()\n",
    "            \n",
    "            call_graph = construct_call_graph(lines)\n",
    "            cgvc = call_graph.n_vertices()\n",
    "            cgec = call_graph.n_edges()\n",
    "            cgdm = call_graph.delta_max()\n",
    "            cgde = call_graph.density()\n",
    "            # cdia = call_graph.diameter() this is constantly problematic !!!\n",
    "            \n",
    "            fname_parts = fname.split('_') # Truncate the file name to the hash value.\n",
    "            trunc_name = fname_parts[1]\n",
    "            trunc_name = trunc_name[:trunc_name.find('.pe.asm')]\n",
    "            \n",
    "            graph_features.append([trunc_name] + [cgvc, cgec, cgdm, cgde])\n",
    "            call_graph.set_graph_name(trunc_name)\n",
    "            #graph_lines.append(call_graph.to_str('multinoleaf')) \n",
    "            graph_lines.append(call_graph.to_str('graphviz'))\n",
    "            \n",
    "            del(call_graph) # for some reason new graphs get appended to the previous graphs if not deleted???\n",
    "            \n",
    "            # Print progress\n",
    "            if (idx + 1) % 100 == 0:\n",
    "                print(pid, idx + 1, 'of', ftot, 'files processed.')\n",
    "                fw.writerows(graph_features)\n",
    "                graph_file.writelines(graph_lines)\n",
    "                graph_features = []\n",
    "                graph_lines = []\n",
    "                \n",
    "        # Write remaining files\n",
    "        if len(graph_lines) > 0:\n",
    "            fw.writerows(graph_features)\n",
    "            graph_file.writelines(graph_lines)\n",
    "            graph_features = []\n",
    "            graph_lines = []\n",
    "\n",
    "    graph_file.close()\n",
    "    \n",
    "    print('Process ID: {:d} finished.'.format(pid))\n",
    "    \n",
    "    return\n",
    "\n",
    "\n",
    "class Multi_Params(object):\n",
    "    def __init__(self, featurefile=\"\", graphfile=\"\", extdrive=\"\", filelist=[]):\n",
    "        self.feature_file = featurefile\n",
    "        self.graph_file = graphfile\n",
    "        self.ext_drive = extdrive\n",
    "        self.file_list = filelist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_call_lines(file_list):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feature_file = 'sorted-pe-call-graph-features-apt.csv'\n",
    "graph_file = 'pe-call-graphs-apt.gv'\n",
    "ext_drive = '/opt/vs/aptasm/'\n",
    "file_ext = '-apt'\n",
    "\n",
    "file_list = os.listdir(ext_drive)\n",
    "tfiles = [i for i in file_list if '.pe.asm' in i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Validate PE Header Feature Values.\n",
    "    Picking up some negative values which stuffs up feature reduction in sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO:\n",
    "# Open pe header feature file.\n",
    "# Parse for \",-\" and replace with \",\".\n",
    "# Done.\n",
    "\n",
    "import os\n",
    "from csv import writer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import scipy.misc\n",
    "import array\n",
    "import time as tm\n",
    "import re\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_out_the_negatives(feature_file_name, cleansed_out_file_name):\n",
    "    # Cleanse the insidious negatives.\n",
    "    #fip = pd.read_csv(feature_file_name, na_filter=False)\n",
    "    fip = open(feature_file_name, 'r')\n",
    "    fop = open(cleansed_out_file_name, 'w')\n",
    "    in_lines = fip.readlines()\n",
    "    counter = 0\n",
    "    ltot = len(in_lines)\n",
    "    \n",
    "    for line in in_lines:\n",
    "        line = line.replace(\",-\",\",\")\n",
    "        fop.write(line)\n",
    "        counter += 1\n",
    "        \n",
    "        # Print the awsome progress\n",
    "        if (counter + 1) % 100 == 0:\n",
    "            print(counter, 'of', ltot, 'lines processed.')\n",
    "    \n",
    "    fop.close()\n",
    "    \n",
    "    print(\"Completed clean of file: {:s}\".format(feature_file_name))\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ext_drive = \"/opt/vs/\"\n",
    "feature_file = ext_drive + \"pe-header-features-vs263.csv\"\n",
    "clean_feature_file = ext_drive + \"pe-header-features-vs263-clean.csv\"\n",
    "clean_out_the_negatives(feature_file, clean_feature_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_out_elves_in_vs263(feature_file_name, cleansed_out_file_name):\n",
    "    # Cleanse the 5 insidious ELF samples in the PE function counts.\n",
    "    fip = open(feature_file_name, 'r')\n",
    "    fop = open(cleansed_out_file_name, 'w')\n",
    "    \n",
    "    counter = 0\n",
    "    elf_counter = 0    \n",
    "  \n",
    "    for line in fip:\n",
    "        if '.elf,' in line:\n",
    "            line = line.replace(\".elf\",\"\")\n",
    "            elf_counter += 1\n",
    "            \n",
    "        fop.write(line)\n",
    "        counter += 1\n",
    "        \n",
    "        # Print the awsome progress\n",
    "        if (counter + 1) % 100 == 0:\n",
    "            print(' {:d} lines processed.'.format(counter))\n",
    "    \n",
    "    fop.close()\n",
    "    fip.close()\n",
    "    \n",
    "    print(\"Completed clean of file: {:s} with {:d} elves removed.\".format(feature_file_name, elf_counter))\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 99 lines processed.\n",
      " 199 lines processed.\n",
      " 299 lines processed.\n",
      " 399 lines processed.\n",
      " 499 lines processed.\n",
      " 599 lines processed.\n",
      " 699 lines processed.\n",
      " 799 lines processed.\n",
      " 899 lines processed.\n",
      " 999 lines processed.\n",
      " 1099 lines processed.\n",
      " 1199 lines processed.\n",
      " 1299 lines processed.\n",
      " 1399 lines processed.\n",
      " 1499 lines processed.\n",
      " 1599 lines processed.\n",
      " 1699 lines processed.\n",
      " 1799 lines processed.\n",
      " 1899 lines processed.\n",
      " 1999 lines processed.\n",
      " 2099 lines processed.\n",
      " 2199 lines processed.\n",
      " 2299 lines processed.\n",
      " 2399 lines processed.\n",
      " 2499 lines processed.\n",
      " 2599 lines processed.\n",
      " 2699 lines processed.\n",
      " 2799 lines processed.\n",
      " 2899 lines processed.\n",
      " 2999 lines processed.\n",
      " 3099 lines processed.\n",
      " 3199 lines processed.\n",
      " 3299 lines processed.\n",
      " 3399 lines processed.\n",
      " 3499 lines processed.\n",
      " 3599 lines processed.\n",
      " 3699 lines processed.\n",
      " 3799 lines processed.\n",
      " 3899 lines processed.\n",
      " 3999 lines processed.\n",
      " 4099 lines processed.\n",
      " 4199 lines processed.\n",
      " 4299 lines processed.\n",
      " 4399 lines processed.\n",
      " 4499 lines processed.\n",
      " 4599 lines processed.\n",
      " 4699 lines processed.\n",
      " 4799 lines processed.\n",
      " 4899 lines processed.\n",
      " 4999 lines processed.\n",
      " 5099 lines processed.\n",
      " 5199 lines processed.\n",
      " 5299 lines processed.\n",
      " 5399 lines processed.\n",
      " 5499 lines processed.\n",
      " 5599 lines processed.\n",
      " 5699 lines processed.\n",
      " 5799 lines processed.\n",
      " 5899 lines processed.\n",
      " 5999 lines processed.\n",
      " 6099 lines processed.\n",
      " 6199 lines processed.\n",
      " 6299 lines processed.\n",
      " 6399 lines processed.\n",
      " 6499 lines processed.\n",
      " 6599 lines processed.\n",
      " 6699 lines processed.\n",
      " 6799 lines processed.\n",
      " 6899 lines processed.\n",
      " 6999 lines processed.\n",
      " 7099 lines processed.\n",
      " 7199 lines processed.\n",
      " 7299 lines processed.\n",
      " 7399 lines processed.\n",
      " 7499 lines processed.\n",
      " 7599 lines processed.\n",
      " 7699 lines processed.\n",
      " 7799 lines processed.\n",
      " 7899 lines processed.\n",
      " 7999 lines processed.\n",
      " 8099 lines processed.\n",
      " 8199 lines processed.\n",
      " 8299 lines processed.\n",
      " 8399 lines processed.\n",
      " 8499 lines processed.\n",
      " 8599 lines processed.\n",
      " 8699 lines processed.\n",
      " 8799 lines processed.\n",
      " 8899 lines processed.\n",
      " 8999 lines processed.\n",
      " 9099 lines processed.\n",
      " 9199 lines processed.\n",
      " 9299 lines processed.\n",
      " 9399 lines processed.\n",
      " 9499 lines processed.\n",
      " 9599 lines processed.\n",
      " 9699 lines processed.\n",
      " 9799 lines processed.\n",
      " 9899 lines processed.\n",
      " 9999 lines processed.\n",
      " 10099 lines processed.\n",
      " 10199 lines processed.\n",
      " 10299 lines processed.\n",
      " 10399 lines processed.\n",
      " 10499 lines processed.\n",
      " 10599 lines processed.\n",
      " 10699 lines processed.\n",
      " 10799 lines processed.\n",
      " 10899 lines processed.\n",
      " 10999 lines processed.\n",
      " 11099 lines processed.\n",
      " 11199 lines processed.\n",
      " 11299 lines processed.\n",
      " 11399 lines processed.\n",
      " 11499 lines processed.\n",
      " 11599 lines processed.\n",
      " 11699 lines processed.\n",
      " 11799 lines processed.\n",
      " 11899 lines processed.\n",
      " 11999 lines processed.\n",
      " 12099 lines processed.\n",
      " 12199 lines processed.\n",
      " 12299 lines processed.\n",
      " 12399 lines processed.\n",
      " 12499 lines processed.\n",
      " 12599 lines processed.\n",
      " 12699 lines processed.\n",
      " 12799 lines processed.\n",
      " 12899 lines processed.\n",
      " 12999 lines processed.\n",
      " 13099 lines processed.\n",
      " 13199 lines processed.\n",
      " 13299 lines processed.\n",
      " 13399 lines processed.\n",
      " 13499 lines processed.\n",
      " 13599 lines processed.\n",
      " 13699 lines processed.\n",
      " 13799 lines processed.\n",
      " 13899 lines processed.\n",
      " 13999 lines processed.\n",
      " 14099 lines processed.\n",
      " 14199 lines processed.\n",
      " 14299 lines processed.\n",
      " 14399 lines processed.\n",
      " 14499 lines processed.\n",
      " 14599 lines processed.\n",
      " 14699 lines processed.\n",
      " 14799 lines processed.\n",
      " 14899 lines processed.\n",
      " 14999 lines processed.\n",
      " 15099 lines processed.\n",
      " 15199 lines processed.\n",
      " 15299 lines processed.\n",
      " 15399 lines processed.\n",
      " 15499 lines processed.\n",
      " 15599 lines processed.\n",
      " 15699 lines processed.\n",
      " 15799 lines processed.\n",
      " 15899 lines processed.\n",
      " 15999 lines processed.\n",
      " 16099 lines processed.\n",
      " 16199 lines processed.\n",
      " 16299 lines processed.\n",
      " 16399 lines processed.\n",
      " 16499 lines processed.\n",
      " 16599 lines processed.\n",
      " 16699 lines processed.\n",
      " 16799 lines processed.\n",
      " 16899 lines processed.\n",
      " 16999 lines processed.\n",
      " 17099 lines processed.\n",
      " 17199 lines processed.\n",
      " 17299 lines processed.\n",
      " 17399 lines processed.\n",
      " 17499 lines processed.\n",
      " 17599 lines processed.\n",
      " 17699 lines processed.\n",
      " 17799 lines processed.\n",
      " 17899 lines processed.\n",
      " 17999 lines processed.\n",
      " 18099 lines processed.\n",
      " 18199 lines processed.\n",
      " 18299 lines processed.\n",
      " 18399 lines processed.\n",
      " 18499 lines processed.\n",
      " 18599 lines processed.\n",
      " 18699 lines processed.\n",
      " 18799 lines processed.\n",
      " 18899 lines processed.\n",
      " 18999 lines processed.\n",
      " 19099 lines processed.\n",
      " 19199 lines processed.\n",
      " 19299 lines processed.\n",
      " 19399 lines processed.\n",
      " 19499 lines processed.\n",
      " 19599 lines processed.\n",
      " 19699 lines processed.\n",
      " 19799 lines processed.\n",
      " 19899 lines processed.\n",
      " 19999 lines processed.\n",
      " 20099 lines processed.\n",
      " 20199 lines processed.\n",
      " 20299 lines processed.\n",
      " 20399 lines processed.\n",
      " 20499 lines processed.\n",
      " 20599 lines processed.\n",
      " 20699 lines processed.\n",
      " 20799 lines processed.\n",
      " 20899 lines processed.\n",
      " 20999 lines processed.\n",
      " 21099 lines processed.\n",
      " 21199 lines processed.\n",
      " 21299 lines processed.\n",
      " 21399 lines processed.\n",
      " 21499 lines processed.\n",
      " 21599 lines processed.\n",
      " 21699 lines processed.\n",
      " 21799 lines processed.\n",
      " 21899 lines processed.\n",
      " 21999 lines processed.\n",
      " 22099 lines processed.\n",
      " 22199 lines processed.\n",
      " 22299 lines processed.\n",
      " 22399 lines processed.\n",
      " 22499 lines processed.\n",
      " 22599 lines processed.\n",
      " 22699 lines processed.\n",
      " 22799 lines processed.\n",
      " 22899 lines processed.\n",
      " 22999 lines processed.\n",
      " 23099 lines processed.\n",
      " 23199 lines processed.\n",
      " 23299 lines processed.\n",
      " 23399 lines processed.\n",
      " 23499 lines processed.\n",
      " 23599 lines processed.\n",
      " 23699 lines processed.\n",
      " 23799 lines processed.\n",
      " 23899 lines processed.\n",
      " 23999 lines processed.\n",
      " 24099 lines processed.\n",
      " 24199 lines processed.\n",
      " 24299 lines processed.\n",
      " 24399 lines processed.\n",
      " 24499 lines processed.\n",
      " 24599 lines processed.\n",
      " 24699 lines processed.\n",
      " 24799 lines processed.\n",
      " 24899 lines processed.\n",
      " 24999 lines processed.\n",
      " 25099 lines processed.\n",
      " 25199 lines processed.\n",
      " 25299 lines processed.\n",
      " 25399 lines processed.\n",
      " 25499 lines processed.\n",
      " 25599 lines processed.\n",
      " 25699 lines processed.\n",
      " 25799 lines processed.\n",
      " 25899 lines processed.\n",
      " 25999 lines processed.\n",
      " 26099 lines processed.\n",
      " 26199 lines processed.\n",
      " 26299 lines processed.\n",
      " 26399 lines processed.\n",
      " 26499 lines processed.\n",
      " 26599 lines processed.\n",
      " 26699 lines processed.\n",
      " 26799 lines processed.\n",
      " 26899 lines processed.\n",
      " 26999 lines processed.\n",
      " 27099 lines processed.\n",
      " 27199 lines processed.\n",
      " 27299 lines processed.\n",
      " 27399 lines processed.\n",
      " 27499 lines processed.\n",
      " 27599 lines processed.\n",
      " 27699 lines processed.\n",
      " 27799 lines processed.\n",
      " 27899 lines processed.\n",
      " 27999 lines processed.\n",
      " 28099 lines processed.\n",
      " 28199 lines processed.\n",
      " 28299 lines processed.\n",
      " 28399 lines processed.\n",
      " 28499 lines processed.\n",
      " 28599 lines processed.\n",
      " 28699 lines processed.\n",
      " 28799 lines processed.\n",
      " 28899 lines processed.\n",
      " 28999 lines processed.\n",
      " 29099 lines processed.\n",
      " 29199 lines processed.\n",
      " 29299 lines processed.\n",
      " 29399 lines processed.\n",
      " 29499 lines processed.\n",
      " 29599 lines processed.\n",
      " 29699 lines processed.\n",
      " 29799 lines processed.\n",
      " 29899 lines processed.\n",
      " 29999 lines processed.\n",
      " 30099 lines processed.\n",
      " 30199 lines processed.\n",
      " 30299 lines processed.\n",
      " 30399 lines processed.\n",
      " 30499 lines processed.\n",
      " 30599 lines processed.\n",
      " 30699 lines processed.\n",
      " 30799 lines processed.\n",
      " 30899 lines processed.\n",
      " 30999 lines processed.\n",
      " 31099 lines processed.\n",
      " 31199 lines processed.\n",
      " 31299 lines processed.\n",
      " 31399 lines processed.\n",
      " 31499 lines processed.\n",
      " 31599 lines processed.\n",
      " 31699 lines processed.\n",
      " 31799 lines processed.\n",
      " 31899 lines processed.\n",
      " 31999 lines processed.\n",
      " 32099 lines processed.\n",
      " 32199 lines processed.\n",
      " 32299 lines processed.\n",
      " 32399 lines processed.\n",
      " 32499 lines processed.\n",
      " 32599 lines processed.\n",
      " 32699 lines processed.\n",
      " 32799 lines processed.\n",
      " 32899 lines processed.\n",
      " 32999 lines processed.\n",
      " 33099 lines processed.\n",
      " 33199 lines processed.\n",
      " 33299 lines processed.\n",
      " 33399 lines processed.\n",
      " 33499 lines processed.\n",
      " 33599 lines processed.\n",
      " 33699 lines processed.\n",
      " 33799 lines processed.\n",
      " 33899 lines processed.\n",
      " 33999 lines processed.\n",
      " 34099 lines processed.\n",
      " 34199 lines processed.\n",
      " 34299 lines processed.\n",
      " 34399 lines processed.\n",
      " 34499 lines processed.\n",
      " 34599 lines processed.\n",
      " 34699 lines processed.\n",
      " 34799 lines processed.\n",
      " 34899 lines processed.\n",
      " 34999 lines processed.\n",
      " 35099 lines processed.\n",
      " 35199 lines processed.\n",
      " 35299 lines processed.\n",
      " 35399 lines processed.\n",
      " 35499 lines processed.\n",
      " 35599 lines processed.\n",
      " 35699 lines processed.\n",
      " 35799 lines processed.\n",
      " 35899 lines processed.\n",
      " 35999 lines processed.\n",
      " 36099 lines processed.\n",
      " 36199 lines processed.\n",
      " 36299 lines processed.\n",
      " 36399 lines processed.\n",
      " 36499 lines processed.\n",
      " 36599 lines processed.\n",
      " 36699 lines processed.\n",
      " 36799 lines processed.\n",
      " 36899 lines processed.\n",
      " 36999 lines processed.\n",
      " 37099 lines processed.\n",
      " 37199 lines processed.\n",
      " 37299 lines processed.\n",
      " 37399 lines processed.\n",
      " 37499 lines processed.\n",
      " 37599 lines processed.\n",
      " 37699 lines processed.\n",
      " 37799 lines processed.\n",
      " 37899 lines processed.\n",
      " 37999 lines processed.\n",
      " 38099 lines processed.\n",
      " 38199 lines processed.\n",
      " 38299 lines processed.\n",
      " 38399 lines processed.\n",
      " 38499 lines processed.\n",
      " 38599 lines processed.\n",
      " 38699 lines processed.\n",
      " 38799 lines processed.\n",
      " 38899 lines processed.\n",
      " 38999 lines processed.\n",
      " 39099 lines processed.\n",
      " 39199 lines processed.\n",
      " 39299 lines processed.\n",
      " 39399 lines processed.\n",
      " 39499 lines processed.\n",
      " 39599 lines processed.\n",
      " 39699 lines processed.\n",
      " 39799 lines processed.\n",
      " 39899 lines processed.\n",
      " 39999 lines processed.\n",
      " 40099 lines processed.\n",
      " 40199 lines processed.\n",
      " 40299 lines processed.\n",
      " 40399 lines processed.\n",
      " 40499 lines processed.\n",
      " 40599 lines processed.\n",
      " 40699 lines processed.\n",
      " 40799 lines processed.\n",
      " 40899 lines processed.\n",
      "Completed clean of file: /opt/vs/call-graph-reduced-function_counts-vs263.csv with 5 elves removed.\n"
     ]
    }
   ],
   "source": [
    "clean_out_elves_in_vs263('/opt/vs/call-graph-reduced-function_counts-vs263.csv', '/opt/vs/call-graph-reduced-fucntion_counts-vs263-clean.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
