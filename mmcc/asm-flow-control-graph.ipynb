{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 1. Generate Flow Control Graphs From Intel x86/AMD 64 Assembly\n",
    "       Parse a directory of .asm files and construct IDA Pro style flow control graphs using code\n",
    "       blocks as vertices and jmp/call instructions as directed edges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import graph as gra # http://www.python-course.eu/graphs_python.php\n",
    "import os\n",
    "from csv import writer\n",
    "from multiprocessing import Pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "opcodes = ['call','int','ja','jb','jc','je','jg','jge','jl','jle','jmp','jna','jnb','jnl','jno','jnp','jns','jnz','jo','jp','jz']\n",
    "blocks = ['sub_', 'loc_', 'locret_']\n",
    "#blocks = ['loc_','locret_']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TRAINING\n",
    "# Now divide the train files into four groups for multiprocessing\n",
    "ext_drive = '/opt/kaggle/train/'\n",
    "tfiles = os.listdir(ext_drive)\n",
    "quart = len(tfiles)/4\n",
    "train1 = tfiles[:quart]\n",
    "train2 = tfiles[quart:(2*quart)]\n",
    "train3 = tfiles[(2*quart):(3*quart)]\n",
    "train4 = tfiles[(3*quart):]\n",
    "print(len(tfiles), quart, (len(train1)+len(train2)+len(train3)+len(train4)))\n",
    "trains = [train1, train2, train3, train4]\n",
    "p = Pool(4)\n",
    "p.map(extract_flow_control_graphs, trains)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TESTING\n",
    "# Now divide the test files into four groups for multiprocessing\n",
    "ext_drive = '/opt/kaggle/test/'\n",
    "tfiles = os.listdir(ext_drive)\n",
    "quart = len(tfiles)/4\n",
    "test1 = tfiles[:quart]\n",
    "test2 = tfiles[quart:(2*quart)]\n",
    "test3 = tfiles[(2*quart):(3*quart)]\n",
    "test4 = tfiles[(3*quart):]\n",
    "print(len(tfiles), quart, (len(test1)+len(test2)+len(test3)+len(test4)))\n",
    "tests = [test1, test2, test3, test4]\n",
    "p = Pool(4)\n",
    "p.map(extract_flow_control_graphs, tests)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def construct_flow_control_graph(lines):\n",
    "    vertex = '.program_entry_point' # this is the root node, corresponds to the program entry point not C main().\n",
    "    vertex_count = 1\n",
    "    edge_count = 0\n",
    "    cfgraph = gra.Graph()\n",
    "    cfgraph.add_vertex(vertex)\n",
    "    \n",
    "    for row in lines:\n",
    "      row = row.rstrip('\\r\\n')  # get rid of newlines they are annoying.\n",
    "      if ';' in row:\n",
    "        row = row.split(';')[0] # get rid of comments they are annoying.\n",
    "        #print(row)\n",
    "      \n",
    "      # get rid of all these things they are annoying.\n",
    "      row = row.replace('short',' ')\n",
    "      row = row.replace('ds:',' ')\n",
    "      row = row.replace('dword',' ')\n",
    "      row = row.replace('ptr',' ').replace(':',' ').replace(',',' ') #.replace('??',' ')\n",
    "      row = row.replace('@','').replace('?','')\n",
    "      parts = row.split() # tokenize code line\n",
    "        \n",
    "      parts_len = len(parts)\n",
    "    \n",
    "      if (parts_len < 3): # this is just a comment line, do NOT change this!!!\n",
    "        continue\n",
    "        \n",
    "      if (parts_len > 3 ):\n",
    "        if (parts[3] == 'endp'): # skip procedure end labels\n",
    "            continue\n",
    "        if (parts[3] == 'proc'): # check for procedures not labelled sub_????\n",
    "            vertex = parts[2]\n",
    "            cfgraph.add_vertex(vertex)\n",
    "            vertex_count += 1\n",
    "            continue\n",
    "            \n",
    "      # check for subroutines and block labels\n",
    "      # block and subroutine labels are always after the .text HHHHHHHH relative address\n",
    "      for block in blocks:\n",
    "          token = parts[2]  \n",
    "          idx = token.find(block)\n",
    "          if (idx == 0): # add new vertex to the graph, we are now in a new subroutine or code block\n",
    "              vertex = token\n",
    "              cfgraph.add_vertex(vertex)\n",
    "              # print(\"Vertex: \" + vertex)\n",
    "              vertex_count += 1\n",
    "              break\n",
    "                              \n",
    "\n",
    "      # now check for edge opcode    \n",
    "      for opcode in opcodes: # check the line for a new edge\n",
    "          if opcode in parts:\n",
    "              # Extract desination address/function name/interrupt number as the directed edge.\n",
    "              idx = parts.index(opcode)\n",
    "              if ((idx + 1) < parts_len):\n",
    "                  next_vertex = parts[idx + 1]\n",
    "              else:\n",
    "                  next_vertex = \"none\"\n",
    "              cfgraph.add_edge(vertex, next_vertex)\n",
    "              # print(\"Edge: \" + vertex + \" \" + parts[idx] + \" \" + edge)\n",
    "              break\n",
    "\n",
    "    print(\"Vertex Count: {:d}\".format(vertex_count))\n",
    "    \n",
    "    return cfgraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "help(str.find)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "help(str.split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_flow_control_graphs(tfiles):\n",
    "    asm_files = [i for i in tfiles if '.asm' in i]\n",
    "    ftot = len(asm_files)\n",
    "    \n",
    "    pid = os.getpid()\n",
    "    print('Process id:', pid)\n",
    "    feature_file = 'data/' + str(pid) + '-malware-flow-control-graph.csv'  \n",
    "    print('Flow Control Graph file:', feature_file)\n",
    "    \n",
    "    graph_lines = []\n",
    "    with open(feature_file, 'w') as f:\n",
    "        # write the column names for the csv file\n",
    "        fw = writer(f)\n",
    "        # colnames = ['filename','entropy','filesize']\n",
    "        # fw.writerow(colnames)\n",
    "        \n",
    "        # Now iterate through the file list and extract the graph from each file.\n",
    "        for idx, fname in enumerate(asm_files):\n",
    "            fasm = open(ext_drive + fname, 'r', errors='ignore')\n",
    "            #filesize = os.path.getsize(ext_drive + fname)\n",
    "            lines = fasm.readlines()\n",
    "            \n",
    "            flow_control_graph = construct_flow_control_graph(lines)\n",
    "\n",
    "            graph_lines.append([fname[:fname.find('.asm')]] + [str(flow_control_graph.to_str_multi_line_sorted())])   \n",
    "            \n",
    "            del(flow_control_graph)\n",
    "            \n",
    "            # Print progress\n",
    "            if (idx+1) % 10 == 0:\n",
    "              print(pid, idx + 1, 'of', ftot, 'files processed.')\n",
    "              fw.writerows(graph_lines)\n",
    "              feature_counts = []\n",
    "                \n",
    "        # Write remaining files\n",
    "        if len(graph_lines) > 0:\n",
    "            fw.writerows(graph_lines)\n",
    "            graph_lines = []\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Convert Flow Control Graphs To Feature Vectors\n",
    "       Since determining graph isomorphisms is NP-Complete, convert the graphs to feature vectors and\n",
    "       use chi-squared tests to reduce the number features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 3. Generate Call Graphs From Intel x86/AMD 64 Assembly\n",
    "       Parse a directory of .asm files and construct call graphs using subroutines\n",
    "       as vertices and call instructions as directed edges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "call_opcodes = ['call','int']\n",
    "call_blocks = ['sub_']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def construct_call_graph(lines):\n",
    "    vertex = '.program_entry_point' # this is the root node, corresponds to the program entry point not C main().\n",
    "    vertex_count = 1\n",
    "    edge_count = 0\n",
    "    cfgraph = gra.Graph()\n",
    "    cfgraph.add_vertex(vertex)\n",
    "    \n",
    "    for row in lines:\n",
    "      row = row.rstrip('\\r\\n')  # get rid of newlines they are annoying.\n",
    "      if ';' in row:\n",
    "        row = row.split(';')[0] # get rid of comments they are annoying.\n",
    "        #print(row)\n",
    "      \n",
    "      # get rid of all these things they are annoying.\n",
    "      row = row.replace('short','').replace('ds:',' ')\n",
    "      row = row.replace('dword','').replace('near','')\n",
    "      row = row.replace('ptr','').replace(':',' ').replace(',',' ') #.replace('??',' ')\n",
    "      row = row.replace('@','').replace('?','')\n",
    "      parts = row.split() # tokenize code line\n",
    "        \n",
    "      if (len(parts) < 4): # this is just a comment line\n",
    "        continue\n",
    "        \n",
    "      if (parts[3] == 'endp'): # ignore subroutine end labels\n",
    "        continue\n",
    "        \n",
    "      # check for subroutines and block labels\n",
    "      # block and subroutine labels are always after the .text HHHHHHHH relative address\n",
    "      for block in call_blocks:\n",
    "          token = parts[2]  \n",
    "          idx = token.find(block)\n",
    "          if ((idx == 0) or (parts[3] == 'proc')):\n",
    "              # add new vertex to the graph, we are now in a new subroutine\n",
    "              vertex = token\n",
    "              cfgraph.add_vertex(vertex)\n",
    "              # print(\"Vertex: \" + vertex)\n",
    "              vertex_count += 1\n",
    "              break\n",
    "\n",
    "      # now check for edge opcode    \n",
    "      for opcode in call_opcodes: # check the line for a new edge\n",
    "          if opcode in parts:\n",
    "              # Extract desination address/function name/interrupt number as the directed edge.\n",
    "              idx = parts.index(opcode)\n",
    "              edge_count += 1\n",
    "              if ((idx + 1) < len(parts)): # in a few ASM files there is no operand, disassembly error?\n",
    "                  next_vertex = parts[idx + 1]\n",
    "              else:\n",
    "                  next_vertex = \"none\"\n",
    "              cfgraph.add_edge(vertex, next_vertex)\n",
    "              # print(\"Edge: \" + vertex + \" \" + parts[idx] + \" \" + edge)\n",
    "              break\n",
    "\n",
    "    # print(\"Vertex Count: {:d}\".format(vertex_count))\n",
    "    \n",
    "    return cfgraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_call_graphs(tfiles):\n",
    "    asm_files = [i for i in tfiles if '.asm' in i]\n",
    "    ftot = len(asm_files)\n",
    "    \n",
    "    pid = os.getpid()\n",
    "    print('Process id:', pid)\n",
    "    feature_file = 'data/' + str(pid) + '-malware-call-graph-features.csv'  \n",
    "    print('Graph Feature file:', feature_file)\n",
    "    \n",
    "    graph_lines = []\n",
    "    graph_features = []\n",
    "    graph_file = open('data/' + str(pid) + '-malware-call-graphs.gv', 'w') # write as a graphviz DOT format file\n",
    "    with open(feature_file, 'w') as f:\n",
    "        # write the column names for the csv file\n",
    "        fw = writer(f)\n",
    "        #colnames = ['filename','vertex_count','edge_count','delta_max','density','diameter']\n",
    "        colnames = ['filename','vertex_count','edge_count','delta_max','density']\n",
    "        fw.writerow(colnames)\n",
    "        \n",
    "        # Now iterate through the file list and extract the call graph from each file.\n",
    "        for idx, fname in enumerate(asm_files):\n",
    "            fasm = open(ext_drive + fname, 'r', errors='ignore')\n",
    "            \n",
    "            lines = fasm.readlines()\n",
    "            \n",
    "            call_graph = construct_call_graph(lines)\n",
    "            cgvc = call_graph.n_vertices()\n",
    "            cgec = call_graph.n_edges()\n",
    "            cgdm = call_graph.delta_max()\n",
    "            cgde = call_graph.density()\n",
    "            # cdia = call_graph.diameter() this is constantly problematic !!!\n",
    "            graph_features.append([fname[:fname.find('.asm')]] + [cgvc, cgec, cgdm, cgde])\n",
    "            call_graph.set_graph_name(fname[:fname.find('.asm')])\n",
    "            # graph_lines.append(call_graph.to_str('multinoleaf')) \n",
    "            graph_lines.append(call_graph.to_str('singlenoleaf'))\n",
    "            \n",
    "            del(call_graph) # for some reason new graphs get appended to the previous graphs if not deleted???\n",
    "            \n",
    "            # Print progress\n",
    "            if (idx + 1) % 10 == 0:\n",
    "              print(pid, idx + 1, 'of', ftot, 'files processed.')\n",
    "              fw.writerows(graph_features)\n",
    "              graph_file.writelines(graph_lines)\n",
    "              graph_features = []\n",
    "              graph_lines = []\n",
    "                \n",
    "        # Write remaining files\n",
    "        if len(graph_lines) > 0:\n",
    "            fw.writerows(graph_features)\n",
    "            graph_file.writelines(graph_lines)\n",
    "            graph_features = []\n",
    "            graph_lines = []\n",
    "\n",
    "    graph_file.close()\n",
    "    \n",
    "    print('Process id: {:d} finished.'.format(pid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 4. Generate Function Counts From Call Graphs.\n",
    "\n",
    "      Construct a dictionary of functions names and counts for every ASM file call graph then\n",
    "      write the function counts out to a csv feature file, it will be a sparse matrix.\n",
    "      feature columns will be like (filename, function names in sorted order.....)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Generate column names for the function count feature set\n",
    "#call_graph_files = ['../3815-malware-call-graphs.gv', '../3816-malware-call-graphs.gv', '../3817-malware-call-graphs.gv', '../3818-malware-call-graphs.gv']\n",
    "#call_graph_files = ['data/2278-malware-call-graphs.gv']\n",
    "\n",
    "def generate_column_names(call_graph_file):\n",
    "    counter = 0\n",
    "    column_names = ['filename']\n",
    "    graph_names = []\n",
    "    graph_name = \"none\"\n",
    "    graph_functions = {}\n",
    "\n",
    "    fapi = open(\"data/APIs.txt\")\n",
    "    defined_apis = fapi.readlines()\n",
    "    defined_apis = defined_apis[0].split(',')\n",
    "    fapi.close()\n",
    "    \n",
    "    pid = os.getpid()\n",
    "    print('Process id:', pid)\n",
    "    column_names_file = 'data/' + str(pid) + '-reduced-column-names.csv'  \n",
    "    print('Column names file: {:s}'.format(column_names_file))\n",
    "    graph_names_file = 'data/' + str(pid) + '-graph-names.csv'  \n",
    "    print('Graph names file: {:s}'.format(graph_names_file))    \n",
    "\n",
    "    with open(call_graph_file, 'r', errors='ignore') as cfg:\n",
    "        print(\"Starting graph file: {:s}\".format(call_graph_file))\n",
    "        for line in cfg:\n",
    "            line = line.rstrip('\\r\\n')  # get rid of newlines they are annoying.\n",
    "            # get rid of all these things they are annoying.\n",
    "            line = line.replace(',',' ').replace('[',' ').replace(']',' ').replace('->',' ').replace(\"\\'\", ' ')\n",
    "            parts = line.split() # tokenize call graph line\n",
    "            graph_name = parts[0]\n",
    "            parts = parts[1:]\n",
    "            graph_names.append(graph_name)\n",
    "            graph_functions = {}\n",
    "            \n",
    "            for func in parts:\n",
    "                if func not in defined_apis: # ignore these API functions, they have already been counted.\n",
    "                    if func.startswith('sub') or func.startswith('loc') or func.startswith('unk'):\n",
    "                        func = func[:5] # lets try to reduce the vast number of functions.\n",
    "                    elif func.startswith('eax+') or func.startswith('ebx+') or func.startswith('ecx+') or func.startswith('edx+'):\n",
    "                        func = func[:5]\n",
    "                    elif func.startswith('edi+') or func.startswith('esi+'):\n",
    "                        func = func[:5]\n",
    "                    elif func.startswith('byte_') or func.startswith('word_'): # or func.startswith('nullsub')\n",
    "                        func = func[:6]\n",
    "                    else: # reduce the feature set some more so my pissy pants PC can handle it.\n",
    "                        func = func[:8]\n",
    "                    if func not in column_names: # NOTE: or in Defined APIs, these have already been counted.    \n",
    "                        column_names.append(func)\n",
    "\n",
    " \n",
    "            counter += 1\n",
    "            # Print progress\n",
    "            if ((counter + 1) % 1000) == 0:\n",
    "                print(\"Processed number {:d} Graph_name {:s} Total column names {:d}\".format(counter,graph_name,len(column_names)))       \n",
    "\n",
    "                \n",
    "    with open(column_names_file, 'w') as cols:\n",
    "        fw = writer(cols)\n",
    "        fw.writerow(column_names)\n",
    "    \n",
    "    print(\"Completed writing {:d} column names.\".format(len(column_names)))\n",
    "\n",
    "    with open(graph_names_file, 'w') as gras:\n",
    "        fw = writer(gras)\n",
    "        fw.writerow(graph_names)\n",
    "    \n",
    "    print(\"Completed writing {:d} graph names.\".format(len(graph_names)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Generate the merged column names file single line.\n",
    "counter = 0\n",
    "column_names = []\n",
    "column_name_files = ['data/3346-reduced-column-names.csv', 'data/3347-reduced-column-names.csv', 'data/3348-reduced-column-names.csv', 'data/3349-reduced-column-names.csv']\n",
    "for cnamefile in column_name_files:\n",
    "    with open(cnamefile, 'r') as cras:\n",
    "        print(\"Starting file: {:s}\".format(cnamefile))\n",
    "        colstr = cras.readline()\n",
    "        colnames = colstr.split(',')\n",
    "        for cname in colnames:\n",
    "            if cname not in column_names:\n",
    "                column_names.append(cname)\n",
    "                \n",
    "            counter += 1\n",
    "            # Print progress\n",
    "            if ((counter + 1) % 1000) == 0:\n",
    "                print(\"Processed column names {:d}\".format(counter))       \n",
    "\n",
    "with open('data/all-reduced-function-column-names.csv', 'w') as cols:\n",
    "    fw = writer(cols)\n",
    "    fw.writerow(column_names)\n",
    "    \n",
    "print(\"Completed writing column names total = {:d}\".format(len(column_names)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting file: data/3346-reduced-column-names.csv\n",
      "Processed column names 999\n",
      "Processed column names 1999\n",
      "Processed column names 2999\n",
      "Processed column names 3999\n",
      "Processed column names 4999\n",
      "Processed column names 5999\n",
      "Processed column names 6999\n",
      "Processed column names 7999\n",
      "Processed column names 8999\n",
      "Processed column names 9999\n",
      "Processed column names 10999\n",
      "Processed column names 11999\n",
      "Processed column names 12999\n",
      "Processed column names 13999\n",
      "Processed column names 14999\n",
      "Processed column names 15999\n",
      "Processed column names 16999\n",
      "Processed column names 17999\n",
      "Processed column names 18999\n",
      "Processed column names 19999\n",
      "Processed column names 20999\n",
      "Processed column names 21999\n",
      "Processed column names 22999\n",
      "Processed column names 23999\n",
      "Processed column names 24999\n",
      "Processed column names 25999\n",
      "Processed column names 26999\n",
      "Processed column names 27999\n",
      "Processed column names 28999\n",
      "Processed column names 29999\n",
      "Processed column names 30999\n",
      "Starting file: data/3347-reduced-column-names.csv\n",
      "Processed column names 31999\n",
      "Processed column names 32999\n",
      "Processed column names 33999\n",
      "Processed column names 34999\n",
      "Processed column names 35999\n",
      "Processed column names 36999\n",
      "Processed column names 37999\n",
      "Processed column names 38999\n",
      "Processed column names 39999\n",
      "Processed column names 40999\n",
      "Processed column names 41999\n",
      "Processed column names 42999\n",
      "Processed column names 43999\n",
      "Processed column names 44999\n",
      "Processed column names 45999\n",
      "Processed column names 46999\n",
      "Processed column names 47999\n",
      "Processed column names 48999\n",
      "Processed column names 49999\n",
      "Processed column names 50999\n",
      "Processed column names 51999\n",
      "Processed column names 52999\n",
      "Processed column names 53999\n",
      "Processed column names 54999\n",
      "Processed column names 55999\n",
      "Processed column names 56999\n",
      "Processed column names 57999\n",
      "Processed column names 58999\n",
      "Processed column names 59999\n",
      "Processed column names 60999\n",
      "Processed column names 61999\n",
      "Processed column names 62999\n",
      "Starting file: data/3348-reduced-column-names.csv\n",
      "Processed column names 63999\n",
      "Processed column names 64999\n",
      "Processed column names 65999\n",
      "Processed column names 66999\n",
      "Processed column names 67999\n",
      "Processed column names 68999\n",
      "Processed column names 69999\n",
      "Processed column names 70999\n",
      "Processed column names 71999\n",
      "Processed column names 72999\n",
      "Processed column names 73999\n",
      "Processed column names 74999\n",
      "Processed column names 75999\n",
      "Processed column names 76999\n",
      "Processed column names 77999\n",
      "Processed column names 78999\n",
      "Processed column names 79999\n",
      "Processed column names 80999\n",
      "Processed column names 81999\n",
      "Processed column names 82999\n",
      "Processed column names 83999\n",
      "Processed column names 84999\n",
      "Processed column names 85999\n",
      "Processed column names 86999\n",
      "Processed column names 87999\n",
      "Processed column names 88999\n",
      "Processed column names 89999\n",
      "Processed column names 90999\n",
      "Processed column names 91999\n",
      "Processed column names 92999\n",
      "Starting file: data/3349-reduced-column-names.csv\n",
      "Processed column names 93999\n",
      "Processed column names 94999\n",
      "Processed column names 95999\n",
      "Processed column names 96999\n",
      "Processed column names 97999\n",
      "Processed column names 98999\n",
      "Processed column names 99999\n",
      "Processed column names 100999\n",
      "Processed column names 101999\n",
      "Processed column names 102999\n",
      "Processed column names 103999\n",
      "Processed column names 104999\n",
      "Processed column names 105999\n",
      "Processed column names 106999\n",
      "Processed column names 107999\n",
      "Processed column names 108999\n",
      "Processed column names 109999\n",
      "Processed column names 110999\n",
      "Processed column names 111999\n",
      "Processed column names 112999\n",
      "Processed column names 113999\n",
      "Processed column names 114999\n",
      "Processed column names 115999\n",
      "Processed column names 116999\n",
      "Processed column names 117999\n",
      "Processed column names 118999\n",
      "Processed column names 119999\n",
      "Processed column names 120999\n",
      "Processed column names 121999\n",
      "Processed column names 122999\n",
      "Processed column names 123999\n",
      "Processed column names 124999\n",
      "Processed column names 125999\n",
      "Processed column names 126999\n",
      "Processed column names 127999\n",
      "Completed writing column names total = 71315\n"
     ]
    }
   ],
   "source": [
    "#Generate the merged column names file multiline.\n",
    "counter = 0\n",
    "column_names = []\n",
    "column_name_files = ['data/3346-reduced-column-names.csv', 'data/3347-reduced-column-names.csv', 'data/3348-reduced-column-names.csv', 'data/3349-reduced-column-names.csv']\n",
    "for cnamefile in column_name_files:\n",
    "    with open(cnamefile, 'r') as cras:\n",
    "        print(\"Starting file: {:s}\".format(cnamefile))\n",
    "        colstr = cras.readline()\n",
    "        colnames = colstr.split(',')\n",
    "        for cname in colnames:\n",
    "            if cname not in column_names:    \n",
    "                column_names.append(cname)\n",
    "                \n",
    "            counter += 1\n",
    "            # Print progress\n",
    "            if ((counter + 1) % 1000) == 0:\n",
    "                print(\"Processed column names {:d}\".format(counter))       \n",
    "\n",
    "with open('data/all-reduced-function-column-names-multiline.csv', 'w') as cols:\n",
    "    for cname in column_names:\n",
    "        outline = cname + \"\\n\"\n",
    "        cols.write(outline)\n",
    "    \n",
    "print(\"Completed writing column names total = {:d}\".format(len(column_names)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "help(cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed writing column names total = 154810\n"
     ]
    }
   ],
   "source": [
    "print(\"Completed writing column names total = {:d}\".format(len(column_names)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "call_graph_files = ['/opt/kaggle/3662-malware-call-graphs-sline.gv', '/opt/kaggle/3663-malware-call-graphs-sline.gv', '/opt/kaggle/3664-malware-call-graphs-sline.gv', '/opt/kaggle/3665-malware-call-graphs-sline.gv']\n",
    "p = Pool(4)\n",
    "p.map(generate_column_names, call_graph_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "792\n"
     ]
    }
   ],
   "source": [
    "print(len(defined_apis))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Generate function counts from graph files of the ASM malware samples.\n",
    "# call_graph_files = ['../3815-malware-call-graphs.gv', '../3816-malware-call-graphs.gv', '../3817-malware-call-graphs.gv', '../3818-malware-call-graphs.gv']\n",
    "\n",
    "def generate_function_counts(call_graph_file):\n",
    "    counter = 0\n",
    "    error_count = 0\n",
    "    \n",
    "    fapi = open(\"data/APIs.txt\")\n",
    "    defined_apis = fapi.readlines()\n",
    "    defined_apis = defined_apis[0].split(',')\n",
    "    fapi.close()\n",
    "    \n",
    "    colf = open('data/all-reduced-function-column-names.csv', 'r')\n",
    "    all_column_names = []\n",
    "    column_lines = colf.readlines()\n",
    "    for line in column_lines:\n",
    "        all_column_names += line.split(',')\n",
    "    col_names_len = len(all_column_names)\n",
    "    colf.close()\n",
    "    print(\"Column Names: {:d}\".format(col_names_len))\n",
    "    \n",
    "    pid = os.getpid()\n",
    "    print('Process id:', pid)\n",
    "    feature_file_name = 'data/' + str(pid) + '-call-graph-reduced-function_counts.csv'  \n",
    "    print('Call graph function counts file: {:s}'.format(feature_file_name))\n",
    "    feature_file = open(feature_file_name, 'w')\n",
    "    fw = writer(feature_file)\n",
    "    \n",
    "    call_graph_function_features = []\n",
    "    \n",
    "    with open(call_graph_file, 'r', errors='ignore') as cfg:\n",
    "        for line in cfg:\n",
    "            line.rstrip('\\r\\n')  # get rid of newlines they are annoying.\n",
    "            # get rid of all these things they are annoying.\n",
    "            line = line.replace(',',' ').replace('[',' ').replace(']',' ').replace('->',' ').replace(\"\\'\", ' ')\n",
    "            parts = line.split() # tokenize graph line\n",
    "            \n",
    "            graph_name = parts[0]\n",
    "            parts = parts[1:]\n",
    "            function_dict = {}\n",
    "            \n",
    "            # now generate the function counts for this call graph\n",
    "            \n",
    "            for func in parts:\n",
    "                if func not in defined_apis: # ignore these API functions, they have already been counted.\n",
    "                    if func.startswith('sub') or func.startswith('loc') or func.startswith('unk'):\n",
    "                        func = func[:5] # lets try to reduce the vast number of functions.\n",
    "                    elif func.startswith('eax+') or func.startswith('ebx+') or func.startswith('ecx+') or func.startswith('edx+'):\n",
    "                        func = func[:5]\n",
    "                    elif func.startswith('edi+') or func.startswith('esi+'):\n",
    "                        func = func[:5]\n",
    "                    elif func.startswith('byte_') or func.startswith('word_'): # or func.startswith('nullsub')\n",
    "                        func = func[:6]\n",
    "                    else: # reduce the feature set some more so my pissy pants PC can handle it.\n",
    "                        func = func[:8]\n",
    "                        \n",
    "                    if (func in function_dict):\n",
    "                        function_dict[func] += 1\n",
    "                    else:\n",
    "                        function_dict[func] = 1\n",
    "            \n",
    "            # now generate the output row for this call graph\n",
    "\n",
    "            function_counts = [0] * col_names_len # zero everything because this is a sparse matrix\n",
    "            for func in function_dict:\n",
    "                for idx, cname in enumerate(all_column_names):\n",
    "                    if func == cname:\n",
    "                        function_counts[idx] = function_dict[func]\n",
    "                        break\n",
    "                \n",
    "            call_graph_function_features.append([graph_name] + function_counts)\n",
    "            \n",
    "            # Print progress and write out rows\n",
    "            counter += 1\n",
    "            if ((counter + 1) % 100) == 0:\n",
    "                print(\"{:d} Graph: {:s} Count: {:d}\".format(pid, graph_name, counter))\n",
    "                fw.writerows(call_graph_function_features)\n",
    "                call_graph_function_features = []\n",
    "                \n",
    "        # Write remaining files\n",
    "        if len(call_graph_function_features) > 0:\n",
    "            fw.writerows(call_graph_function_features)\n",
    "            call_graph_function_features = []  \n",
    "    \n",
    "    feature_file.close()\n",
    "    \n",
    "    print(\"Completed processing {:d} graphs.\".format(counter))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column Names: 71319\n",
      "Column Names: 71319\n",
      "Process id: 5833\n",
      "Column Names: 71319\n",
      "Column Names: 71319\n",
      "Call graph function counts file: data/5833-call-graph-reduced-function_counts.csv\n",
      "Process id: 5834\n",
      "Process id: 5836\n",
      "Process id: 5835\n",
      "Call graph function counts file: data/5834-call-graph-reduced-function_counts.csv\n",
      "Call graph function counts file: data/5835-call-graph-reduced-function_counts.csv\n",
      "Call graph function counts file: data/5836-call-graph-reduced-function_counts.csv\n",
      "5833 Graph: aviAP6tKyz1hRZMqGU53 Count: 99\n",
      "5833 Graph: jaHTAc6MVElKrygs1CzO Count: 199\n",
      "5836 Graph: anbzmJ7dC1EM3B9QcPDv Count: 99\n",
      "5834 Graph: iQZCbSt5Pg7Ajqk0LYvl Count: 99\n",
      "5833 Graph: AaZv2sYJqpI8He5NBPfD Count: 299\n",
      "5835 Graph: 82nLVASm1tBsdgHMvEXP Count: 99\n",
      "5833 Graph: h2dmlEMoAOI3fPxZ0Bsc Count: 399\n",
      "5836 Graph: HkFgNRjpGq8VcAW0J9bx Count: 199\n",
      "5833 Graph: JsZfmrqkFHCBy8YIa7nc Count: 499\n",
      "5836 Graph: Dq9nwWYMFS0xf1PGuaZe Count: 299\n",
      "5835 Graph: dFHf8NmMGkjsXULCnYya Count: 199\n",
      "5833 Graph: cpvSmajetq6923K05MRD Count: 599\n",
      "5833 Graph: CMlwq0OXr8ygBbNESAIs Count: 699\n",
      "5834 Graph: 5YFaPQwRb9Z6tMnkyvcd Count: 199\n",
      "5833 Graph: dyTqfGCNl7Hv21oZLMK5 Count: 799\n",
      "5835 Graph: KivjcOQFy2PmDhodWJxC Count: 299\n",
      "5836 Graph: J1rAVnKEwlsFqkG9xYNh Count: 399\n",
      "5834 Graph: 9JN4L3y2vZm5qUcdglSu Count: 299\n",
      "5834 Graph: 6tMgf5pGIsvHSn83WZyx Count: 399\n",
      "5833 Graph: c6Da2l807vGgAbYnUjPm Count: 899\n",
      "5836 Graph: C7mXMKDZBFjnNwxYH1et Count: 499\n",
      "5835 Graph: 7pL84lUq3OAP0DiZ2mxj Count: 399\n",
      "5834 Graph: IsKl4oiejb6qy3wZALzV Count: 499\n",
      "5836 Graph: 21l6hcFemsiIapSqw8XO Count: 599\n",
      "5833 Graph: 87Cf2go9YulDiHEyekaS Count: 999\n",
      "5833 Graph: 7ujgT8KqGXmU5JFoM6LH Count: 1099\n",
      "5835 Graph: 8pm2Rv3Feu4LEnKd7WMZ Count: 499\n",
      "5834 Graph: 8rKwml3nGVhWAiUjDH19 Count: 599\n",
      "5833 Graph: 5SgZNAyTDU8FCiEvwX7k Count: 1199\n",
      "5833 Graph: hmjcZH7rQ4qkLlbDzePy Count: 1299\n",
      "5835 Graph: elrTVLuNkdYoPWhxUAIS Count: 599\n",
      "5833 Graph: GEaJRH0xlprO3C8ShcQK Count: 1399\n",
      "5834 Graph: 7vKDaxCOzwoU6ZT1nNys Count: 699\n",
      "5836 Graph: je4HWtx9M5ySpPBO0wgf Count: 699\n",
      "5835 Graph: Ccj3AiR725Y8vHFaXPkG Count: 699\n",
      "5833 Graph: G3td1uFSQnHxrZ9jPE4X Count: 1499\n",
      "5834 Graph: i9psO08wRdorlvU2HBSj Count: 799\n",
      "5836 Graph: EqhtziP8KUOaIHlx9ZwG Count: 799\n",
      "5835 Graph: JbzyqeI9ThRA4LWQN823 Count: 799\n",
      "5833 Graph: JgoL72E0WlXhmFHD5cxp Count: 1599\n",
      "5834 Graph: cEaWDOBIweu0jkQUNTLH Count: 899\n",
      "5836 Graph: 3gJ82lOIDrUae5FdNfMZ Count: 899\n",
      "5835 Graph: fDJXgvz2cAtPopmIO0hi Count: 899\n",
      "5833 Graph: k0ZLV6BAGJySHuP3RK78 Count: 1699\n",
      "5834 Graph: Cv742oSZumb1kGIU8YBN Count: 999\n",
      "5833 Graph: 3x9R8E6Gv5JhDda2eoq4 Count: 1799\n",
      "5836 Graph: e9Jm5ZnFL3IS6ANtf1q4 Count: 999\n",
      "5835 Graph: 9eKQwWVFlonXmSTaG5jH Count: 999\n",
      "5833 Graph: c02hlI4zUA38QbmGDw1p Count: 1899\n",
      "5835 Graph: BdonVvH3SC2IFgc6NQTh Count: 1099\n",
      "5836 Graph: IuAnZRqFaMl5eYbPwXvz Count: 1099\n",
      "5833 Graph: iVwHJulRcYEMXqfUKQ5T Count: 1999\n",
      "5835 Graph: IdyaQv1LrRA9cSM5PX7j Count: 1199\n",
      "5834 Graph: 2LfPBwRadN3TulW86pvO Count: 1099\n",
      "5836 Graph: 87zKHh2vErX6McwSZQyT Count: 1199\n",
      "5833 Graph: aF4NnlKgwEO2CiT5uHb6 Count: 2099\n",
      "5835 Graph: 8QRIkDJS4bgy2nEvMhfa Count: 1299\n",
      "5833 Graph: a5nMiy8gBFtpuGTxIv0W Count: 2199\n",
      "5836 Graph: aUt7mRJKdfkuXpSLjHEy Count: 1299\n",
      "5835 Graph: 6ZpMEfH0nyKexuT7YPQm Count: 1399\n",
      "5834 Graph: 4Gp7IWxvROy0MuS5PNzT Count: 1199\n",
      "5834 Graph: E1ALI9eYXgG2Nk0HOoby Count: 1299\n",
      "5836 Graph: ksTyZ4jN21lBiC9UYOGe Count: 1399\n",
      "5835 Graph: if0JHIlxvu42A5aDjh6b Count: 1499\n",
      "5834 Graph: 4g2UZv3nOGWbTCiptDsQ Count: 1399\n",
      "5833 Graph: fYiBUrAX4R3e8EyJtP2x Count: 2299\n",
      "5834 Graph: J7pqfAnFV3goS1rWbkMT Count: 1499\n",
      "5834 Graph: LgeBlyYQAD1NiVGRuxwk Count: 1599\n",
      "5833 Graph: dOeUI4W0VjhNE7uF8lz1 Count: 2399\n",
      "5834 Graph: e3XnUREiQbm2co1yLMaG Count: 1699\n",
      "5836 Graph: bnTwRQSvo3CLKsgGaI45 Count: 1499\n",
      "5833 Graph: H7tCOwf5rzSluXiGsx8W Count: 2499\n",
      "5835 Graph: Hu9pJxZ8iDTOElkA1ghf Count: 1599\n",
      "5836 Graph: hHMbnm8juIizgTR7LAWC Count: 1599\n",
      "5833 Graph: H5QR3qYhCvUI6px1oaEV Count: 2599\n",
      "5834 Graph: B50qpIVmRKnr41xaPSgX Count: 1799\n",
      "5835 Graph: B5KcvwyrkjX8oZWbMRap Count: 1699\n",
      "5836 Graph: 9mHqfWkgcJb7utKZ5rCV Count: 1699\n",
      "Completed processing 2666 graphs.\n",
      "5834 Graph: iWScf1VBXaT78d5qDJFY Count: 1899\n",
      "5836 Graph: 07nrG1cLKUPxjOlWMFiV Count: 1799\n",
      "5834 Graph: 1QHbFGR3qcur6aWZw0Jk Count: 1999\n",
      "5835 Graph: j39FkSLKI7NT46Qicvwm Count: 1799\n",
      "5834 Graph: 71XJkmfwVpndPa42sbDE Count: 2099\n",
      "5836 Graph: 7pbVgdSnjUPTrwcR5GXu Count: 1899\n",
      "5835 Graph: By43LOPkYhDjrGTg0ien Count: 1899\n",
      "5836 Graph: k89IEjZqBQTxAYLGWCwi Count: 1999\n",
      "5834 Graph: D8hnW1FkOeZw5mVsKyx7 Count: 2199\n",
      "5835 Graph: ARaMSnYZ0XzjUg8swodL Count: 1999\n",
      "5836 Graph: fhPezOdViLjBl7pyaFNw Count: 2099\n",
      "5834 Graph: 3d2x0BYR7aiJLfPNsbgo Count: 2299\n",
      "5836 Graph: E5eMjyTfCLnsQPqvowWY Count: 2199\n",
      "5835 Graph: CwaO90vYBFn32T4uRMDA Count: 2099\n",
      "5835 Graph: ig4IwJckBlRaT3FUN6rV Count: 2199\n",
      "5834 Graph: F8CeVXzxLhnjo0BHATPZ Count: 2399\n",
      "5836 Graph: 7BNLHFXEAJRS1bwZxziq Count: 2299\n",
      "5834 Graph: 06KfrF7ltESna2ZHPVp5 Count: 2499\n",
      "5836 Graph: GHavcNsxJtgbO1IiQmB5 Count: 2399\n",
      "5835 Graph: hOuU4H27YSEGojD8aBQ6 Count: 2299\n",
      "5834 Graph: Bm1lbzD7fUOh8soCgRjV Count: 2599\n",
      "5835 Graph: 2qgetG9lN5EZdfQXH4a8 Count: 2399\n",
      "5836 Graph: cAK1HjqnlsSFpP0Ivw7x Count: 2499\n",
      "5834 Graph: Iq6zLDfSXargmVQAZbHl Count: 2699\n",
      "Completed processing 2724 graphs.\n",
      "5836 Graph: 9gE6rY37hlG2Jdmy5IAT Count: 2599\n",
      "5835 Graph: bcdvUs26L3ofyDO5P1Wm Count: 2499\n",
      "5836 Graph: 0cdnSIvN489sFUwYlrMQ Count: 2699\n",
      "Completed processing 2736 graphs.\n",
      "5835 Graph: BkV8KZjcfSy3DFa5MYre Count: 2599\n",
      "5835 Graph: d9eL3MgusYQWwtf6l2yn Count: 2699\n",
      "Completed processing 2742 graphs.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None, None, None, None]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "call_graph_files = ['/opt/kaggle/3662-malware-call-graphs-sline.gv', '/opt/kaggle/3663-malware-call-graphs-sline.gv', '/opt/kaggle/3664-malware-call-graphs-sline.gv', '/opt/kaggle/3665-malware-call-graphs-sline.gv']\n",
    "p = Pool(4)\n",
    "p.map(generate_function_counts, call_graph_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Ok, so we still have 71000+ features even after severely reducing the function name lengths.\n",
    "# This is a problem. Having to process such a huge sparse matrix requires a lot of memory.\n",
    "# Solution 1: rent an AWS server with plenty-o-ram.\n",
    "# Solution 2: buy more RAM for my linux box.\n",
    "# Solution 3: break the sparse matrix into smaller chunks and process individually.\n",
    "# Solution 4: try the pandas sparse matrix data structure.\n",
    "# Goto: feature-reduction-call-graphs.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TRAINING\n",
    "# Now divide the train files into four groups for multiprocessing\n",
    "ext_drive = '/opt/kaggle/train/'\n",
    "tfiles = os.listdir(ext_drive)\n",
    "quart = int(len(tfiles)/4)\n",
    "train1 = tfiles[:quart]\n",
    "train2 = tfiles[quart:(2*quart)]\n",
    "train3 = tfiles[(2*quart):(3*quart)]\n",
    "train4 = tfiles[(3*quart):]\n",
    "print(len(tfiles), quart, (len(train1)+len(train2)+len(train3)+len(train4)))\n",
    "trains = [train1, train2, train3, train4]\n",
    "p = Pool(4)\n",
    "p.map(extract_call_graphs, trains)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TESTING\n",
    "# Now divide the test files into four groups for multiprocessing\n",
    "ext_drive = '/opt/kaggle/test/'\n",
    "tfiles = os.listdir(ext_drive)\n",
    "quart = int(len(tfiles)/4)\n",
    "test1 = tfiles[:quart]\n",
    "test2 = tfiles[quart:(2*quart)]\n",
    "test3 = tfiles[(2*quart):(3*quart)]\n",
    "test4 = tfiles[(3*quart):]\n",
    "print(len(tfiles), quart, (len(test1)+len(test2)+len(test3)+len(test4)))\n",
    "tests = [test1, test2, test3, test4]\n",
    "p = Pool(4)\n",
    "p.map(extract_call_graphs, tests)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 5. Test Code Only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process id: 4845\n",
      "Graph file: data/4845-malware-graph.csv\n",
      "Vertex Count: 1103\n",
      "Vertex Count: 6418\n"
     ]
    }
   ],
   "source": [
    "# Test graph generation\n",
    "ext_drive = '/opt/kaggle/train/'\n",
    "tfiles = ['0A32eTdBKayjCWhZqDOQ.asm', '1aAwe4J9VHrsq8uEoZhf.asm']\n",
    "extract_graph(tfiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process id: 17216\n",
      "Graph file: data/17216-malware-graph.csv\n",
      "Vertex Count: 1103\n",
      "Vertex Count: 180\n"
     ]
    }
   ],
   "source": [
    "# Test graph generation\n",
    "ext_drive = '/opt/kaggle/train/'\n",
    "tfiles = ['0A32eTdBKayjCWhZqDOQ.asm', '0ACDbR5M3ZhBJajygTuf.asm']\n",
    "extract_graph(tfiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process id: 21155\n",
      "Flow Control Graph file: data/21155-malware-flow-control-graph.csv\n",
      "Vertex Count: 179\n"
     ]
    }
   ],
   "source": [
    "# Test graph generation\n",
    "ext_drive = '/opt/kaggle/train/'\n",
    "tfiles = ['0ACDbR5M3ZhBJajygTuf.asm']\n",
    "extract_flow_control_graphs(tfiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process id: 17385\n",
      "Graph file: data/17385-malware-call-graph.csv\n",
      "Vertex Count: 179\n",
      "Vertex Count: 5\n"
     ]
    }
   ],
   "source": [
    "# Test call graph generation\n",
    "ext_drive = '/opt/kaggle/train/'\n",
    "tfiles = ['0A32eTdBKayjCWhZqDOQ.asm', '0ACDbR5M3ZhBJajygTuf.asm']\n",
    "extract_call_graphs(tfiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process id: 2278\n",
      "Graph Feature file: data/2278-malware-call-graph-features.csv\n",
      "Process id: 2278 finished.\n"
     ]
    }
   ],
   "source": [
    "# Test call graph generation\n",
    "ext_drive = '/opt/kaggle/train/'\n",
    "tfiles = ['0A32eTdBKayjCWhZqDOQ.asm', '0ACDbR5M3ZhBJajygTuf.asm']\n",
    "extract_call_graphs(tfiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "g = { \"a\" : [\"c\"],\n",
    "      \"b\" : [\"c\",\"e\",\"f\"],\n",
    "      \"c\" : [\"a\",\"b\",\"d\",\"e\"],\n",
    "      \"d\" : [\"c\"],\n",
    "      \"e\" : [\"b\",\"c\",\"f\"],\n",
    "      \"f\" : [\"b\",\"e\"]\n",
    "}\n",
    "\n",
    "\n",
    "graph = gra.Graph(g)\n",
    "\n",
    "diameter = graph.diameter()\n",
    "\n",
    "print(diameter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process id: 20756\n",
      "Graph file: data/20756-malware-call-graph.csv\n",
      "Vertex Count: 90\n",
      "Vertex Count: 4\n"
     ]
    }
   ],
   "source": [
    "# Test call graph generation\n",
    "ext_drive = '/opt/kaggle/train/'\n",
    "tfiles = ['0A32eTdBKayjCWhZqDOQ.asm', '0ACDbR5M3ZhBJajygTuf.asm']\n",
    "extract_call_graphs(tfiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process id: 20756\n",
      "Graph file: data/20756-malware-call-graph.csv\n",
      "Vertex Count: 90\n",
      "Vertex Count: 4\n"
     ]
    }
   ],
   "source": [
    "# Test call graph generation\n",
    "ext_drive = '/opt/kaggle/train/'\n",
    "tfiles = ['0A32eTdBKayjCWhZqDOQ.asm', '0ACDbR5M3ZhBJajygTuf.asm']\n",
    "extract_call_graphs(tfiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(cfgraph.vertices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fasm = open('/opt/kaggle/train/1aAwe4J9VHrsq8uEoZhf.asm', 'r', errors='ignore')\n",
    "lines = fasm.readlines()\n",
    "parse_asm_code(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fasm = open('/opt/kaggle/train/0A32eTdBKayjCWhZqDOQ.asm', 'r', errors='ignore')\n",
    "lines = fasm.readlines()\n",
    "cfgraph = parse_asm_code(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "help(open)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bar': 2, 'foo': 1}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import OrderedDict\n",
    "a = {'foo': 1, 'bar': 2}\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('bar', 2), ('foo', 1)])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{'foo': 1, 'bar': 2}\n",
    "b = OrderedDict(sorted(a.items()))\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 5, 10]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myDic={10: 'b', 3:'a', 5:'c'}\n",
    "sorted_list=sorted(myDic.keys())\n",
    "sorted_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Deprecated, too slowwwww.\n",
    "\n",
    "def construct_function_dict(call_graph):\n",
    "    vertex_dict = call_graph.get_vertex_counts()\n",
    "    file_name = call_graph.get_graph_name()\n",
    "    function_dict[file_name] = vertex_dict\n",
    "            \n",
    "    return\n",
    "\n",
    "def generate_column_names():\n",
    "    for file_name in function_dict:\n",
    "        function_counts = function_dict[file_name]\n",
    "        for func_name in function_counts:\n",
    "            if func_name not in function_column_names:\n",
    "                function_column_names.append(func_name)\n",
    "            \n",
    "    function_column_names.sort()\n",
    "    \n",
    "    return\n",
    "\n",
    "def write_function_counts():\n",
    "    # open function count feature file\n",
    "    pid = os.getpid()\n",
    "    feature_file = 'data/' + str(pid) + '-malware-function-count-features.csv'  \n",
    "    print('Function Count Feature file:', feature_file)\n",
    "    f = open(feature_file, 'w')\n",
    "    fw = writer(f)\n",
    "    \n",
    "    colnames = ['filename'] + function_column_names\n",
    "    fw.writerow(colnames)\n",
    "    \n",
    "    counter = 0\n",
    "    function_features = []\n",
    "    function_count_list = [0] * len(function_column_names)\n",
    "    for filename in function_dict:\n",
    "        function_counts = function_dict[filename]\n",
    "        for function in function_counts:\n",
    "            idx = function_column_names.index(function)\n",
    "\n",
    "            function_count_list[idx] = function_counts[function]\n",
    "        \n",
    "        print(filename)\n",
    "        print(function_count_list)\n",
    "        function_features.append([filename] + function_count_list)\n",
    "        counter += 1\n",
    "        # Print progress\n",
    "        if (counter + 1) % 10 == 0:\n",
    "            print(pid, counter + 1, ' files processed.')\n",
    "            fw.writerows(function_features)\n",
    "            function_features = []\n",
    "                \n",
    "        \n",
    "    # Write remaining files\n",
    "    if (len(function_features) > 0):\n",
    "        fw.writerows(function_features)\n",
    "        func_features = []\n",
    "       \n",
    "        \n",
    "    f.close()\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
