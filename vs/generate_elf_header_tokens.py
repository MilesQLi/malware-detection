# generate-elf-header-tokens.py
#
# Parse a bunch of ELF Header dump files generated by objdump and
# extract keywords such section names, import libs and functions.
# These tokens will be used for feature extraction from the
# ELF headers.
#
# Inputs : list of ELF Header files.
#          Temp file name for token counts.
#          File name for combined token counts.
#
# Outputs: elf-header-tokens.txt
#          elf-header-token-counts.csv
#          row format = [token_name, count]
#
# Author: Derek Chadwick
# Date  : 17/09/2016


import os
from csv import writer
import numpy as np
import pandas as pd
import re



def save_token_counts(token_counter_map, out_file_name):
    # Output the PE Header token counts.
    pid = os.getpid()
    out_file = "data/" + str(pid) + "-" + out_file_name
    fop = open(out_file, 'w')
    csv_wouter = writer(fop)

    outlines = []
    sorted_keys = token_counter_map.keys()
    sorted_keys.sort()
    counter = 0
    
    for key in sorted_keys:
        outlines.append([key, token_counter_map[key]])
        counter += 1
        if (counter % 100) == 0: # write out some lines
            csv_wouter.writerows(outlines)
            outlines = []
            print("Processed token {:s} -> {:d}.".format(key, token_counter_map[key]))

    # Finish off.
    if (len(outlines) > 0):
        csv_wouter.writerows(outlines)
        outlines = []

    print("Completed writing {:d} tokens.".format(len(sorted_keys)))    
    fop.close()

    return


def get_token_count_map(token_df):
    # Read in the token count file and create a dict.
    token_dict = {}
    type_y = np.array(token_df['token_name'])
    
    for idx in range(token_df.shape[0]): # First fill the dict with the token counts
        token_dict[token_df.iloc[idx,0]] = token_df.iloc[idx,1]
        

    return token_dict

    
def generate_elf_tokens(mp_params):
    # Parse a bunch of ELF headers dumped by objdump and extract
    # section names, import DLLs, import functions and exported functions.
    file_list = mp_params.file_list
    out_count_file = mp_params.count_file
    
    psections = re.compile('\s+\d{1,2}\s+(\.\w+|\w+)\s+\d+')  # Pattern for section names.
    pfunctions = re.compile('\s+\w+\s+\d{1,4}\s+(.+)\s*')     # Pattern for import function names.

    
    token_counter_map = {}
    counter = 0
    pid = os.getpid()
    
    for idx, fname in enumerate(file_list):

        fip = open(fname, 'r')
        in_lines = fip.readlines()
        
        counter += 1
        
        for line in in_lines:

            line = line.rstrip() # get rid of newlines they are annoying.
            token_val = ""

            m = psections.match(line)
            if m != None:
                token_val = m.group(1)
                #print("Section: {:s}".format(token_val))
            else:
                m = pfunctions.match(line)
                if m != None:
                    token_val = m.group(1)
                else:
                    continue
                        
            # Clean the token name, the function name regex is picking up random crap.
            idx = token_val.find('\t')
            if idx > 0:
                token_val = token_val[0:idx]
                
            # Count the token type.
            if token_val in token_counter_map.keys():
                token_counter_map[token_val] += 1
            else:
                token_counter_map[token_val] = 1


        if (counter % 1000) == 0:
            print("{:d} Processed {:d} header files.".format(pid, counter))

        fip.close()
        
        
    save_token_counts(token_counter_map, out_count_file)
    
    return


def save_combine(token_counter_map, out_file_name):
    # Save the combined token counts.
    
    out_file = "data/" + out_file_name
    fop = open(out_file, 'w')
    csv_wouter = writer(fop)
    cols = ['token_name','count'] 
    csv_wouter.writerow(cols)
    
    outlines = []
    sorted_keys = token_counter_map.keys()
    sorted_keys.sort()
    counter = 0
    
    for key in sorted_keys:
        outlines.append([key, token_counter_map[key]])
        counter += 1
        if (counter % 100) == 0: # write out some lines
            csv_wouter.writerows(outlines)
            outlines = []
            print("Processed token {:s} -> {:d}.".format(key, token_counter_map[key]))

    # Finish off.
    if (len(outlines) > 0):
        csv_wouter.writerows(outlines)
        outlines = []

    fop.close()
    
    print("Completed writing {:d} tokens.".format(len(sorted_keys)))  
    
    return


def combine_token_files(token_file, count_file):
    # Function to combine the newly generated token files into one file:
    # 1. list data directory
    # 2. For each file in file list that matches (\d\d\d\d-pe-header-tokens.csv)
    # 3. Trim the filenames if necessary (should remove VirusShare_  prefix).
    # 4. Concatenate the unsorted token feature files.
    # 5. Sort and write to data/sorted-token-features.csv

    
    p1 = re.compile('\d{3,5}-' + count_file) # This is the PID prefix for each file.
    file_list = os.listdir('data/')
    counter = 0
    token_map = {}
    
    for file_name in file_list:
        if p1.match(file_name):
            fip = open('data/' + file_name, 'r')
            in_lines = fip.readlines()
            for line in in_lines:
                tokens = line.split(',')
                if tokens[0] not in token_map.keys():
                    token_map[tokens[0]] = int(tokens[1])
                else:
                    token_map[tokens[0]] += int(tokens[1])
                    
            counter += len(in_lines)
            fip.close()
            
 

    save_combine(token_map, token_file)
    
    print('Completed combine of {:d} PE/COFF header tokens.'.format(counter)) 
    
    return


class Multi_Params(object):
    def __init__(self, tokenfile="", countfile="", filelist=[]):
        self.token_file = tokenfile
        self.count_file = countfile
        self.file_list = filelist
        

# Start of script.

#TODO: parse command line options for input/output file names.

#file_list = os.listdir('/opt/vs/train1hdr/')
#generate_pe_tokens(file_list,'data/pe-header-tokens-vs251.txt','data/pe-header-token-counts-vs251.csv')

#token_file = 'pe-header-tokens-vs263.csv'
#count_file = 'pe-header-token-counts-vs263.csv'
#ext_drive = '/opt/vs/train3hdr/'

token_file = 'pe-header-tokens-vs264.txt'
count_file = 'pe-header-token-counts-vs264.csv'
ext_drive = '/opt/vs/train4hdr/'

file_list = os.listdir(ext_drive)
tfiles = []

for fname in file_list:
    tfiles.append(ext_drive + fname)
    
#quart = len(tfiles)/4
#train1 = tfiles[:quart]
#train2 = tfiles[quart:(2*quart)]
#train3 = tfiles[(2*quart):(3*quart)]
#train4 = tfiles[(3*quart):]

print("Files: {:d} - {:d} - {:d}".format(len(tfiles), quart, (len(train1)+len(train2)+len(train3)+len(train4))))

mp1 = Multi_Params(token_file, count_file, tfiles)
#mp2 = Multi_Params(token_file, count_file, train2)
#mp3 = Multi_Params(token_file, count_file, train3)
#mp4 = Multi_Params(token_file, count_file, train4)

#trains = [mp1, mp2, mp3, mp4]
#p = Pool(4)
#p.map(generate_pe_tokens, trains)

#combine_token_files(token_file, count_file)

# End of Script.
