# malware-detection

Experiments in malware detection and classification using machine learning techniques.

## 1. Microsoft Malware Classification Challenge
    https://www.kaggle.com/c/malware-classification
### 1.1 Feature Engineering
    Initial feature engineering consisited of extracting various keyword counts from the ASM files 
    and the entropy and file size from the BYTE files of the 10868 malware samples in the training set. 
    Image files of the first 1000 bytes of the ASM and BYTE files were created and combined with 
    keyword and entropy data. This resulted in a set of 2018 features.
    Flow control graphs and call graphs were generated for each ASM sample. A feature set was
    then generated from the graphs, including graph maximum delta, density, diameter and function
    counts etc.
### 1.2 Feature Selection
    Statistical analysis of the feature set using chi-squared tests to remove features that are 
    independent of the class labels or have low variance. The BYTE file images were found to be weak
    learners and were removed from the feature set. A comparison of the best features from the chi-squared
    tests with reduced feature sets of between 10% - 50% of the original features.
#### 1.2.1 Selection Comparison
    Testing with an ExtraTreesClassifier and 10-fold cross validation produced the following results:
    - Original ASM Keyword Counts (1006 features): logloss = 0.034
    - 10% Best ASM Features with Entropy and Image Features (202 features): logloss = 0.0174
    - 20% Best ASM with Entropy and Image Features (402 features): logloss = 0.0164
    - 30% Best ASM with Entropy and Image Features with Feature Statistics (621 features): 
      multiclass logloss = 0.0133
      score = 0.9978
      Confusion Matrix:
      [[1540    0    0    0    0    1    0    0    0]
      [   1 2475    2    0    0    0    0    0    0]
      [   0    0 2942    0    0    0    0    0    0]
      [   1    0    0  474    0    0    0    0    0]
      [   2    0    0    0   38    2    0    0    0]
      [   3    0    0    0    0  748    0    0    0]
      [   1    0    0    0    0    0  397    0    0]
      [   0    0    0    0    0    0    0 1225    3]
      [   0    0    0    0    0    0    0    8 1005]]
    - 40% Best ASM and image features with feature statistics:
      ExtraTreesClassifier with 1000 estimators on 10868 training samples and 823 features 
      using 10-fold cross validation:
        multiclass logloss = 0.0135
        score = 0.9976
        Confustion Matrix:
        [[1541    0    0    0    0    0    0    0    0]
        [   1 2475    2    0    0    0    0    0    0]
        [   0    0 2942    0    0    0    0    0    0]
        [   1    0    0  474    0    0    0    0    0]
        [   5    0    0    0   37    0    0    0    0]
        [   5    0    0    0    0  746    0    0    0]
        [   1    0    0    0    0    0  397    0    0]
        [   0    0    0    0    0    0    0 1227    1]
        [   0    0    0    0    0    0    0    9 1004]]
#### 1.2.2 Feature Selection Summary
     The performance of the ExtraTreesClassifier is optimal at around 30% of ASM and image features 
     with highest variance plus sample statistics, entropy and file size.
### 1.3 Model Selection
    Selection of candidate models using GridSearchCV to find classifier parameters. 
    - SVM:
    - ExtraTrees:
    - XGBoost: 30% Best Features
               logloss: 0.0080
               accuracy: 0.9981
               Confusion Matrix:
               [[1540    0    0    0    0    1    0    0    0]
                [   2 2475    0    1    0    0    0    0    0]
                [   0    0 2941    0    0    0    1    0    0]
                [   0    0    0  474    0    1    0    0    0]
                [   1    0    0    0   41    0    0    0    0]
                [   4    0    0    0    1  746    0    0    0]
                [   0    0    0    0    0    0  398    0    0]
                [   0    0    0    0    0    0    0 1227    1]
                [   0    0    0    0    0    0    0    8 1005]]               
    - NaiveBayes:
    - KNN:
### 1.4 Graphs
!["File Entropy Graph 1"](https://github.com/dchad/malware-detection/blob/master/resources/file-entropy-by-class.png "File Entropy by Malware Class")
         1. Shannon's Entropy by malware class. A score of 0.0 means the bytes are all the same value, 
            a score of 1.0 means every byte in the file has a different value.
!["File Entropy Graph 2"](https://github.com/dchad/malware-detection/blob/master/resources/file-entropy-by-size.png "File Entropy by File Size")
         2. Shannon's Entropy by file size. A score of 0.0 means the bytes are all the same value, 
            a score of 1.0 means every byte in the file has a different value.
!["ASM Registry Counts"](https://github.com/dchad/malware-detection/blob/master/resources/register-counts.png "EDX by ESI Registry Counts")
         3. Assembler register EDX by ESI counts.
### 1.5 Conclusions
   TODO:
## 2. VirusShare.com Malware Collection Analysis
   VirusShare.com regularly publishes huge collections of malware binaries for use by researchers. 
   Each malware archive is currently around 25GB in size. Several of the latest archives have been 
   downloaded to use as training and test sets. The archives used are:
   Training - VirusShare_00251.zip
   Test     - VirusShare_00252.zip
   
### 2.1 Converting to ASM and Generating Training Labels
### 2.2 Feature Extraction
### 2.3 Feature Selection
### 2.4 Model Selection
### 2.5 Conclusions
## 3. Automated Sensor Malware Detection
TODO:
## 4. References
TODO:
## 5. Notes on installing xgboost for Python.
### 5.1 Source Install.
 If installing from source, after building and installing you have
 problems loading other packages it is because of the xgboost-0.4-py2.7.egg.pth
 file that the install script dumps in the python dist-packages
 directory. You will have to delete the .pth file then
 go change the installation of the xgboost egg and egg-info files in the 
 python dist-packages directory from:

 /usr/local/lib/python2.7/dist-packages/xgboost-0.4-py2.7.egg/EGG_INFO

 to:

 /usr/local/lib/python2.7/dist-packages/xgboost-0.4-py2.7.dist-info

 and: 

 /usr/local/lib/python2.7/dist-packages/xgboost-0.4-py2.7.egg/xgboost

 to:

 /usr/local/lib/python2.7/dist-packages/xgboost

 Now python will be able to find all the packages.
### 5.2 Pip Install. 
 If installing from pip, DONT EVEN TRY, it is a nightmare.
### 5.3 Anaconda Install.
 XGBoost is not a part of the official distribution but several 
 community members have created Conda packages for it. The
 most up to date package seems to be by user creditx. The following
 command will install the package:

 conda install -c creditx xgboost
